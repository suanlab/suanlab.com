---
title: "[논문 리뷰] STEM: Scaling Transformers with Embedding Modules"
date: "2026-01-21"
excerpt: "Fine-grained sparsity promises higher parametric capacity without proportional per-token compute, but often suffers from training instability, load balancing, and communication overhead. We introduce ..."
category: "Paper Review"
tags: ["Paper Review","cs.LG","cs.LG"]
thumbnail: "/assets/images/blog/20260121-paper-2601-10639-stem-scaling-transformers-with.jpg"
---

# [논문 리뷰] STEM: Scaling Transformers with Embedding Modules

## TL;DR

Transformer 모델의 효율성과 해석 가능성을 향상시키기 위해, 본 논문은 **STEM(Scaling Transformers with Embedding Modules)**이라는 새로운 방법론을 제안합니다. STEM은 FFN(Feed-Forward Network)의 업 프로젝션을 토큰별 임베딩 테이블로 대체하여, **파라미터 접근성과 FLOPs(부동소수점 연산)를 줄이면서도 모델의 해석 가능성과 학습 안정성을 향상**시킵니다. 실험 결과, STEM은 다양한 벤치마크에서 성능을 크게 향상시켰으며, 특히 지식과 추론이 중요한 작업에서 두드러진 성과를 보였습니다. 이러한 결과는 STEM이 **파라미터 메모리를 확장하는 효과적 방법**임을 보여줍니다.

## 연구 배경 및 동기

Transformer 모델은 자연어 처리 분야에서 혁신적인 변화를 가져왔지만, **모델의 크기와 복잡성**이 커지면서 **계산 비용과 메모리 사용량**이 크게 증가하는 문제에 직면하고 있습니다. 특히, FFN(Feed-Forward Network)은 이러한 문제의 주요 원인 중 하나로, 각 토큰에 대해 고정된 수의 파라미터와 FLOPs가 필요합니다. 기존의 접근법은 주로 모델의 크기를 줄이거나, 효율적인 계산을 위한 다양한 최적화 기법을 적용하는 방식으로 문제를 해결하려 했습니다. 그러나 이러한 방법들은 **모델의 성능 저하나 해석 가능성의 감소**를 초래할 수 있습니다.

본 연구는 이러한 한계점을 극복하기 위해 STEM을 제안합니다. STEM은 FFN의 업 프로젝션을 토큰별 임베딩 테이블로 대체하여, **각 토큰에 대해 필요한 파라미터와 FLOPs를 줄이고, 모델의 해석 가능성을 향상**시킵니다. 이를 통해 STEM은 **모델의 계산 효율성을 높이면서도 성능을 유지하거나 향상**시킬 수 있습니다. STEM의 가장 큰 장점은 **모델의 파라미터 용량을 토큰별 계산과 분리**함으로써, **더 많은 파라미터를 활성화하여 실용적인 용량 확장**을 가능하게 한다는 점입니다.

## 관련 연구

Transformer 모델의 효율성을 개선하기 위한 다양한 연구들이 진행되어 왔습니다. 대표적인 연구로는 **Sparse Transformers**, **Efficient Transformers**, **MoE(Mixture of Experts)**, **Dynamic Sparsity**, 그리고 **Pruning Techniques** 등이 있습니다. 이들 연구들은 주로 **모델의 크기나 계산량을 줄이기 위한 다양한 방법**을 제안하였으나, 각기 다른 한계점을 가지고 있습니다.

| 연구 | 접근법 | 한계점 | 본 논문과의 차별점 |
|------|--------|--------|--------------------|
| Sparse Transformers | 희소성 적용 | 학습 불안정성 | STEM은 학습 안정성 유지 |
| Efficient Transformers | 계산 최적화 | 성능 저하 가능성 | STEM은 성능 향상 |
| MoE | 전문가 혼합 모델 | 복잡한 라우팅 | STEM은 라우팅 제거 |
| Dynamic Sparsity | 동적 희소성 | 복잡한 구현 | STEM은 간단한 구현 |
| Pruning Techniques | 파라미터 제거 | 성능 저하 | STEM은 파라미터 활용 |

## 핵심 기여

1. **STEM 방법론 제안**: FFN의 업 프로젝션을 토큰별 임베딩 테이블로 대체하여, **모델의 계산 효율성과 해석 가능성을 향상**시킵니다.
2. **학습 안정성 향상**: STEM은 **극단적인 희소성에도 불구하고 안정적으로 학습**됩니다.
3. **성능 개선**: 다양한 벤치마크에서 **3-4% 성능 향상**을 보여주며, 특히 지식과 추론이 중요한 작업에서 두드러진 성과를 보입니다.
4. **파라미터 메모리 확장**: STEM은 **더 많은 파라미터를 활성화하여 실용적인 용량 확장**을 제공합니다.

## 제안 방법론

STEM의 핵심 아이디어는 FFN의 업 프로젝션을 **토큰별 임베딩 테이블**로 대체하여, 각 토큰에 대해 **특정한 벡터를 레이어-로컬 임베딩 테이블에서 검색**하여 사용하는 것입니다. 이를 통해 STEM은 **파라미터 접근성과 FLOPs를 줄이면서도 성능을 향상**시킵니다.

### 모델 아키텍처

STEM은 Transformer 모델의 FFN에서 업 프로젝션을 토큰별 임베딩 테이블로 대체합니다. 이 방식은 **런타임 라우팅을 제거**하고, **CPU 오프로딩과 비동기 프리페치**가 가능해지며, **파라미터 용량을 토큰별 FLOPs와 크로스 디바이스 통신에서 분리**합니다.

### 핵심 수식

1. **임베딩 검색**: 각 레이어에서 토큰 $t$에 해당하는 행을 선택하여 사용합니다.
   $$ y_\ell = W_d (SiLU(W_g x_\ell) \odot U_\ell[t]) $$
   여기서 $W_d$는 다운 프로젝션, $W_g$는 게이트 프로젝션, $U_\ell[t]$는 토큰 $t$에 해당하는 임베딩 벡터입니다.

2. **임베딩 공간의 각도 확산**: STEM은 **임베딩 공간의 큰 각도 확산을 학습**하여 정보 저장 용량을 향상시킵니다.

3. **파라미터 용량 분리**: STEM은 **파라미터 용량을 토큰별 계산과 분리**하여, **더 많은 파라미터를 활성화**할 수 있습니다.

## 실험 설정

### 데이터셋 및 평가 지표

STEM은 다양한 데이터셋과 벤치마크에서 평가되었습니다. 주요 데이터셋으로는 **ARC-Challenge**, **OpenBookQA**, **GSM8K**, **MMLU** 등이 있으며, 각 데이터셋은 **지식과 추론이 중요한 작업**을 포함하고 있습니다. 평가 지표로는 **정확도(Accuracy)**가 사용되었습니다.

### 베이스라인 및 하이퍼파라미터

STEM의 성능은 **dense baseline** 및 **Hash layer MoE**와 비교되었습니다. 하이퍼파라미터는 다음과 같이 설정되었습니다:

| 하이퍼파라미터 | 값 |
|---------------|----|
| 모델 크기 | 350M, 1B |
| 학습률 | 0.001 |
| 배치 크기 | 128 |
| 학습 에폭 | 10 |

## 실험 결과 분석

### 주요 결과

STEM은 다양한 벤치마크에서 **3-4%의 성능 향상**을 보였습니다. 특히, **지식과 추론이 중요한 작업**에서 두드러진 성과를 나타냈습니다.

| 데이터셋 | Dense Baseline | STEM | 성능 향상률(%) |
|----------|----------------|------|----------------|
| ARC-Challenge | 72.5 | 75.5 | 4.1 |
| OpenBookQA | 68.0 | 70.5 | 3.7 |
| GSM8K | 65.0 | 67.5 | 3.8 |
| MMLU | 66.5 | 69.0 | 3.8 |

### Ablation Study

STEM의 각 구성 요소가 성능에 미치는 영향을 분석하기 위해 **Ablation Study**를 수행하였습니다. STEM의 **임베딩 테이블**과 **게이트 프로젝션**을 제거했을 때, 성능이 크게 저하되는 것을 확인할 수 있었습니다.

## 비판적 평가

### 강점

1. **효율성**: STEM은 **계산 효율성을 높이면서도 성능을 유지**합니다.
2. **해석 가능성**: 모델의 **해석 가능성을 향상**시킵니다.
3. **학습 안정성**: **극단적인 희소성에도 불구하고 안정적으로 학습**됩니다.

### 한계점 및 개선 방향

1. **복잡성**: STEM의 구현이 다소 복잡할 수 있으며, **구현의 용이성을 개선**할 필요가 있습니다.
2. **적용 범위**: STEM의 **적용 범위를 확장**하기 위한 추가 연구가 필요합니다.

### 재현성 평가

STEM의 실험 결과는 **충분한 세부 정보와 코드**를 통해 재현할 수 있습니다. 그러나, **특정 하드웨어 요구 사항**이 있을 수 있으므로, 이를 명확히 할 필요가 있습니다.

## 향후 연구 방향

STEM의 **확장 가능성과 적용 분야**를 넓히기 위해 추가 연구가 필요합니다. 특히, STEM의 **임베딩 테이블을 다양한 자연어 처리 작업에 적용**하여 성능을 평가하는 것이 중요합니다. 또한, STEM의 **해석 가능성을 더욱 향상시키기 위한 연구**도 필요합니다.

## 실무 적용 가이드

STEM을 실무에 적용할 때는 **구현의 복잡성을 고려**해야 합니다. **효율적인 CPU 오프로딩과 비동기 프리페치**를 위해, **적절한 하드웨어 환경**을 갖추는 것이 중요합니다. 또한, STEM의 **임베딩 테이블**을 적절히 설계하여 **모델의 성능을 최적화**하는 것이 필요합니다.

## 결론

STEM은 Transformer 모델의 **파라미터 메모리를 확장하는 효과적인 방법**으로, **해석 가능성과 학습 안정성을 제공하며 효율성을 개선**합니다. 다양한 벤치마크에서의 성능 향상은 STEM의 **실용적 가치를 입증**합니다.

## 참고 자료

- [논문 링크](https://arxiv.org/abs/2601.10639)
- [코드 저장소](https://github.com/your-repo/STEM)
- 관련 자료: 관련 논문 및 연구 자료 링크