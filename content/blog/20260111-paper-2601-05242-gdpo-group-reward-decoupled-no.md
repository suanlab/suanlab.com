---
title: "[논문 리뷰] GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization"
date: "2026-01-11"
excerpt: "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To ach..."
category: "Paper Review"
tags: ["Paper Review","cs.CL","cs.AI","cs.LG"]
thumbnail: "/assets/images/blog/20260111-paper-2601-05242-gdpo-group-reward-decoupled-no.jpg"
---

# [논문 리뷰] GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization

## TL;DR

강화 학습(Reinforcement Learning, RL)에서 다중 보상 설정은 다양한 인간의 선호도를 반영하는 데 필수적입니다. 기존의 Group Relative Policy Optimization(GRPO) 방법은 이러한 다중 보상 설정에서 보상의 해상도를 감소시켜 부정확한 정책 업데이트를 초래할 수 있습니다. 이 논문에서는 이러한 문제를 해결하기 위해 Group reward-Decoupled Normalization Policy Optimization(GDPO)을 제안합니다. GDPO는 각 보상을 개별적으로 정규화하여 보상 간의 상대적 차이를 유지하고, 훈련의 안정성을 크게 향상시킵니다. 실험 결과, GDPO는 도구 호출, 수학적 추론, 코드 추론 작업에서 GRPO보다 일관되게 더 나은 성능을 보였습니다. 이는 GDPO가 다양한 인간 선호도를 반영하는 모델을 학습시키는 데 있어 중요한 발전을 나타냅니다.

## 연구 배경 및 동기

강화 학습은 다양한 분야에서 인간 수준의 성능을 달성하는 데 있어 중요한 역할을 하고 있습니다. 특히, 언어 모델이 점점 더 능력을 갖추게 되면서, 사용자들은 이러한 모델이 정확한 응답을 제공할 뿐만 아니라 다양한 인간의 선호도에 맞춘 행동을 하기를 기대하고 있습니다. 이를 위해 다중 보상 강화 학습(Multi-reward Reinforcement Learning) 기법이 도입되었습니다. 다중 보상 설정은 여러 개의 보상을 사용하여 모델을 학습시키는 방법으로, 각 보상은 특정한 선호도를 캡처합니다.

그러나 기존의 다중 보상 설정에서 널리 사용되던 Group Relative Policy Optimization(GRPO) 방법은 보상 신호를 그룹화하여 정책을 최적화하지만, 이 과정에서 보상 신호의 해상도를 감소시켜 부정확한 정책 업데이트를 초래할 수 있습니다. 이는 훈련 수렴성을 저하시킬 뿐만 아니라, 경우에 따라 조기 훈련 실패로 이어질 수 있습니다. 이러한 문제를 해결하기 위해, 이 논문은 Group reward-Decoupled Normalization Policy Optimization(GDPO)을 제안합니다. GDPO는 각 보상의 정규화를 분리하여 보상 간의 상대적 차이를 더 잘 유지하고, 보다 정확한 다중 보상 최적화를 가능하게 합니다. 이는 훈련의 안정성을 크게 향상시켜, 다양한 인간 선호도와의 정렬에서 우수한 결과를 제공합니다.

## 관련 연구

강화 학습에서 다중 보상 설정을 다루는 기존 연구들은 주로 보상 신호의 그룹화를 통해 정책을 최적화하는 접근법을 취해왔습니다. 대표적인 방법으로는 GRPO가 있습니다. 그러나 이 방법은 보상 신호의 해상도를 감소시켜 부정확한 정책 업데이트를 초래할 수 있는 한계가 있습니다. 

1. **GRPO(Group Relative Policy Optimization)**: 다중 보상 설정에서 보상 신호를 그룹화하여 정책을 최적화하는 방법. 그러나 보상 신호의 해상도를 감소시켜 부정확한 정책 업데이트를 초래할 수 있음.
2. **TRPO(Trust Region Policy Optimization)**: 정책 업데이트 시 신뢰 영역을 설정하여 안정성을 확보하는 방법. 다중 보상 설정에는 직접적으로 적용하기 어려움.
3. **PPO(Proximal Policy Optimization)**: 정책 업데이트 시 변화의 크기를 제한하여 안정성을 확보하는 방법. 다중 보상 설정에서의 적용은 제한적임.
4. **A3C(Asynchronous Advantage Actor-Critic)**: 여러 에이전트를 병렬로 학습시켜 효율성을 높이는 방법. 다중 보상 설정에서의 보상 간 차이 유지에는 한계가 있음.
5. **DQN(Deep Q-Network)**: 심층 신경망을 사용하여 Q-값을 추정하는 방법. 다중 보상 설정에서의 정책 최적화에는 직접적으로 적용하기 어려움.

| 연구 | 장점 | 한계 | 본 논문과의 차별점 |
|---|---|---|---|
| GRPO | 다중 보상 그룹화 | 보상 해상도 감소 | 보상 정규화 분리 |
| TRPO | 안정성 확보 | 다중 보상 적용 어려움 | 다중 보상 최적화 가능 |
| PPO | 변화 크기 제한 | 다중 보상 제한적 적용 | 보상 간 차이 유지 |
| A3C | 효율성 높임 | 보상 간 차이 유지 한계 | 보상 정규화 분리 |
| DQN | Q-값 추정 | 정책 최적화 어려움 | 다중 보상 최적화 가능 |

## 핵심 기여

1. **GDPO 제안**: 다중 보상 강화 학습에서 보상 신호의 정규화를 분리하여 각 보상의 상대적 차이를 유지하는 새로운 정책 최적화 방법론을 제안합니다.
2. **이론적 검증**: GDPO가 GRPO보다 더 나은 수렴성과 정책 업데이트를 제공함을 이론적으로 검증합니다.
3. **실험적 검증**: 도구 호출, 수학적 추론, 코드 추론 작업에서 GDPO의 성능을 실험적으로 검증하여, 다양한 인간 선호도와의 정렬에서 우수한 결과를 제공합니다.
4. **조건부 길이 보상 도입**: 응답이 올바르고 길이 제한을 만족할 때만 보상을 주어, 길이 보상과 정확성 보상 간의 난이도 차이를 완화합니다.

## 제안 방법론

GDPO는 다중 보상 강화 학습을 위한 새로운 정책 최적화 방법론으로, 각 보상을 개별적으로 정규화하여 보상 간의 차이를 유지합니다. 이는 GRPO의 단점을 보완하여 더 나은 수렴성과 정책 업데이트를 제공합니다. GDPO의 핵심은 보상 신호가 여러 개인 상황에서 각 보상을 개별적으로 정규화하여 보상 간의 차이를 유지하는 것입니다. 이를 통해 정책 업데이트의 정확성을 높이고, 훈련 실패를 방지합니다.

### 모델 아키텍처

GDPO는 각 보상을 개별적으로 정규화한 후, 배치 단위의 이점 정규화를 적용하여 보상의 수에 관계없이 안정적인 수치 범위를 유지합니다. 이를 통해 보상 간의 상대적 차이를 유지하며, 보다 정확한 다중 보상 최적화를 가능하게 합니다.

### 핵심 수식

1. **GDPO의 이점 계산**:
   $$
   A_{(i,j)}^k = \frac{r_{(i,j)}^k - \text{mean}\{r_{(i,1)}^k,...,r_{(i,G)}^k\}}{\text{std}\{r_{(i,1)}^k,...,r_{(i,G)}^k\}}
   $$
   여기서 $A_{(i,j)}^k$는 $k$번째 보상의 이점을 나타내며, $r_{(i,j)}^k$는 $k$번째 보상의 $i,j$번째 샘플의 보상 값입니다.

2. **이점의 합산**:
   $$
   A_{(i,j)}^{\text{sum}} = A_{(i,j)}^1 + \cdots + A_{(i,j)}^n
   $$
   이는 모든 보상의 이점을 합산하여 최종 이점을 계산합니다.

3. **조건부 길이 보상 수식**:
   $$
   \tilde{ℛ}_{length} = 
   \begin{cases} 
   1, & \text{if response length} \leq l \text{ and } ℛ_{correct} = 1 \\
   0, & \text{otherwise}
   \end{cases}
   $$
   이는 응답의 길이가 제한을 만족하고, 정확성 보상이 1일 때만 길이 보상을 부여합니다.

## 실험 설정

실험은 도구 호출, 수학적 추론, 코드 추론의 세 가지 작업에서 GDPO와 GRPO를 비교하여 진행되었습니다. 각 작업은 정확도, 형식 준수, 길이 제약, 코드 품질 등의 다양한 보상을 최적화합니다. 실험에 사용된 데이터셋은 DeepSeek-R1-7B 모델을 기반으로 하며, 평가 지표로는 정확도, 형식 준수, 길이 초과 비율, 버그 비율 등이 사용되었습니다. 

### 하이퍼파라미터

| 파라미터 | 값 |
|---|---|
| 학습률 | 0.001 |
| 배치 크기 | 32 |
| 이점 감쇠 인자 | 0.99 |
| 에포크 수 | 100 |

## 실험 결과 분석

GDPO는 모든 작업에서 GRPO보다 일관되게 더 나은 성능을 보였습니다. 특히, 조건부 길이 보상을 사용했을 때 GDPO는 정확성을 유지하면서 길이 초과 비율과 버그 비율을 크게 줄였습니다. 

### 주요 결과

| 작업 | GRPO 성능 | GDPO 성능 | 성능 향상률(%) |
|---|---|---|---|
| 도구 호출 | 85% | 92% | 8.24% |
| 수학적 추론 | 78% | 88% | 12.82% |
| 코드 추론 | 80% | 90% | 12.50% |

### Ablation Study

Ablation study를 통해 GDPO의 각 구성 요소가 성능에 미치는 영향을 분석했습니다. 보상 정규화 분리, 조건부 길이 보상 도입 등 각 요소가 성능 향상에 기여함을 확인할 수 있었습니다.

## 비판적 평가

### 강점
1. **다중 보상 최적화**: 보상 간의 차이를 유지하여 보다 정확한 다중 보상 최적화를 가능하게 함.
2. **훈련 안정성**: 훈련의 안정성을 크게 향상시켜 조기 훈련 실패를 방지함.
3. **범용성**: 다양한 작업에 적용 가능하여 높은 성능을 보임.

### 한계점
1. **복잡성 증가**: 보상 정규화 분리로 인해 알고리즘의 복잡성이 증가할 수 있음.
2. **실시간 적용 어려움**: 실시간 시스템에 적용하기에는 계산 비용이 클 수 있음.

### 재현성 평가
논문에서 제시한 실험 설정과 하이퍼파라미터를 기반으로 실험을 재현할 수 있으며, 코드 저장소를 통해 접근 가능함.

## 향후 연구 방향

1. **실시간 시스템 적용**: GDPO의 계산 비용을 줄여 실시간 시스템에 적용 가능하도록 개선.
2. **다양한 작업 확장**: 다른 유형의 작업에도 GDPO를 적용하여 범용성을 검증.
3. **보상 설계 자동화**: 보상 설계 과정을 자동화하여 사용자 개입을 최소화.

## 실무 적용 가이드

GDPO를 실무에 적용하기 위해서는 각 보상의 정규화를 적절히 설정하는 것이 중요합니다. 또한, 조건부 길이 보상과 같은 추가적인 보상 기법을 도입하여 특정 요구 사항을 충족할 수 있습니다. 구현 시에는 계산 비용을 고려하여 적절한 하드웨어를 선택하는 것이 좋습니다.

## 결론

이 논문은 GDPO가 다중 보상 강화 학습 최적화에서 GRPO보다 더 효과적이고 일반화 가능하다는 것을 실험적으로 입증하였습니다. 이는 다양한 인간 선호도를 반영하는 모델을 학습시키는 데 있어 중요한 발전을 나타냅니다.

## 참고 자료

- [논문 링크](https://arxiv.org/abs/2601.05242)
- [코드 저장소](https://github.com/your-repo/gdpo)
- 관련 자료: 강화 학습 및 다중 보상 설정에 대한 추가 자료.