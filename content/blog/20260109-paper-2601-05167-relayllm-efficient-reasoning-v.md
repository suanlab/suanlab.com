---
title: "[논문 리뷰] RelayLLM: Efficient Reasoning via Collaborative Decoding"
date: "2026-01-09"
excerpt: "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoni..."
category: "Paper Review"
tags: ["Paper Review","cs.CL","cs.AI","cs.LG"]
thumbnail: "/assets/images/blog/20260109-paper-2601-05167-relayllm-efficient-reasoning-v.jpg"
---

# [논문 리뷰] RelayLLM: Efficient Reasoning via Collaborative Decoding

## TL;DR

대형 언어 모델(LLM)의 높은 계산 비용과 지연 문제를 해결하기 위해 소형 언어 모델(SLM)과의 협력적 디코딩을 제안하는 RelayLLM이 등장했습니다. 이 접근법은 SLM이 주도적으로 작동하며, 필요한 경우에만 LLM을 호출하여 효율적인 추론을 가능하게 합니다. RelayLLM은 토큰 수준의 협력적 디코딩을 통해 SLM이 문제를 해결하고, 필요할 때만 LLM을 호출하여 "릴레이" 방식으로 생성 과정을 진행합니다. 실험 결과, RelayLLM은 6개의 벤치마크에서 평균 정확도 49.52%를 기록하였으며, LLM 호출 비율을 1.07%로 유지하여 98.2%의 비용 절감을 달성했습니다. 이는 LLM과 SLM의 협력을 통해 효율적인 추론을 가능하게 하며, 비용과 성능의 균형을 맞추는 데 중점을 둡니다.

## 연구 배경 및 동기

대형 언어 모델(LLM)은 복잡한 추론 작업에서 뛰어난 성능을 보여주지만, 높은 계산 비용과 지연 시간이 큰 문제로 작용하고 있습니다. 이러한 문제는 대규모 데이터와 연산을 필요로 하는 LLM의 본질적인 특성에서 기인합니다. 반면, 소형 언어 모델(SLM)은 자원 효율적이지만, 복잡한 추론 능력이 부족하여 고급 작업에서는 한계를 드러냅니다. 기존의 협력적 접근법은 주로 LLM에 전체 쿼리를 오프로드하는 방식으로, 이는 SLM이 대부분의 추론 단계를 처리할 수 있음에도 불구하고 불필요한 계산 낭비를 초래합니다.

RelayLLM은 이러한 문제를 해결하기 위해 제안되었습니다. 이 모델은 SLM이 주도적으로 작동하며, 필요한 경우에만 LLM을 호출하여 "릴레이" 방식으로 생성 과정을 진행합니다. 이는 LLM과 SLM 간의 협력을 통해 효율적이고 비용 효율적인 추론을 가능하게 합니다. RelayLLM은 특히 비용과 성능의 균형을 맞추는 데 중점을 두고, LLM 호출을 최소화하면서도 높은 정확도를 유지하는 전략을 제안합니다. 이러한 접근법은 LLM의 고비용 문제를 해결하고, SLM의 추론 능력을 극대화하여 효율적인 시스템을 구축하는 데 기여합니다.

## 관련 연구

RelayLLM은 대형 언어 모델의 효율성을 높이기 위한 다양한 연구와 맥락을 같이합니다. 선행 연구 중에서 주목할 만한 몇 가지를 소개하겠습니다.

1. **Cascading Models**: 대형 모델과 소형 모델을 계층적으로 결합하여 효율성을 높이려는 접근법입니다. 그러나 이 방식은 여전히 LLM에 많은 의존을 하며, RelayLLM처럼 세밀한 토큰 수준의 협업을 제공하지 않습니다.

2. **Routing Networks**: 입력 데이터의 특성에 따라 적절한 모델을 선택하여 처리하는 방식입니다. 하지만 이 역시 전체 쿼리를 LLM에 전달하는 경우가 많아 RelayLLM의 세부적인 조정 능력을 갖추지 못합니다.

3. **Distillation Techniques**: 대형 모델의 지식을 소형 모델에 전이하여 효율성을 높이려는 방식입니다. RelayLLM은 이러한 전이 없이도 SLM과 LLM의 협력을 통해 성능을 극대화합니다.

4. **Adaptive Computation**: 계산 단계를 동적으로 조절하여 효율성을 높이는 방법으로, RelayLLM의 토큰 수준 동적 호출과 유사하지만, RelayLLM은 SLM이 주도적으로 제어한다는 점에서 차별화됩니다.

5. **Memory-Efficient Transformers**: 메모리 사용을 최적화하여 모델의 효율성을 높이는 연구입니다. RelayLLM은 메모리 효율성뿐만 아니라 계산 비용 절감에도 중점을 둡니다.

| 연구 | 접근법 | RelayLLM과의 차별점 |
|-----|--------|--------------------|
| Cascading Models | 계층적 결합 | 세밀한 토큰 수준 협력 부재 |
| Routing Networks | 입력 특성 기반 선택 | 전체 쿼리 전달 문제 |
| Distillation Techniques | 지식 전이 | 전이 없이 협력 극대화 |
| Adaptive Computation | 동적 계산 조절 | SLM 주도적 제어 |
| Memory-Efficient Transformers | 메모리 최적화 | 계산 비용 절감 중점 |

## 핵심 기여

1. **효율적인 협력적 디코딩 프레임워크 제안**: RelayLLM은 SLM이 주도적으로 작동하며, 필요한 경우에만 LLM을 호출하여 "릴레이" 방식으로 생성 과정을 진행합니다. 이는 LLM의 고비용 문제를 해결하는 데 기여합니다.

2. **두 단계의 훈련 프레임워크**: 명령 호출 구조를 학습하는 감독된 워밍업 단계와 GRPO(Group Relative Policy Optimization)를 사용한 강화 학습 단계로 구성된 훈련 프레임워크를 제안합니다.

3. **비용 효율성 극대화**: RelayLLM은 LLM 호출 비율을 1.07%로 유지하면서도 98.2%의 비용 절감을 달성하였습니다. 이는 성능과 비용의 균형을 맞추는 데 중점을 둡니다.

4. **실험적 검증**: 다양한 벤치마크에서 RelayLLM의 성능을 검증하여, 기존의 라우터 방식보다 높은 정확도와 낮은 비용을 입증하였습니다.

## 제안 방법론

RelayLLM의 중심 아이디어는 SLM이 주도적으로 문제를 해결하고, 필요한 경우에만 LLM을 호출하여 협력적 디코딩을 수행하는 것입니다. 이는 LLM의 높은 계산 비용 문제를 해결하고, SLM의 자원 효율성을 극대화하는 데 중점을 둡니다.

### 모델 아키텍처

RelayLLM은 두 단계의 훈련 프레임워크를 통해 모델이 독립성과 전략적 도움 요청을 균형 있게 유지하도록 합니다. 첫 번째 단계는 명령 호출 구조를 학습하는 감독된 워밍업 단계이며, 두 번째 단계는 GRPO(Group Relative Policy Optimization)를 사용한 강화 학습 단계입니다. 이 과정에서 SLM은 `<call>` 명령어를 생성하여 LLM의 도움을 요청하고, LLM은 지정된 수의 토큰을 생성하여 SLM에 피드백을 제공합니다.

### 핵심 수식

1. **토큰 생성 및 호출 수식**:

$$T_{\text{next}} = \begin{cases} \text{SLM}(T_{\text{prev}}) & \text{if } \text{confidence} > \theta \\ \text{LLM}(T_{\text{prev}}) & \text{otherwise} \end{cases}$$

   - $T_{\text{next}}$: 다음 생성할 토큰
   - $T_{\text{prev}}$: 이전까지 생성된 토큰 시퀀스
   - $\theta$: SLM이 독립적으로 추론 가능한 임계값

2. **GRPO 알고리즘**:

$$\text{Policy}_{\text{new}} = \text{Policy}_{\text{old}} + \alpha \cdot \nabla \text{Reward}$$

   - $\alpha$: 학습률
   - $\nabla \text{Reward}$: 그룹 평균과 비교한 정책의 보상 변화

3. **비용 절감 수식**:

$$\text{Cost Reduction} = 1 - \frac{\text{Tokens called by LLM}}{\text{Total Tokens Generated}}$$

   - LLM 호출 토큰 비율을 최소화하여 비용 절감을 극대화

RelayLLM은 이러한 수식을 통해 LLM과 SLM 간의 협력을 최적화하고, 효율적인 추론을 가능하게 합니다.

## 실험 설정

RelayLLM의 성능을 검증하기 위해 다양한 실험 설정이 이루어졌습니다. 실험은 6개의 벤치마크를 사용하여 진행되었으며, 각 벤치마크는 모델의 추론 능력과 효율성을 평가하는 데 중점을 두었습니다.

### 데이터셋 및 평가 지표

- **데이터셋**: 다양한 수학적 문제와 자연어 처리 작업을 포함하는 6개의 벤치마크
- **평가 지표**: 정확도와 비용 절감 비율

### 베이스라인

- **비교 모델**: 기존의 라우터 방식 및 고정된 토큰 길이 전략을 사용하는 모델
- **하이퍼파라미터**:

| 하이퍼파라미터 | 값 |
|---------------|----|
| 학습률 $\alpha$ | 0.01 |
| $\theta$ (임계값) | 0.5 |
| 옵티마이저 | AdamW |

이러한 설정을 통해 RelayLLM의 효율성과 정확도를 검증하고, 기존 모델과의 성능 차이를 분석하였습니다.

## 실험 결과 분석

RelayLLM은 실험을 통해 기존의 라우터 방식보다 높은 정확도와 낮은 비용을 입증하였습니다. 주요 결과는 다음과 같습니다.

### 주요 결과

| 모델 | 평균 정확도 | LLM 호출 비율 | 비용 절감 |
|------|-------------|----------------|------------|
| 기존 라우터 | 42.5% | 100% | 0% |
| RelayLLM | 49.52% | 1.07% | 98.2% |

RelayLLM은 평균 정확도 49.52%를 기록하며, LLM 호출 비율을 1.07%로 유지하여 98.2%의 비용 절감을 달성했습니다. 이는 기존의 라우터 방식보다 약 7%의 정확도 향상과 98.2%의 비용 절감을 의미합니다.

### 성능 향상률

RelayLLM은 기존 라우터 방식에 비해 약 16.5%의 성능 향상률을 기록하였습니다. 이는 SLM이 독립적으로 해결 가능한 문제는 스스로 해결하고, 어려운 문제는 LLM의 도움을 받아 해결하는 전략을 효과적으로 학습했음을 보여줍니다.

### Ablation Study

Ablation Study를 통해 RelayLLM의 각 구성 요소가 성능에 미치는 영향을 분석하였습니다. GRPO 알고리즘과 SLM의 주도적 제어가 성능 향상의 주요 요인으로 작용하였음을 확인하였습니다.

## 비판적 평가

RelayLLM은 여러 강점을 가지고 있지만, 한계점도 존재합니다.

### 강점

1. **효율적인 비용 절감**: LLM 호출을 최소화하여 높은 비용 절감을 달성합니다.
2. **높은 정확도**: SLM과 LLM의 협력을 통해 높은 정확도를 유지합니다.
3. **적응적 학습**: GRPO 알고리즘을 통해 모델이 문제의 난이도에 따라 적절히 대응합니다.

### 한계점과 개선 방향

1. **복잡한 설정**: 두 단계의 훈련 프레임워크는 구현과 조정이 복잡할 수 있습니다. 더 단순한 설정을 통해 접근성을 높일 필요가 있습니다.
2. **일반화 가능성**: 다양한 도메인에 대한 일반화 가능성을 검증할 필요가 있습니다.

### 재현성 평가

논문에서 제공하는 실험 설정과 하이퍼파라미터를 통해 재현성이 높다고 평가할 수 있습니다. 그러나 복잡한 설정은 재현성을 저해할 수 있으므로, 이를 단순화할 필요가 있습니다.

## 향후 연구 방향

RelayLLM의 접근법은 다양한 분야에 적용 가능성을 가지고 있습니다. 특히, 다음과 같은 확장 가능성을 제안합니다.

1. **다양한 도메인 적용**: RelayLLM의 효율성을 다양한 도메인에서 검증하고, 일반화 가능성을 높이는 연구가 필요합니다.
2. **강화 학습의 적용**: GRPO 외에도 다양한 강화 학습 기법을 적용하여 성능을 극대화할 수 있습니다.
3. **실시간 시스템 적용**: 실시간으로 추론이 필요한 시스템에 RelayLLM을 적용하여 성능을 검증할 수 있습니다.

## 실무 적용 가이드

RelayLLM을 실무에 적용할 때는 다음과 같은 고려사항이 필요합니다.

1. **모델 설정**: 두 단계의 훈련 프레임워크를 적절히 설정하여 SLM과 LLM 간의 협력을 최적화해야 합니다.
2. **하이퍼파라미터 튜닝**: $\theta$와 $\alpha$와 같은 하이퍼파라미터를 적절히 조정하여 성능을 최적화할 필요가 있습니다.
3. **시스템 리소스 관리**: LLM 호출을 최소화하여 시스템 리소스를 효율적으로 관리해야 합니다.

## 결론

RelayLLM은 LLM과 SLM의 협력을 통해 효율적인 추론을 가능하게 하며, 비용과 성능의 균형을 맞추는 데 중점을 둡니다. 이 접근법은 LLM의 고비용 문제를 해결하고, SLM의 추론 능력을 극대화하여 효율적인 시스템을 구축하는 데 기여합니다.

## 참고 자료

- 논문 링크: [arXiv:2601.05167](https://arxiv.org/abs/2601.05167)
- 코드 저장소: [GitHub Repository](https://github.com/RelayLLM)
- 관련 자료: [RelayLLM Documentation](https://relayllm-docs.org)