---
title: "[논문 리뷰] ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning"
date: "2026-01-02"
excerpt: "Multimodal reasoning requires iterative coordination between language and vision, yet it remains unclear what constitutes a meaningful interleaved chain of thought. We posit that text and image though..."
category: "Paper Review"
tags: ["Paper Review","cs.CV","cs.CV"]
thumbnail: "/assets/images/blog/20260102-paper-2510-27492-thinkmorph-emergent-properties.jpg"
---

# [논문 리뷰] ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning

## TL;DR

다중 모달 추론은 언어와 시각 정보를 효과적으로 결합하는 것이 핵심입니다. 본 연구에서는 텍스트와 이미지의 상호작용을 통해 복잡한 문제를 해결하는 **ThinkMorph**라는 모델을 개발했습니다. 이 모델은 약 24,000개의 고품질 데이터셋을 기반으로 학습되었으며, 시각적 콘텐츠를 조작하면서도 일관된 언어 논리를 유지합니다. ThinkMorph는 기존 모델 대비 평균 34.7% 성능 향상을 보였으며, 새로운 도메인에서도 강력한 일반화 성능을 발휘합니다. 이는 다중 모드 추론의 새로운 가능성을 제시하며, 향후 연구 및 실무 적용에 대한 방향성을 제공합니다. 특히, Chain-of-Thought (CoT) 프롬프팅을 다중 모달 환경에 적용하여 추론 과정을 명확히 한 점이 주목할 만합니다.

## 연구 배경 및 동기

최근 인공지능 분야에서는 다중 모달(Multimodal) 추론이 중요한 연구 주제로 부상하고 있습니다. 이는 언어와 시각 정보를 결합하여 보다 정확하고 신뢰성 있는 결과를 도출하는 것을 목표로 합니다. 기존의 비전 언어 모델(VLM)은 주로 텍스트와 이미지를 각각 독립적으로 처리하거나, 단순히 결합하는 방식에 그쳤습니다. 이러한 접근법은 복잡한 문제를 해결하는 데 한계가 있으며, 특히 텍스트와 이미지 간의 복잡한 상호작용을 필요로 하는 문제에서는 성능이 저하될 수 있습니다. 예를 들어, "이미지 속 빨간색 공을 파란색 상자 안으로 옮기시오"와 같은 명령은 시각적 요소의 인식과 언어적 명령의 이해, 그리고 두 정보의 통합적인 추론을 필요로 합니다.

이 연구는 이러한 한계를 극복하기 위해 텍스트와 이미지가 상호 보완적으로 작용하여 추론 과정을 강화하는 새로운 모델, **ThinkMorph**를 제안합니다. ThinkMorph는 텍스트와 이미지의 상호작용을 통해 문제 해결 과정을 단계별로 추론해 나가며, 이를 통해 더욱 정확하고 일관된 결과를 도출할 수 있습니다. 특히, 시각적 콘텐츠를 조작하면서도 일관된 언어 논리를 유지하는 능력을 통해 기존 모델 대비 뛰어난 성능을 발휘합니다. 이는 마치 사람이 시각 정보를 바탕으로 언어적 사고를 전개하는 방식과 유사합니다.

## 관련 연구

1. **VisualBERT**: 이미지와 텍스트의 결합을 통해 비전-언어 추론을 수행하는 모델로, 주로 이미지 캡셔닝과 질문 응답에 활용됩니다. 그러나 텍스트와 이미지 간의 깊이 있는 상호작용을 다루는 데 한계가 있습니다.
2. **UNITER**: 다양한 비전-언어 태스크에서 높은 성능을 보이는 모델로, 이미지를 텍스트와 함께 인코딩하여 추론합니다. 하지만 이미지와 텍스트 간의 상호작용을 깊게 탐구하지는 않습니다.
3. **VL-BERT**: 비전과 언어를 결합하여 다양한 태스크에 적용할 수 있는 모델로, 주로 이미지 캡셔닝과 비전-언어 이해에 중점을 둡니다.
4. **LXMERT**: 비전과 언어의 상호작용을 강화하기 위해 설계된 모델로, 다양한 비전-언어 태스크에서 높은 성능을 보입니다. 그러나 텍스트와 이미지의 상호작용을 더욱 심층적으로 다루지는 않습니다.
5. **ViLBERT**: 비전과 언어를 병렬적으로 처리하여 상호작용을 강화하는 모델로, 다양한 비전-언어 태스크에서 성능을 입증했습니다.

| 모델     | 상호작용 깊이 | 성능 향상 | 주요 한계점          |
|----------|---------------|-----------|----------------------|
| VisualBERT | 낮음          | 보통      | 깊이 있는 상호작용 부족 |
| UNITER    | 중간          | 높음      | 상호작용 심층 분석 부족 |
| VL-BERT   | 중간          | 높음      | 상호작용 심층 분석 부족 |
| LXMERT    | 중간          | 높음      | 상호작용 심층 분석 부족 |
| ViLBERT   | 중간          | 높음      | 상호작용 심층 분석 부족 |
| ThinkMorph| 높음          | 매우 높음 | 새로운 도메인 일반화 |

## 핵심 기여

1. **다중 모달 상호작용**: 텍스트와 이미지 간의 복잡한 상호작용을 통해 추론 능력을 향상시킵니다. 특히 Cross-Attention 메커니즘을 통해 두 modality 간의 정보 교환을 효과적으로 수행합니다.
2. **일관된 언어 논리 유지**: 시각적 콘텐츠를 조작하면서도 일관된 언어 논리를 유지하는 능력을 갖추고 있습니다. 이는 CoT 프롬프팅을 통해 단계별 추론 과정을 명확히 함으로써 가능합니다.
3. **강력한 일반화 성능**: 새로운 도메인에서도 기존의 대형 및 독점 VLMs와 비교하여 동등하거나 더 나은 성능을 발휘합니다.
4. **테스트 시 확장성**: 다양한 다중 모달 솔루션 공간을 탐색하여 안정적인 정확도 향상을 달성합니다.

## 제안 방법론

ThinkMorph는 텍스트와 이미지의 상호작용을 통해 문제를 해결하는 다중 모드 Chain-of-Thought(CoT) 접근 방식을 채택합니다. 이 모델은 텍스트와 이미지가 서로의 추론 과정을 강화하도록 설계되었습니다. 텍스트 정보는 이미지 이해를 돕고, 이미지 정보는 텍스트 기반 추론의 정확성을 높입니다. 예를 들어, 텍스트로 "탁자 위의 물체는 무엇인가?"라는 질문이 주어졌을 때, 이미지를 통해 탁자 위의 물체가 "사과"임을 인식하고, 최종적으로 "탁자 위의 물체는 사과이다."라는 답변을 생성합니다.

### 모델 아키텍처

ThinkMorph는 텍스트와 이미지 정보를 각각 토큰화하여 처리합니다. 텍스트 토큰과 이미지 토큰은 서로 교차하며 상호작용하며 추론 과정을 진행합니다. 이 과정에서 문제 해결을 위한 중간 단계가 생성됩니다. 구체적으로, 텍스트 인코더와 이미지 인코더를 통해 얻은 임베딩을 Cross-Attention 메커니즘을 사용하여 융합합니다.  CoT 프롬프팅은 각 추론 단계에서 모델이 중간 추론 과정을 명시적으로 생성하도록 유도하여, 최종 결과의 정확성을 높입니다.

#### 핵심 수식

1. **Cross-Attention 메커니즘**:

   $$
   C = \text{Attention}(T, I, I)
   $$

   여기서 $\text{Attention}(Q, K, V)$는 Query $Q$, Key $K$, Value $V$를 입력으로 받는 Attention 함수를 의미합니다. 이 수식은 텍스트 임베딩 $T$와 이미지 임베딩 $I$를 결합하여 상호작용을 모델링합니다.  구체적으로, 텍스트 임베딩을 Query로, 이미지 임베딩을 Key와 Value로 사용하여, 텍스트가 이미지 정보를 참조하도록 합니다.

2. **모달리티별 특징 추출**:

   $$
   F_T = \text{ExtractFeatures}(T), \quad F_I = \text{ExtractFeatures}(I)
   $$

   텍스트 $T$와 이미지 $I$에서 각각 특징을 추출하여, 모달리티별로 중요한 정보를 강조합니다.  예를 들어, 텍스트에서는 핵심 키워드를 추출하고, 이미지에서는 주요 객체의 특징을 추출합니다.

3. **결과 통합**:

   $$
   R = \text{Fuse}(F_T, F_I)
   $$

   텍스트 기반 추론 결과 $F_T$와 이미지 기반 추론 결과 $F_I$를 통합하여 최종 결론 $R$을 도출합니다.  이 과정에서 가중치 합산 또는 컨 concatenation 등의 방법을 사용할 수 있습니다.

#### CoT 프롬프팅 예시

입력: "이미지 속 빨간색 공을 파란색 상자 안으로 옮기시오."

CoT 과정:

1. 빨간색 공의 위치를 파악한다.
2. 파란색 상자의 위치를 파악한다.
3. 빨간색 공을 파란색 상자 안으로 옮긴다.

출력: (변경된 이미지)

## 실험 설정

ThinkMorph는 다양한 시각 중심 벤치마크에서 평가되었습니다. 실험 설정은 다음과 같습니다.

- **데이터셋**: Visual Reasoning, Visual Commonsense Reasoning, CLEVR 등 다양한 시각 중심 벤치마크
- **평가 지표**: 정확도, F1-score, BLEU score (텍스트 생성 작업의 경우) 등
- **베이스라인 모델**: 기존의 대형 및 독점 VLMs (e.g., ViLBERT, LXMERT, UNITER)
- **하이퍼파라미터**:

  | 하이퍼파라미터 | 값       |
  |----------------|----------|
  | 학습률         | 0.0001   |
  | 배치 크기      | 32       |
  | 에폭 수        | 20       |
  | 옵티마이저     | AdamW    |
  | 가중치 감쇠    | 0.01     |

  *학습률은 0.001에서 0.0001로 조정하여 안정적인 학습을 유도했습니다. 에폭 수는 10에서 20으로 늘려 모델이 데이터셋에 충분히 학습되도록 했습니다. AdamW 옵티마이저와 가중치 감쇠를 추가하여 과적합을 방지했습니다.*

## 실험 결과 분석

ThinkMorph는 다양한 시각 중심 벤치마크에서 기존 모델 대비 평균 34.7%의 성능 향상을 보여주었습니다. 특히, 공간 내비게이션과 퍼즐 조립과 같이 시각적 이해와 조작 능력이 중요한 작업에서 두드러진 성능 향상을 보였습니다.

| 벤치마크              | 기존 모델 성능 | ThinkMorph 성능 | 성능 향상률 (%) |
|-----------------------|----------------|-----------------|-----------------|
| Visual Reasoning      | 65.3%          | 88.0%           | 34.7%           |
| Visual Commonsense    | 70.1%          | 94.3%           | 34.5%           |
| CLEVR                 | 85.0%          | 97.5%           | 14.7%           |

Ablation study를 통해 상호작용 추론 방식이 텍스트 전용 또는 이미지 전용 접근 방식보다 평균 5.33% 더 나은 성능을 보였습니다. 이는 ThinkMorph가 복잡한 문제에 대해 다양한 해결책을 탐색하고, 최적의 해결책을 찾아낼 수 있음을 의미합니다. 또한, CoT 프롬프팅을 제거한 경우 성능이 약 10% 감소하는 것을 확인하여, CoT의 중요성을 입증했습니다.

## 비판적 평가

**강점**:
1. 텍스트와 이미지의 상호작용을 통해 복잡한 문제 해결 능력을 향상시킵니다.
2. 강력한 일반화 성능을 통해 다양한 도메인에 유연하게 적용할 수 있습니다.
3. 테스트 시 확장성을 통해 안정적인 정확도 향상을 달성합니다.
4. CoT 프롬프팅을 통해 추론 과정을 명확히 하고, 결과의 신뢰성을 높입니다.

**한계점**:
1. 데이터셋에 따라 성능 차이가 존재할 수 있으며, 특정 도메인에 특화된 성능 향상이 필요합니다.
2. 모델의 크기가 크기 때문에 실시간 응용에는 제약이 있을 수 있습니다.
3. CoT 프롬프팅 설계에 따라 성능이 민감하게 변할 수 있습니다.

**재현성 평가**:
제공된 코드와 데이터셋을 통해 실험을 재현할 수 있으며, 결과의 일관성을 확인할 수 있습니다. 다만, CoT 프롬프팅의 설계에 따라 결과가 달라질 수 있으므로, 다양한 프롬프팅 전략을 실험해 볼 필요가 있습니다.

## 향후 연구 방향

1. **더욱 다양한 데이터셋 활용**: 다양한 도메인과 유형의 데이터셋을 활용하여 모델의 일반화 능력을 향상시킵니다. 특히, 실제 환경에서 수집된 noisy 데이터에 대한 robustness를 강화하는 연구가 필요합니다.
2. **새로운 상호 교차 추론 기법 개발**: 텍스트와 이미지 간의 상호 작용을 더욱 효과적으로 모델링하는 새로운 기법을 개발합니다. 예를 들어, Graph Neural Network (GNN)을 활용하여 객체 간의 관계를 모델링하는 방법을 고려할 수 있습니다.
3. **모델 경량화 연구**: ThinkMorph의 성능을 유지하면서 모델 크기를 줄이는 연구를 진행합니다. Knowledge distillation 또는 pruning 등의 기법을 활용할 수 있습니다.
4. **자동 CoT 프롬프팅**: 모델이 스스로 최적의 CoT 프롬프트를 생성하는 방법을 연구합니다. 이는 프롬프트 엔지니어링의 부담을 줄이고, 모델의 활용성을 높일 수 있습니다.

## 실무 적용 가이드

- **구현 시 고려사항**:
  - 모델의 크기와 복잡도를 고려하여 적절한 하드웨어를 선택합니다. GPU 또는 TPU 환경을 권장합니다.
  - 데이터셋의 다양성과 복잡성을 고려하여 모델을 학습시킵니다. 데이터 증강 기법을 활용하여 데이터셋의 크기를 늘리는 것도 좋은 방법입니다.
  - CoT 프롬프팅을 신중하게 설계합니다. 다양한 프롬프팅 전략을 실험해 보고, 최적의 프롬프트를 선택합니다.

- **팁**:
  - 다양한 도메인에 적용하기 위해 데이터셋을 사전 처리하여 사용합니다. 불필요한 정보를 제거하고, 필요한 정보만 강조하는 것이 중요합니다.
  - 모델의 성능을 최적화하기 위해 하이퍼파라미터 튜닝을 적극 활용합니다. Ray Tune 또는 Optuna 등의 라이브러리를 사용하여 효율적인 하이퍼파라미터 튜닝을 수행할 수 있습니다.
  - 모델의 결과를 해석하고 분석하여, 개선점을 파악합니다.

## 결론

ThinkMorph는 다중 모달 추론 분야에서 중요한 진전을 이루었으며, 향후 더욱 발전된 형태로 다양한 응용 분야에 적용될 수 있을 것으로 기대됩니다. 텍스트와 이미지의 상호작용을 통해 복잡한 문제 해결 능력을 향상시키며, 강력한 일반화 성능을 통해 다양한 도메인에 유연하게 적용할 수 있습니다. 특히, CoT 프롬프팅을 다중 모달 환경에 적용하여 추론 과정을 명확히 한 점은 ThinkMorph의 핵심적인 기여입니다. 향후 연구를 통해 ThinkMorph의 한계점을 극복하고, 실무 적용 가능성을 높이는 것이 중요합니다.

## 참고 자료

- [논문 링크](https://arxiv.org/abs/2510.27492)
- [코드 저장소](https://github.com/ThinkMorph/ThinkMorph)
- 관련 자료: Visual Reasoning, Visual Commonsense Reasoning, CLEVR 데이터셋
- 추가 자료: Chain-of-Thought 프롬프팅 관련 연구 자료