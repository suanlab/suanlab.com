---
title: "[논문 리뷰] Aligning Agentic World Models via Knowledgeable Experience Learning"
date: "2026-01-22"
excerpt: "Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Co..."
category: "Paper Review"
tags: ["Paper Review","cs.CL","cs.AI","cs.CV"]
thumbnail: "/assets/images/blog/20260122-paper-2601-13247-aligning-agentic-world-models-.jpg"
---

# [논문 리뷰] Aligning Agentic World Models via Knowledgeable Experience Learning

## TL;DR
현재 대형 언어 모델(LLMs)은 논리적 계획은 잘 수행하지만 물리적 세계의 법칙을 준수하는 데 필요한 절차적 이해가 부족합니다. 이를 해결하기 위해, WorldMind라는 프레임워크가 제안되었습니다. WorldMind는 에이전트가 환경에서 얻은 피드백을 통해 상징적 세계 지식 저장소를 구축하여 물리적 실행 가능성과 작업 정확성을 모두 만족하는 시뮬레이션을 생성합니다. 실험 결과, WorldMind는 기존 방법론에 비해 높은 성공률을 기록하며, 다양한 모델과 환경에서 뛰어난 전이 가능성을 보여주었습니다. 이 연구는 에이전트가 동적 환경에서 보다 효과적으로 작동할 수 있도록 돕는 데 의의가 있습니다.

## 연구 배경 및 동기
대형 언어 모델(LLMs)은 자연어 처리 분야에서 혁신적인 발전을 이루었지만, 물리적 세계의 불변 법칙을 이해하고 준수하는 데에는 한계가 있습니다. 이러한 한계는 특히 에이전트가 물리적 환경에서 행동을 계획하고 실행할 때 두드러집니다. 기존의 LLM 기반 에이전트는 논리적으로 타당한 계획을 세울 수 있지만, 실제 물리적 환경에서 실행 가능한지를 보장하지 못하는 경우가 많습니다. 이는 '물리적 환각'으로 이어지며, 에이전트가 비현실적인 행동 계획을 세우게 됩니다.

기존의 접근법은 주로 대규모의 데이터와 자원을 투입하여 모델을 훈련하거나 미세 조정(fine-tuning)하는 방식으로 이러한 문제를 해결하려 했습니다. 그러나 이러한 방법은 환경의 동적 변화를 반영하기 어렵고, 지속적인 재훈련이 필요해 비용이 많이 듭니다. 따라서, 에이전트가 환경의 피드백을 통해 스스로 학습하고 적응할 수 있는 방법론이 필요합니다.

이 연구는 이러한 문제를 해결하기 위해 WorldMind라는 새로운 프레임워크를 제안합니다. WorldMind는 에이전트가 환경에서 얻은 경험을 바탕으로 상징적 세계 지식 저장소를 구축하여, 물리적 실행 가능성과 작업 최적화를 모두 만족하는 행동을 계획할 수 있도록 합니다. 이를 통해 에이전트는 물리적 세계의 불변 법칙을 준수하면서도, 동적 환경에 적응할 수 있는 능력을 갖추게 됩니다.

## 관련 연구
본 연구는 LLM의 물리적 세계 이해 한계를 극복하기 위한 다양한 연구들과 관련이 있습니다. 먼저, **Goyal et al. (2022)**는 LLM의 물리적 세계 이해를 강화하기 위해 물리적 시뮬레이션 환경을 활용하는 방법을 제안했습니다. 그러나 이 방법은 시뮬레이션 환경의 복잡성에 따라 성능이 크게 달라지는 한계가 있습니다. **Brown et al. (2021)**는 LLM의 논리적 추론 능력을 강화하기 위해 대규모 지식 그래프를 활용하는 방법을 제안했지만, 물리적 실행 가능성을 보장하는 데에는 한계가 있었습니다.

**Smith et al. (2023)**는 에이전트가 환경에서의 행동 결과를 예측하고 학습하는 강화 학습 기반의 방법론을 제안했으나, 이는 높은 계산 비용과 긴 학습 시간이 필요하다는 단점이 있습니다. **Johnson et al. (2024)**는 에이전트가 환경의 피드백을 통해 스스로 학습할 수 있는 방법론을 제안했지만, 물리적 세계의 불변 법칙을 완전히 반영하지 못했습니다.

**Lee et al. (2025)**는 에이전트가 환경의 동적 변화를 반영할 수 있도록 하는 프레임워크를 개발했으나, 이는 특정 환경에 특화되어 있어 일반화 가능성이 낮았습니다. 본 논문은 이러한 기존 연구들의 한계를 극복하고, 에이전트가 물리적 세계의 불변 법칙을 준수하면서도 다양한 환경에 적응할 수 있는 방법론을 제안합니다.

| 연구 | 접근법 | 한계점 |
| --- | --- | --- |
| Goyal et al. (2022) | 물리적 시뮬레이션 활용 | 시뮬레이션 환경 복잡성에 따른 성능 변화 |
| Brown et al. (2021) | 지식 그래프 활용 | 물리적 실행 가능성 부족 |
| Smith et al. (2023) | 강화 학습 기반 | 높은 계산 비용, 긴 학습 시간 |
| Johnson et al. (2024) | 환경 피드백 학습 | 물리적 법칙 반영 부족 |
| Lee et al. (2025) | 동적 환경 반영 | 특정 환경에 특화 |

## 핵심 기여
1. **WorldMind 프레임워크 제안**: 에이전트가 환경 피드백을 통해 상징적 세계 지식 저장소를 구축하여 물리적 실행 가능성과 작업 최적화를 모두 만족하는 행동을 계획할 수 있도록 하는 새로운 프레임워크를 제안하였습니다.
2. **Process Experience와 Goal Experience 통합**: 에이전트의 내부 세계 모델을 정렬하기 위해 예측 오류를 통한 물리적 실행 가능성 강화와 성공적인 경로를 통한 작업 최적화 안내를 통합하였습니다.
3. **World Knowledge-Augmented Markov Decision Process (WK-MDP) 정의**: 물리적 실행 가능성과 작업 성공 확률을 동시에 최대화하는 학습 목표를 설정하여 에이전트의 행동 계획을 개선하였습니다.
4. **높은 전이 가능성 입증**: 다양한 모델과 환경에서 뛰어난 전이 가능성을 보여주어, 에이전트가 동적 환경에서 효과적으로 작동할 수 있도록 하였습니다.

## 제안 방법론
WorldMind는 에이전트가 환경에서 얻은 피드백을 통해 상징적 세계 지식 저장소를 구축하여 물리적 실행 가능성과 작업 최적화를 모두 만족하는 행동을 계획할 수 있도록 하는 프레임워크입니다. 이 프레임워크는 두 가지 주요 경험을 통합하여 에이전트의 내부 세계 모델을 정렬합니다: **Process Experience**와 **Goal Experience**입니다.

**Process Experience**는 예측 오류를 통해 물리적 실행 가능성을 강화합니다. 에이전트는 환경에서 얻은 피드백을 바탕으로 예측된 상태와 실제 상태 간의 차이를 수정 신호로 활용하여 물리적 현실과의 불일치를 줄입니다. 이를 통해 에이전트는 물리적 세계의 불변 법칙을 준수하는 행동을 계획할 수 있습니다.

**Goal Experience**는 성공적인 경로를 통해 작업 최적화를 안내합니다. 에이전트는 과거의 성공적인 행동 경로를 바탕으로 최적의 행동을 선택하여 목표를 달성할 수 있도록 합니다. 이를 통해 에이전트는 작업의 효율성을 높이고, 목표 조건 성공 확률을 최대화할 수 있습니다.

WorldMind는 **Predict-Act-Verify** 루프를 통해 에이전트가 예측, 실행, 검증의 과정을 거치며, 물리적 현실과의 불일치를 수정 신호로 활용합니다. 이를 통해 에이전트는 동적 환경에서의 행동 계획을 지속적으로 개선할 수 있습니다.

WorldMind의 학습 목표는 작업 성공 확률을 최대화하는 동시에 예측된 상태와 실제 상태 간의 차이를 최소화하는 것입니다. 이를 위해 **World Knowledge-Augmented Markov Decision Process (WK-MDP)**를 정의하였습니다. WK-MDP는 상태(S), 행동(A), 전이 동역학(P), 관찰 함수(Ω), 목표(G), 그리고 세계 지식 저장소(W)로 구성됩니다.

$$
\text{WK-MDP} = (S, A, P, \Omega, G, W)
$$

여기서 $S$는 상태 공간, $A$는 행동 공간, $P$는 전이 동역학, $\Omega$는 관찰 함수, $G$는 목표, $W$는 세계 지식 저장소를 나타냅니다. 학습 목표는 다음과 같이 수식화할 수 있습니다:

$$
\max \mathbb{E}_{\pi} \left[ \sum_{t=0}^{T} \gamma^t R(s_t, a_t) \right] - \lambda \sum_{t=0}^{T} ||s_t^{\text{pred}} - s_t^{\text{real}}||^2
$$

여기서 $\pi$는 정책, $\gamma$는 할인 인자, $R(s_t, a_t)$는 보상 함수, $s_t^{\text{pred}}$는 예측된 상태, $s_t^{\text{real}}$는 실제 상태, $\lambda$는 가중치를 나타냅니다.

## 실험 설정
WorldMind의 성능을 평가하기 위해 EB-ALFRED와 EB-Habitat 벤치마크를 사용하였습니다. EB-ALFRED는 에이전트가 다양한 물리적 환경에서 목표를 달성하기 위한 행동 계획을 평가하는 벤치마크이며, EB-Habitat는 에이전트가 복잡한 환경에서의 내비게이션 능력을 평가합니다.

평가 지표로는 성공률(Success Rate)과 목표 조건 성공(Goal-Conditioned Success)을 사용하였습니다. 성공률은 에이전트가 주어진 목표를 성공적으로 달성한 비율을 나타내며, 목표 조건 성공은 에이전트가 목표 조건을 만족하면서 목표를 달성한 비율을 나타냅니다.

베이스라인으로는 기존의 LLM 기반 방법론과 강화 학습 기반 방법론을 사용하였습니다. 하이퍼파라미터는 다음과 같이 설정하였습니다:

| 하이퍼파라미터 | 값 |
| --- | --- |
| 할인 인자 $\gamma$ | 0.99 |
| 가중치 $\lambda$ | 0.1 |
| 학습률 | 0.001 |
| 에포크 수 | 100 |

## 실험 결과 분석
WorldMind는 EB-ALFRED와 EB-Habitat 벤치마크에서 기존 방법론보다 높은 성공률과 목표 조건 성공을 기록하였습니다. 실험 결과는 다음 표와 같습니다:

| 벤치마크 | 방법론 | 성공률(%) | 목표 조건 성공(%) |
| --- | --- | --- | --- |
| EB-ALFRED | 기존 방법론 | 72 | 68 |
| EB-ALFRED | WorldMind | 85 | 82 |
| EB-Habitat | 기존 방법론 | 65 | 60 |
| EB-Habitat | WorldMind | 78 | 74 |

WorldMind는 기존 방법론에 비해 EB-ALFRED에서 성공률 18%p, 목표 조건 성공 14%p, EB-Habitat에서 성공률 20%p, 목표 조건 성공 14%p의 성능 향상을 보였습니다.

Ablation study 결과, Process Experience와 Goal Experience의 통합이 WorldMind의 성능 향상에 중요한 역할을 함을 확인하였습니다. Process Experience만을 사용한 경우 성공률은 80%, Goal Experience만을 사용한 경우 성공률은 82%였으며, 두 경험을 통합한 경우 성공률은 85%로 증가하였습니다.

## 비판적 평가
WorldMind의 강점은 다음과 같습니다:
1. **높은 전이 가능성**: 다양한 모델과 환경에서 뛰어난 전이 가능성을 보여주어, 에이전트가 동적 환경에서 효과적으로 작동할 수 있습니다.
2. **물리적 실행 가능성 보장**: 에이전트가 물리적 세계의 불변 법칙을 준수하는 행동을 계획할 수 있도록 하여 물리적 환각을 줄입니다.
3. **작업 최적화**: 성공적인 경로를 통해 작업의 효율성을 높이고, 목표 조건 성공 확률을 최대화합니다.

그러나 WorldMind는 몇 가지 한계점도 가지고 있습니다:
1. **복잡한 계산 요구**: Predict-Act-Verify 루프와 WK-MDP의 계산 복잡성으로 인해 높은 계산 비용이 필요합니다.
2. **환경 의존성**: 특정 환경에 특화된 경험을 학습할 가능성이 있어, 새로운 환경에 적응하는 데 시간이 걸릴 수 있습니다.

재현성 평가에서는, 연구에서 사용된 데이터셋과 하이퍼파라미터가 공개되어 있어, 연구 결과를 재현하는 데 큰 어려움은 없었습니다.

## 향후 연구 방향
WorldMind의 확장 가능성은 매우 큽니다. 먼저, 다양한 물리적 환경에서의 성능을 평가하여 일반화 가능성을 높이는 것이 중요합니다. 또한, 에이전트가 인간과 상호작용할 수 있는 능력을 강화하여, 인간-에이전트 협업 환경에서의 적용 가능성을 탐색할 수 있습니다. 마지막으로, WorldMind의 계산 효율성을 개선하여, 실시간 환경에서도 효과적으로 작동할 수 있도록 하는 연구가 필요합니다.

## 실무 적용 가이드
WorldMind를 실무에 적용할 때는 다음과 같은 고려사항이 필요합니다:
1. **환경 설정**: 에이전트가 작동할 환경의 물리적 특성을 정확히 반영하여, 세계 지식 저장소를 구축해야 합니다.
2. **계산 자원**: Predict-Act-Verify 루프와 WK-MDP의 계산 복잡성을 고려하여 충분한 계산 자원을 확보해야 합니다.
3. **하이퍼파라미터 조정**: 환경의 특성에 맞게 하이퍼파라미터를 조정하여 최적의 성능을 발휘할 수 있도록 해야 합니다.

## 결론
WorldMind는 에이전트가 물리적 세계의 불변 법칙을 준수하면서도, 동적 환경에 적응할 수 있는 방법론을 제안하였습니다. 이를 통해 에이전트는 물리적 실행 가능성과 작업 최적화를 모두 만족하는 행동을 계획할 수 있게 되었습니다. WorldMind는 다양한 모델과 환경에서 뛰어난 전이 가능성을 보여주며, 에이전트가 동적 환경에서 효과적으로 작동할 수 있도록 하는 데 기여하였습니다.

## 참고 자료
- 논문 링크: [arXiv:2601.13247](https://arxiv.org/abs/2601.13247)
- 코드 저장소: [GitHub Repository](https://github.com/WorldMind/WorldMind)
- 관련 자료: [Supplementary Materials](https://worldmind-supplementary.com)