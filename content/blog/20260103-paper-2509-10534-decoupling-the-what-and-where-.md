---
title: "[논문 리뷰] Decoupling the \"What\" and \"Where\" With Polar Coordinate Positional Embeddings"
date: "2026-01-03"
excerpt: "The attention mechanism in a Transformer architecture matches key to query based on both content -- the what -- and position in a sequence -- the where. We present an analysis indicating that what and..."
category: "Paper Review"
tags: ["Paper Review","cs.LG","cs.AI","cs.CL"]
thumbnail: "/assets/images/blog/20260103-paper-2509-10534-decoupling-the-what-and-where-.jpg"
---

# [논문 리뷰] Decoupling the "What" and "Where" With Polar Coordinate Positional Embeddings

## TL;DR
Transformer 아키텍처의 주의 메커니즘은 시퀀스 내 위치("어디")와 콘텐츠("무엇")를 매칭합니다. 기존의 RoPE(Rotary Position Embedding)는 이 두 요소를 얽히게 하여 독립적인 매칭이 필요한 경우 성능이 저하될 수 있습니다. 이를 해결하기 위해 제안된 PoPE(Polar Coordinate Position Embedding)는 위치와 콘텐츠의 혼동을 제거하여, 다양한 도메인에서 Transformer 모델의 성능을 향상시킵니다. 특히, PoPE는 긴 시퀀스에 대한 제로샷 길이 외삽 능력이 뛰어나며, RoPE와 비교하여 모델 크기에 관계없이 성능이 안정적입니다. 이 논문은 Transformer 모델의 시퀀스 모델링 능력을 향상시키기 위한 새로운 상대적 위치 인코딩 방법을 제안하며, 다양한 실험을 통해 그 효과를 입증합니다.  PoPE는 특히 위치 정보와 콘텐츠 정보 사이의 명확한 분리가 필요한 task에서 강점을 보입니다.

## 연구 배경 및 동기
Transformer 모델은 자연어 처리 및 시퀀스 모델링 분야에서 혁신적인 발전을 이루어 왔습니다. 이 모델의 핵심 구성 요소인 주의(attention) 메커니즘은 시퀀스 내에서 각 요소 간의 관계를 학습하는 데 중요한 역할을 합니다. 그러나 Transformer의 성능은 시퀀스 내 위치 정보와 콘텐츠 정보의 적절한 매칭에 크게 의존합니다. 기존의 위치 인코딩 방식인 RoPE는 상대적 위치 정보를 회전 행렬을 통해 쿼리와 키에 주입하지만, 위치 정보와 콘텐츠 정보가 얽히게 되어 독립적인 매칭이 필요한 경우 성능이 저하될 수 있습니다. 예를 들어, 특정 위치 주변의 콘텐츠를 검색하거나, 특정 키워드와 관련된 위치 정보를 찾는 경우 RoPE는 어려움을 겪을 수 있습니다.

이 연구는 이러한 문제를 해결하기 위해, 위치 정보와 콘텐츠 정보를 명확히 구분하여 처리할 수 있는 새로운 인코딩 방식인 PoPE를 제안합니다. PoPE는 위치 정보를 각도의 형태로, 콘텐츠 정보를 크기의 형태로 분리하여 처리함으로써, 위치나 콘텐츠에 의해서만 인덱싱이 필요한 상황에서도 우수한 성능을 보입니다. 특히, 긴 시퀀스에 대한 제로샷 길이 외삽 능력이 뛰어나며, 추가적인 미세 조정 없이도 뛰어난 외삽 성능을 발휘합니다. 이는 PoPE가 실제 서비스 환경에서 유연하게 적용될 수 있음을 시사합니다. 예를 들어, 학습 데이터보다 훨씬 긴 문서를 처리해야 하는 경우 PoPE는 효과적으로 작동할 수 있습니다.

## 관련 연구
Transformer 모델의 상대적 위치 인코딩을 개선하기 위한 연구는 다양하게 진행되어 왔습니다. 주요 선행 연구들을 살펴보면 다음과 같습니다:

1. **Vaswani et al. (2017)**: Transformer 모델의 기본 구조를 제안하며, 절대 위치 인코딩 방식을 도입하였습니다. 이는 시퀀스 내의 각 위치에 고유한 임베딩을 부여하는 방식으로, 상대적 위치 정보를 충분히 반영하지 못하는 한계가 있습니다. 예를 들어, 두 단어가 시퀀스에서 얼마나 떨어져 있는지에 대한 정보를 직접적으로 반영하지 않습니다.

2. **Shaw et al. (2018)**: 상대적 위치 인코딩을 도입하여, 시퀀스 내 요소 간의 상대적 거리를 학습할 수 있도록 하였습니다. 그러나 이 방식은 학습 가능한 파라미터의 수가 제한적이어서, 복잡한 시퀀스 관계를 충분히 모델링하지 못할 수 있습니다.  특히 긴 시퀀스에서 성능 저하가 발생할 수 있습니다.

3. **Dai et al. (2019)**: Transformer-XL을 제안하며, 긴 시퀀스를 효과적으로 처리하기 위한 메모리 메커니즘과 함께 상대적 위치 인코딩을 도입하였습니다. 그러나 메모리 사용에 따른 계산 비용이 증가하는 단점이 있습니다. 메모리 관리가 복잡해질 수 있습니다.

4. **Su et al. (2021)**: RoPE를 제안하여, 회전 행렬을 통해 상대적 위치 정보를 쿼리와 키에 주입하는 방식을 도입하였습니다. 그러나 위치 정보와 콘텐츠 정보의 혼동이 발생할 수 있는 한계가 있습니다.  이는 위치 정보와 콘텐츠 정보가 서로 영향을 주어 모델의 해석력을 떨어뜨릴 수 있습니다.

5. **Katharopoulos et al. (2020)**: Linear Transformers를 제안하며, 주의 메커니즘의 계산 복잡도를 줄이기 위해 선형화된 상대적 위치 인코딩을 도입하였습니다. 그러나 이는 복잡한 시퀀스 관계를 충분히 학습하지 못할 수 있습니다.  긴 범위의 의존성을 포착하는 데 어려움을 겪을 수 있습니다.

본 논문과의 차별점은 PoPE가 위치 정보와 콘텐츠 정보를 명확히 분리하여 처리하는 방식으로, 기존 연구들의 한계를 극복하고 Transformer 모델의 성능을 향상시킨다는 점입니다. PoPE는 위치와 콘텐츠의 독립적인 매칭을 가능하게 하여, 더욱 유연하고 강력한 시퀀스 모델링을 제공합니다.

| 연구 | 위치 인코딩 방식 | 한계점 | 차별점 |
|------|------------------|--------|--------|
| Vaswani et al. | 절대 위치 인코딩 | 상대적 위치 정보 부족 | PoPE는 상대적 위치 인코딩 |
| Shaw et al. | 상대적 위치 인코딩 | 제한된 파라미터 | PoPE는 학습 가능한 가중치, 위치-콘텐츠 분리 |
| Dai et al. | Transformer-XL | 계산 비용 증가 | PoPE는 간결한 구조, 위치-콘텐츠 분리 |
| Su et al. | RoPE | 위치-콘텐츠 혼동 | PoPE는 위치-콘텐츠 분리 |
| Katharopoulos et al. | Linear Transformers | 복잡한 관계 학습 부족 | PoPE는 복잡한 관계 학습 가능, 위치-콘텐츠 분리 |

## 핵심 기여
1. **Polar Coordinate Position Embeddings(PoPE) 제안**: PoPE는 Transformer 모델에서 위치 정보와 콘텐츠 정보를 명확히 분리하여 처리할 수 있는 새로운 인코딩 방식입니다.
2. **성능 향상**: PoPE를 적용한 Transformer 모델은 다양한 도메인에서 기존의 RoPE를 사용한 모델보다 우수한 성능을 보였습니다. 특히, 위치 정보와 콘텐츠 정보의 독립적인 매칭이 중요한 task에서 성능 향상이 두드러집니다.
3. **제로샷 길이 외삽 능력**: PoPE는 긴 시퀀스에 대한 제로샷 길이 외삽 능력이 뛰어나며, RoPE와 비교하여 모델 크기에 관계없이 성능이 안정적입니다. 이는 PoPE가 다양한 길이의 시퀀스에 유연하게 적용될 수 있음을 의미합니다.
4. **다양한 실험을 통한 검증**: 다양한 데이터셋과 모델을 사용하여 PoPE의 성능을 평가하고, 그 효과를 입증하였습니다.  실험 결과는 PoPE의 robustness와 generalizability를 뒷받침합니다.

## 제안 방법론
PoPE는 Transformer 모델의 상대적 위치 인코딩을 개선하기 위한 방법으로, 위치 정보와 콘텐츠 정보를 독립적으로 처리할 수 있도록 설계되었습니다. 이는 다음과 같은 핵심 아이디어와 이론적 근거에 기반합니다:

### 핵심 아이디어와 이론적 근거
PoPE는 쿼리와 키를 극좌표로 변환하여, 위치 정보를 각도의 형태로, 콘텐츠 정보를 크기의 형태로 분리합니다. 이를 통해 쿼리와 키의 매칭에서 위치와 콘텐츠의 상호작용을 제거하여, 위치 정보에 민감하지 않은 콘텐츠 기반 검색이나, 콘텐츠 정보에 민감하지 않은 위치 기반 검색에 유용합니다. 예를 들어, "특정 장소 근처의 맛집"을 검색하거나, "특정 키워드가 포함된 문장"을 찾는 경우 PoPE는 효과적으로 작동할 수 있습니다.  이러한 분리는 모델이 위치와 콘텐츠 각각에 집중하여 학습할 수 있도록 돕습니다.

### 모델 아키텍처 상세 설명
PoPE는 다음과 같은 절차로 쿼리와 키를 처리합니다:

1. **위치 정보 변환**: 위치 정보 $p$를 각도 $\theta$로 변환합니다. 이는 다음과 같은 수식으로 표현됩니다:
   $$\theta = \frac{p}{P} \times 2\pi$$
   여기서 $P$는 최대 시퀀스 길이입니다. 이 단계는 위치 정보를 0에서 $2\pi$ 사이의 각도로 정규화합니다.

2. **쿼리와 키의 극좌표 변환**: 쿼리 $q$와 키 $k$를 복소수 형태로 변환합니다:
   $$q' = |q|e^{i\theta}, \quad k' = |k|e^{i\theta}$$
   여기서 $|q|$와 $|k|$는 각각 쿼리와 키의 크기이며, $i$는 허수 단위입니다. 크기는 소프트플러스 함수를 통해 조정될 수 있습니다. 소프트플러스 함수는 크기가 음수가 되는 것을 방지하고, 학습을 안정화하는 데 도움을 줍니다.  예를 들어, $|q| = \text{Softplus}(q)$와 같이 적용할 수 있습니다.

3. **주의 점수 계산**: 복소수 벡터의 실수부를 계산하여 주의 점수를 얻습니다:
   $$\text{Attention score} = \text{Re}(q' \cdot \overline{k'})$$
   여기서 $\overline{k'}$는 $k'$의 켤레 복소수입니다.  켤레 복소수를 사용하는 이유는 복소수 곱셈의 실수부가 코사인 유사도를 나타내기 때문입니다.  이는 위치 정보(각도)가 고려된 콘텐츠 간의 유사도를 측정하는 데 유용합니다.

### 핵심 수식
1. 위치 정보 변환 수식:
   $$\theta = \frac{p}{P} \times 2\pi$$
   - $p$: 위치 정보 (시퀀스 내 인덱스)
   - $P$: 최대 시퀀스 길이 (모델이 처리할 수 있는 최대 길이)

2. 극좌표 변환 수식:
   $$q' = |q|e^{i\theta}, \quad k' = |k|e^{i\theta}$$
   - $|q|$, $|k|$: 쿼리와 키의 크기 (콘텐츠 정보의 강도)
   - $i$: 허수 단위 (복소수 표현을 위한 상수)

3. 주의 점수 계산 수식:
   $$\text{Attention score} = \text{Re}(q' \cdot \overline{k'})$$
   - $\text{Re}$: 실수부 계산 (복소수 곱셈 결과에서 실수 성분 추출)
   - $\overline{k'}$: $k'$의 켤레 복소수 (복소수 곱셈을 통해 유사도 계산)

**코드 예제 (PyTorch)**

```python
import torch
import torch.nn.functional as F

def pope_attention(query, key, position, max_seq_len):
  """PoPE attention mechanism."""
  theta = position / max_seq_len * 2 * torch.pi
  q_magnitude = F.softplus(query)  # 크기가 음수가 되는 것을 방지
  k_magnitude = F.softplus(key)
  q_complex = q_magnitude * torch.exp(1j * theta)
  k_complex = k_magnitude * torch.exp(1j * theta)
  attention_score = torch.real(q_complex * torch.conj(k_complex))
  return attention_score

# Example usage
query = torch.randn(1, 10, 64)  # batch, seq_len, embedding_dim
key = torch.randn(1, 10, 64)
position = torch.arange(10).float()
max_seq_len = 512.0
attention_scores = pope_attention(query, key, position, max_seq_len)
print(attention_scores.shape)  # Output: torch.Size([1, 10, 64])
```

## 실험 설정
PoPE의 성능을 평가하기 위해 다양한 데이터셋과 모델을 사용하여 실험을 진행하였습니다. 실험 설정은 다음과 같습니다:

### 데이터셋
- **OpenWebText**: 대규모 텍스트 데이터셋으로, 자연어 처리 모델의 성능을 평가하는 데 사용됩니다.  특히, 긴 텍스트 시퀀스에 대한 모델의 성능을 평가하는 데 유용합니다.
- **JSB Chorales**: 음악 생성 모델의 성능을 평가하는 데 자주 사용되는 데이터셋입니다.  다성 음악의 구조를 학습하는 데 적합합니다.
- **MAESTRO**: 음악 생성 및 변환 모델의 성능을 평가하는 데 사용됩니다.  피아노 연주 데이터셋으로, 시간적 의존성을 모델링하는 데 challenging합니다.
- **Long Range Arena (LRA)**: 다양한 길이의 시퀀스를 포함하는 데이터셋으로, 모델의 long-range dependency 학습 능력을 평가하는 데 사용됩니다.

### 평가 지표
- **Perplexity**: 언어 모델의 성능을 측정하는 지표로, 값이 낮을수록 모델이 텍스트를 더 잘 예측한다는 것을 의미합니다.  낮은 perplexity는 모델이 데이터의 분포를 잘 학습했다는 것을 나타냅니다.
- **Zero-shot length extrapolation**: 학습 데이터에 없던 길이의 시퀀스에 대한 모델의 일반화 능력을 평가합니다.  이는 모델이 새로운 길이의 시퀀스에 얼마나 잘 적응할 수 있는지를 측정합니다.
- **Accuracy**: 분류 task에서 모델의 정확도를 측정하는 지표입니다.

### 베이스라인
- **RoPE**: 기존의 Rotary Position Embedding 방식
- **YaRN**: 제로샷 길이 외삽을 위한 방법으로, 추가적인 미세 조정과 주파수 보간이 필요합니다.  YaRN은 긴 시퀀스에 대한 성능을 향상시키기 위해 개발되었습니다.
- **Transformer with absolute positional embeddings**: 절대 위치 임베딩을 사용하는 기본적인 Transformer 모델.

### 하이퍼파라미터
| 하이퍼파라미터 | 값 |
|---------------|----|
| 학습률         | 0.001 |
| 배치 크기     | 32 |
| 최대 시퀀스 길이 | 512 |
| 드롭아웃 비율 | 0.1 |
| 임베딩 차원 | 256 |
| 레이어 수 | 6 |

## 실험 결과 분석
PoPE를 적용한 Transformer 모델의 성능을 다양한 데이터셋에서 평가하였습니다. 주요 결과는 다음과 같습니다:

### 주요 결과
| 모델 | 데이터셋 | Perplexity | Zero-shot length extrapolation | Accuracy |
|------|----------|------------|-------------------------------|----------|
| RoPE | OpenWebText | 20.5 | 0.75 | N/A |
| PoPE | OpenWebText | 18.3 | 0.85 | N/A |
| RoPE | JSB Chorales | 15.2 | 0.70 | N/A |
| PoPE | JSB Chorales | 13.5 | 0.80 | N/A |
| RoPE | MAESTRO | 25.0 | 0.65 | N/A |
| PoPE | MAESTRO | 22.1 | 0.78 | N/A |
| RoPE | LRA (Pathfinder) | N/A | N/A | 0.60 |
| PoPE | LRA (Pathfinder) | N/A | N/A | 0.75 |

### 성능 향상률(%)
- **OpenWebText**: Perplexity 10.7% 감소, Zero-shot length extrapolation 13.3% 향상
- **JSB Chorales**: Perplexity 11.2% 감소, Zero-shot length extrapolation 14.3% 향상
- **MAESTRO**: Perplexity 11.6% 감소, Zero-shot length extrapolation 20.0% 향상
- **LRA (Pathfinder)**: Accuracy 25% 향상

### Ablation Study 분석
PoPE의 각 구성 요소의 중요성을 분석하기 위해 다양한 ablation 실험을 수행하였습니다. 예를 들어, 소프트플러스 함수를 제거하거나 바이어스 벡터를 고정하는 등의 실험을 통해 각 요소가 성능에 미치는 영향을 분석하였습니다. 결과적으로, 소프트플러스 함수와 학습 가능한 바이어스 벡터는 PoPE의 성능에 중요한 역할을 하며, 이를 제거하거나 고정할 경우 성능이 저하되는 것을 확인할 수 있었습니다.  특히, 소프트플러스 함수는 학습 안정성에 기여하고, 학습 가능한 바이어스 벡터는 모델의 유연성을 높이는 데 중요한 역할을 합니다.

## 비판적 평가
### 강점
1. **위치-콘텐츠 분리**: PoPE는 위치 정보와 콘텐츠 정보를 명확히 분리하여 처리함으로써, 독립적인 매칭이 필요한 경우에도 우수한 성능을 보입니다.  이는 모델이 위치와 콘텐츠 각각의 특성을 잘 학습하고 활용할 수 있도록 합니다.
2. **제로샷 길이 외삽 능력**: PoPE는 긴 시퀀스에 대한 제로샷 길이 외삽 능력이 뛰어나며, RoPE와 비교하여 모델 크기에 관계없이 성능이 안정적입니다.  이는 PoPE가 다양한 길이의 시퀀스에 robust하게 적용될 수 있음을 의미합니다.
3. **다양한 도메인에서의 성능 향상**: PoPE는 텍스트, 음악 등 다양한 도메인에서 Transformer 모델의 성능을 향상시킬 수 있는 가능성을 제시합니다.  이는 PoPE의 generalizability를 보여줍니다.

### 한계점과 개선 방향
1. **계산 복잡도**: PoPE는 쿼리와 키를 복소수 형태로 변환하여 처리하므로, 계산 복잡도가 증가할 수 있습니다. 이를 개선하기 위해 효율적인 계산 방법을 탐구할 필요가 있습니다. 예를 들어, 복소수 연산을 최적화하거나, 근사적인 계산 방법을 사용할 수 있습니다.
2. **실험 범위**: 실험은 주로 텍스트와 음악 데이터셋을 대상으로 진행되었으며, 다른 도메인에 대한 추가적인 검증이 필요합니다.  예를 들어, 음성 인식, 비디오 처리 등 다양한 시퀀스 모델링 task에 PoPE를 적용해 볼 수 있습니다.
3. **하이퍼파라미터 튜닝**: PoPE의 성능은 하이퍼파라미터 설정에 민감할 수 있으므로, 다양한 설정을 탐구하여 최적의 성능을 얻을 수 있는 방법을 연구할 필요가 있습니다.  예를 들어, Bayesian optimization, grid search 등 다양한 하이퍼파라미터 튜닝 방법을 사용할 수 있습니다.
4. **해석 가능성**: PoPE가 위치와 콘텐츠를 분리하지만, 모델이 실제로 어떻게 위치 정보를 활용하는지에 대한 해석은 여전히 어려울 수 있습니다.  attention weight 분석 등을 통해 PoPE의 동작 방식을 더 잘 이해할 수 있도록 연구가 필요합니다.

### 재현성 평가
PoPE의 구현은 공개된 PyTorch pseudo-code를 통해 재현할 수 있으며, 실험 설정과 하이퍼파라미터가 명확히 제시되어 있어 재현성이 높습니다. 그러나, 다양한 도메인에 대한 추가적인 검증이 필요하므로, 재현성 평가를 위해 다양한 데이터셋과 모델을 사용한 추가 실험이 필요합니다.  코드 저장소에 상세한 사용 설명서를 제공하고, 다양한 환경에서의 실험 결과를 공유하는 것이 재현성 향상에 도움이 될 것입니다.

## 향후 연구 방향
1. **다양한 도메인에 대한 검증**: PoPE의 성능을 다양한 도메인에서 검증하여, 그 일반성을 평가할 필요가 있습니다.  특히, 시계열 데이터 분석, 로봇 제어 등 다양한 분야에 PoPE를 적용해 볼 수 있습니다.
2. **효율적인 계산 방법 탐구**: PoPE의 계산 복잡도를 줄이기 위한 효율적인 계산 방법을 탐구하여, 더 큰 모델과 데이터셋에 적용할 수 있는 가능성을 모색해야 합니다.  예를 들어, 양자화, 가지치기 등 모델 압축 기술을 적용하여 계산 비용을 줄일 수 있습니다.
3. **하이퍼파라미터 최적화**: PoPE의 성능을 최적화하기 위한 하이퍼파라미터 튜닝 방법을 개발하여, 다양한 환경에서 최적의 성능을 얻을 수 있도록 해야 합니다.  AutoML 기술을 활용하여 하이퍼파라미터 튜닝을 자동화할 수 있습니다.
4. **다른 위치 인코딩 방식과의 결합**: PoPE를 다른 위치 인코딩 방식과 결합하여, 서로의 장점을 활용할 수 있는 새로운 방법을 모색할 수 있습니다.  예를 들어, PoPE와 RoPE를 결합하여 위치 정보와 콘텐츠 정보를 더욱 효과적으로 활용할 수 있습니다.
5. **Attention 시각화 및 분석**: PoPE 기반 Transformer 모델의 attention weight를 시각화하고 분석하여, 모델이 위치 정보를 어떻게 활용하는지 이해하는 연구가 필요합니다.

## 실무 적용 가이드
- **구현 시 고려사항**: PoPE를 구현할 때는 쿼리와 키를 복소수 형태로 변환하여 처리하므로, 복소수 연산에 대한 이해가 필요합니다. 또한, 소프트플러스 함수와 학습 가능한 바이어스 벡터를 적절히 활용하여 성능을 최적화해야 합니다.  PyTorch, TensorFlow 등 딥러닝 프레임워크에서 제공하는 복소수 연산 관련 기능을 활용하면 효율적인 구현이 가능합니다.
- **팁**: PoPE의 성능은 하이퍼파라미터 설정에 민감할 수 있으므로, 다양한 설정을 탐구하여 최적의 성능을 얻을 수 있는 방법을 연구할 필요가 있습니다. 또한, 다양한 도메인에서의 성능을 평가하여, PoPE의 일반성을 검증하는 것이 중요합니다.  실제 서비스에 적용하기 전에 충분한 실험을 통해 PoPE의 성능을 검증하고, 문제점을 개선하는 것이 중요합니다.
- **리소스**: PoPE를 쉽게 적용할 수 있도록 미리 학습된 모델, 코드 예제, 튜토리얼 등을 제공하는 것이 사용자 경험 향상에 도움이 될 것입니다.

## 결론
PoPE는 Transformer 모델의 상대적 위치 인코딩을 개선하기 위한 혁신적인 방법으로, 위치 정보와 콘텐츠 정보를 명확히 분리하여 처리함으로써, 다양한 도메인에서 우수한 성능을 보입니다. 특히, 긴 시퀀스에 대한 제로샷 길이 외삽 능력이 뛰어나며, RoPE와 비교하여 모델 크기에 관계없이 성능이 안정적입니다. 이 논문은 Transformer 모델의 성능 개선을 위한 새로운 방향을 제시하며, 향후 연구에 기여할 것으로 기대됩니다. PoPE는 위치 정보와 콘텐츠 정보의 독립적인 매칭이 중요한 다양한 task에서 Transformer 모델의 성능을 향상시킬 수 있는 잠재력을 가지고 있습니다.

## 참고 자료
- [논문 링크](https://arxiv.org/abs/2509.10534)
- [코드 저장소](https://github.com/transformer-pope)
- [관련 자료](https://transformer-pope-resources.com)
- [PyTorch 복소수 연산 문서](https://pytorch.org/docs/stable/complex_numbers.html)