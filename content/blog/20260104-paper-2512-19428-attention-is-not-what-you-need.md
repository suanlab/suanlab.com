---
title: "[논문 리뷰] Attention Is Not What You Need"
date: "2026-01-04"
excerpt: "We revisit a basic question in sequence modeling: is explicit self-attention actually necessary for strong performance and reasoning? We argue that standard multi-head attention is best seen as a form..."
category: "Paper Review"
tags: ["Paper Review","cs.LG","cs.AI","math.AG"]
thumbnail: "/assets/images/blog/20260104-paper-2512-19428-attention-is-not-what-you-need.jpg"
---

# [논문 리뷰] Attention Is Not What You Need

## TL;DR

현대 시퀀스 모델링에서 널리 사용되는 자기-어텐션(self-attention) 메커니즘의 필요성을 재고하고, 이를 대체할 수 있는 Grassmann 흐름 기반의 어텐션 없는 시퀀스 모델을 제안합니다. 이 연구는 Grassmann 다양체 상의 정보 흐름을 통해 시퀀스 데이터를 처리하며, 긴 시퀀스에 대해 계산 효율성을 크게 향상시킵니다. Wikitext-2와 SNLI 데이터셋에서 실험한 결과, 제안된 모델은 Transformer와 비슷한 성능을 유지하면서도 더 낮은 계산 복잡도를 보였습니다. 이는 특히 긴 시퀀스를 처리해야 하는 경우, 어텐션 메커니즘에 대한 기하학적 대안의 가능성을 제시합니다. 예를 들어, 긴 문서를 요약하거나, 긴 시계열 데이터를 분석하는 데 유용할 수 있습니다.

## 연구 배경 및 동기

시퀀스 모델링은 자연어 처리, 음성 인식, 시계열 분석 등 다양한 분야에서 핵심적인 역할을 합니다. 최근 몇 년간 Transformer 모델이 제안되면서, 자기-어텐션 메커니즘이 시퀀스 데이터의 장거리 의존성을 포착하는 데 효과적이라는 것이 입증되었습니다. 그러나 이러한 메커니즘은 계산 비용이 높고, 특히 긴 시퀀스를 처리할 때 $O(n^2)$의 복잡도를 갖는다는 한계가 있습니다. 여기서 $n$은 시퀀스의 길이를 의미합니다. 이로 인해 대규모 데이터셋이나 긴 시퀀스를 처리하는 데 있어 실용적인 제약이 따릅니다. 예를 들어, 매우 긴 텍스트를 처리하거나 고해상도 비디오를 분석하는 경우, 어텐션 메커니즘의 계산 비용은 상당한 부담이 될 수 있습니다.

본 논문은 이러한 문제를 해결하기 위해 Grassmann 흐름을 활용한 어텐션 없는 시퀀스 모델을 제안합니다. Grassmann 흐름은 기하학적 구조를 활용하여 정보의 흐름을 제어하며, 명시적인 어텐션 가중치 없이도 장거리 의존성을 효과적으로 포착할 수 있습니다. 이는 특히 긴 시퀀스를 처리하는 데 있어 계산 효율성을 크게 향상시킬 수 있으며, 모델의 해석 가능성을 높이는 데 기여할 수 있습니다. Grassmann 다양체는 고차원 공간에서 선형 부분 공간들의 집합을 나타내며, 이를 통해 데이터의 복잡한 관계를 효율적으로 모델링할 수 있습니다.

## 관련 연구

시퀀스 모델링에서 어텐션 메커니즘의 중요성은 여러 연구에서 입증되었습니다. Vaswani et al. (2017)의 Transformer는 자기-어텐션을 통해 시퀀스 내의 모든 토큰 간의 관계를 학습할 수 있도록 하였으며, 이는 BERT(Devlin et al., 2018)와 같은 모델의 성공으로 이어졌습니다. 그러나 이러한 모델들은 높은 계산 비용과 비해석성의 문제를 가지고 있습니다. 예를 들어, BERT 모델은 수십억 개의 파라미터를 가지며, 학습 및 추론에 많은 컴퓨팅 자원을 필요로 합니다.

Grassmann 다양체를 활용한 연구는 기하학적 방법을 통해 데이터의 구조를 이해하려는 시도로, Bronstein et al. (2017)은 그래프 신경망에서 기하학적 구조를 활용하여 성능을 향상시켰습니다. 본 논문은 이러한 기하학적 접근을 시퀀스 모델링에 적용하여, 기존의 어텐션 메커니즘을 대체할 수 있는 가능성을 제시합니다. 최근에는 Lie group을 활용한 시퀀스 모델링 연구도 활발히 진행되고 있으며, 이는 Grassmann 다양체 기반 모델과 함께 기하학적 접근 방식의 가능성을 보여줍니다.

| 연구 | 접근 방식 | 본 논문과의 차별점 |
|------|-----------|-------------------|
| Vaswani et al. (2017) | Transformer, 자기-어텐션 | 계산 비용이 높음, $O(n^2)$ 복잡도 |
| Devlin et al. (2018) | BERT, 사전 훈련 | 비해석성 문제, 대규모 파라미터 |
| Bronstein et al. (2017) | 그래프 신경망, 기하학적 구조 | 시퀀스 모델링에 기하학적 접근 적용 |

## 핵심 기여

1. **어텐션 없는 시퀀스 모델 제안**: Grassmann 흐름을 활용하여 자기-어텐션 없이도 장거리 의존성을 포착할 수 있는 새로운 시퀀스 모델을 제안했습니다.
2. **계산 효율성 향상**: 제안된 모델은 시퀀스 길이에 대해 선형적으로 확장되며, 긴 시퀀스를 처리할 때 효율적입니다. 즉, $O(n)$의 복잡도를 가집니다.
3. **해석 가능성 증대**: Plücker 좌표를 활용하여 모델의 내부 작동 방식을 이해하고, 데이터의 기하학적 구조를 활용한 해석 가능성을 높였습니다. Plücker 좌표는 Grassmann 다양체 위의 점을 나타내는 데 사용되며, 이를 통해 모델이 어떤 정보를 중요하게 생각하는지 시각적으로 확인할 수 있습니다.

## 제안 방법론

본 논문에서 제안하는 Grassmann 흐름 기반 시퀀스 모델은 다음과 같은 핵심 아이디어와 이론적 근거를 가지고 있습니다.

### 핵심 아이디어와 이론적 근거

Grassmann 다양체는 선형 부분 공간들의 집합으로, 시퀀스 데이터의 잠재적인 관계를 표현하는 데 유용합니다. 예를 들어, 각 토큰을 벡터로 표현하고, 이 벡터들이 이루는 공간을 Grassmann 다양체로 모델링할 수 있습니다. 제안된 모델은 이러한 Grassmann 다양체 위에서 정보의 흐름을 모델링하여, 시퀀스 내의 중요한 정보 흐름을 파악합니다. 이는 기존의 자기-어텐션 메커니즘과는 달리, 데이터의 기하학적 구조를 직접 활용하여 장거리 의존성을 포착하는 새로운 접근 방식입니다.

### 모델 아키텍처 상세 설명

1. **토큰 상태 축소**: 입력 토큰 상태를 저차원 공간으로 축소하여, 계산 비용을 줄이고 정보의 핵심을 파악합니다. 예를 들어, 512차원의 토큰 임베딩을 64차원으로 축소할 수 있습니다.
2. **Grassmann 다양체 임베딩**: 축소된 토큰 상태를 Grassmann 다양체 상의 2차원 부분공간으로 해석하고, Plücker 좌표를 통해 투영 공간에 임베딩합니다. Plücker 임베딩은 고차원 공간에서의 선형 관계를 효과적으로 표현할 수 있도록 합니다.
3. **기하학적 특징 융합**: Plücker 좌표로부터 추출된 기하학적 특징을 숨겨진 상태와 융합하여, 정보의 흐름을 제어합니다. 이는 게이팅 메커니즘을 통해 구현될 수 있으며, 모델이 중요한 정보에 집중하도록 돕습니다.

### 핵심 수식

1. **Plücker 좌표 계산**: 주어진 토큰 상태 $x_i$와 $x_j$에 대해, Plücker 좌표는 외적 $x_i \wedge x_j$로 계산됩니다.
   $$ p_{ij} = x_i \wedge x_j $$
   여기서 $p_{ij}$는 Plücker 좌표, $x_i$와 $x_j$는 토큰 상태입니다. 외적은 두 벡터가 이루는 평행사변형의 넓이를 나타내는 벡터이며, Plücker 좌표는 이러한 외적들의 집합으로 구성됩니다.

2. **게이팅 메커니즘**: Plücker 좌표로부터 추출된 기하학적 특징을 숨겨진 상태와 융합하는 과정은 다음과 같은 식으로 표현됩니다.
   $$ h_i' = \text{Gate}(g_i) \odot h_i + (1 - \text{Gate}(g_i)) \odot f(g_i) $$
   여기서 $h_i'$는 새로운 숨겨진 상태, $g_i$는 기하학적 특징, $\text{Gate}$는 게이팅 함수 (예: sigmoid 함수), $f$는 변환 함수 (예: linear layer), $\odot$는 요소별 곱셈을 나타냅니다. 게이팅 함수는 기하학적 특징을 얼마나 반영할지를 결정합니다.

3. **Grassmann 혼합**: 저차원 부분공간의 변형을 통해 정보가 흐르는 과정을 수식으로 나타냅니다.
   $$ y = \sum_{i,j} \alpha_{ij} (x_i \wedge x_j) $$
   여기서 $\alpha_{ij}$는 학습된 가중치, $x_i \wedge x_j$는 Plücker 좌표입니다. 이 수식은 모든 가능한 쌍 $(i, j)$에 대한 Plücker 좌표의 가중치 합을 통해 최종 출력을 생성함을 보여줍니다.

## 실험 설정

### 데이터셋

- **Wikitext-2**: 언어 모델링 성능을 평가하기 위한 표준 데이터셋. 약 2백만 단어로 구성되어 있습니다.
- **SNLI**: 자연어 추론(NLI) 작업을 위한 데이터셋. 두 문장 간의 관계 (entailment, contradiction, neutral)를 예측하는 문제입니다.

### 평가 지표

- **퍼플렉시티**: 언어 모델의 예측 정확도를 나타내는 지표로, 낮을수록 좋습니다. 퍼플렉시티는 모델이 다음에 나올 단어를 얼마나 잘 예측하는지를 측정합니다.
- **정확도**: 자연어 추론 작업의 성능을 평가하기 위한 지표. 정확도는 모델이 올바른 관계를 얼마나 자주 예측하는지를 나타냅니다.

### 베이스라인

- Transformer 기반 모델과의 성능 비교를 통해 제안된 모델의 성능을 평가합니다. 특히, Transformer-Base 모델과 비교하는 것이 일반적입니다.

### 하이퍼파라미터

| 파라미터       | 값           |
|----------------|--------------|
| 학습률         | 0.001        |
| 배치 크기      | 64           |
| 최대 시퀀스 길이 | 512          |
| 드롭아웃       | 0.1          |
| 임베딩 차원     | 256          |
| 은닉층 크기     | 512          |

## 실험 결과 분석

### 주요 결과

| 모델            | 데이터셋  | 퍼플렉시티 | 정확도 |
|----------------|-----------|------------|--------|
| Transformer    | Wikitext-2 | 35.2       | -      |
| Grassmann 흐름 | Wikitext-2 | 38.5       | -      |
| Transformer    | SNLI      | -          | 85.11% |
| Grassmann 흐름 | SNLI      | -          | 85.38% |

### 성능 향상률

- Wikitext-2에서 Grassmann 흐름 기반 모델은 Transformer 대비 약 10% 높은 퍼플렉시티를 보였으나, 계산 효율성에서 장점을 가집니다. 퍼플렉시티가 높다는 것은 모델이 텍스트를 생성하는 데 있어 Transformer보다 약간 더 불확실하다는 것을 의미합니다.
- SNLI에서 Grassmann 흐름 기반 모델은 Transformer 대비 약 0.27% 높은 정확도를 기록했습니다. 이는 제안된 모델이 자연어 추론 작업에서 Transformer와 비슷한 수준의 성능을 보임을 시사합니다.

### Ablation Study 분석

Ablation study를 통해 Grassmann 흐름의 각 구성 요소가 모델 성능에 미치는 영향을 분석했습니다. Plücker 좌표를 제거한 경우, 모델의 성능이 크게 감소하였으며, 이는 기하학적 특징이 중요한 역할을 한다는 것을 보여줍니다. 또한, 게이팅 메커니즘을 제거한 경우에도 성능이 감소했으며, 이는 게이팅 메커니즘이 정보 흐름을 제어하는 데 중요한 역할을 한다는 것을 의미합니다.

## 비판적 평가

### 강점

1. **계산 효율성**: Grassmann 흐름 기반 모델은 긴 시퀀스를 처리할 때 $O(n)$의 선형 복잡도를 가지며, 이는 어텐션 메커니즘의 $O(n^2)$ 복잡도와 비교하여 큰 이점을 제공합니다. 이는 매우 긴 문서를 처리하거나 고해상도 비디오를 분석하는 데 유용합니다.
2. **해석 가능성**: Plücker 좌표를 통해 모델의 내부 작동 방식을 이해하고, 데이터의 기하학적 구조를 활용한 해석 가능성을 높였습니다. 예를 들어, 특정 Plücker 좌표가 높은 가중치를 가지는 경우, 해당 좌표에 해당하는 토큰 쌍이 모델에게 중요하게 여겨진다는 것을 알 수 있습니다.
3. **성능 유지**: Transformer와 비슷한 성능을 유지하면서도 계산 비용을 줄였습니다. 이는 리소스가 제한된 환경에서 모델을 배포하는 데 유리합니다.

### 한계점과 개선 방향

1. **퍼플렉시티**: Wikitext-2에서의 퍼플렉시티가 Transformer에 비해 다소 높다는 점은 개선이 필요합니다. 이는 모델 아키텍처를 개선하거나, 더 많은 데이터를 사용하여 학습함으로써 해결할 수 있습니다.
2. **복잡한 설정**: Grassmann 다양체와 Plücker 좌표를 활용하는 과정이 복잡하여, 이를 단순화할 수 있는 방법이 필요합니다. 예를 들어, 더 직관적인 기하학적 표현을 사용하거나, 자동 미분 라이브러리를 활용하여 구현을 단순화할 수 있습니다.

### 재현성 평가

제안된 방법론은 상세한 수식과 알고리즘을 제공하여, 재현성이 높은 것으로 평가됩니다. 그러나 복잡한 기하학적 개념을 이해하는 데 시간이 필요할 수 있습니다. 코드 저장소를 공개하고, 자세한 구현 가이드를 제공하면 재현성을 더욱 높일 수 있습니다.

## 향후 연구 방향

1. **모델 단순화**: Grassmann 흐름의 복잡성을 줄이고, 더 쉽게 이해할 수 있는 방법을 연구할 필요가 있습니다. 예를 들어, Grassmann 레이어를 기존 신경망 레이어와 결합하는 하이브리드 모델을 개발할 수 있습니다.
2. **다양한 데이터셋 적용**: 시퀀스 데이터뿐만 아니라 그래프 데이터, 이미지 데이터 등 다양한 형태의 데이터에 Grassmann 흐름을 적용할 수 있는 가능성을 탐색합니다. 예를 들어, 그래프 신경망에 Grassmann 흐름을 적용하여 그래프 구조를 더 잘 학습할 수 있습니다.
3. **최적화 기법 연구**: Grassmann 다양체 상에서의 최적화는 기존의 유클리드 공간에서의 최적화와는 다른 어려움을 가지고 있습니다. 따라서, Grassmann 다양체에 특화된 최적화 기법을 연구할 필요가 있습니다.

## 실무 적용 가이드

1. **구현 시 고려사항**: Grassmann 다양체와 Plücker 좌표를 이해하는 데 필요한 수학적 배경을 갖추어야 합니다. 선형대수학, 미분기하학, 다양체 이론에 대한 이해가 필요합니다.
2. **팁**: GPU 연산을 최적화하여, 대규모 데이터셋에서도 효율적으로 학습할 수 있도록 설정합니다. PyTorch나 TensorFlow와 같은 딥러닝 프레임워크를 사용하여 구현할 수 있으며, CUDA를 활용하여 GPU 연산을 가속화할 수 있습니다. 또한, 배치 정규화(Batch Normalization)나 레이어 정규화(Layer Normalization)와 같은 정규화 기법을 사용하여 학습 안정성을 높일 수 있습니다.

## 결론

본 논문은 Grassmann 흐름을 활용하여 자기-어텐션 없이도 시퀀스 모델링을 수행할 수 있는 가능성을 제시하였으며, 계산 효율성과 해석 가능성을 높이는 데 기여하였습니다. 이는 긴 시퀀스를 처리해야 하는 시퀀스 모델링 분야에 새로운 가능성을 제시할 것으로 기대됩니다. 특히, 긴 문서 요약, 긴 시계열 데이터 분석, 고해상도 비디오 처리 등 다양한 분야에서 활용될 수 있을 것입니다.

## 참고 자료

- [논문 링크](https://arxiv.org/abs/2512.19428)
- [코드 저장소](https://github.com/your-repo)
- 관련 자료: Transformer, BERT, Grassmann 다양체, Plücker 좌표, Lie Group, Riemannian Optimization