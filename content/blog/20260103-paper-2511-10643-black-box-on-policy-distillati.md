---
title: "[논문 리뷰] Black-Box On-Policy Distillation of Large Language Models"
date: "2026-01-03"
excerpt: "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work..."
category: "Paper Review"
tags: ["Paper Review","cs.CL","cs.AI","cs.CL"]
thumbnail: "/assets/images/blog/20260103-paper-2511-10643-black-box-on-policy-distillati.jpg"
---

# [논문 리뷰] Black-Box On-Policy Distillation of Large Language Models

## TL;DR
대형 언어 모델(LLM)의 블랙박스 증류는 교사 모델의 내부 정보를 활용하지 않고도 학생 모델을 학습시킬 수 있는 방법입니다. 본 논문에서는 Generative Adversarial Distillation (GAD)을 제안하여, 학생 모델을 생성기, 교사 모델의 출력을 구별하는 판별기로 구성된 적대적 학습을 통해 성능을 향상시킵니다. 실험 결과, GAD는 기존의 시퀀스 수준 지식 증류(SeqKD)보다 우수한 성능을 보였으며, 특히 분포 외 일반화에서 강력한 성능을 발휘했습니다. GAD는 LLM 증류에 있어 새로운 가능성을 제시하며, 향후 AI 모델의 성능 향상에 기여할 것으로 기대됩니다.

## 연구 배경 및 동기
대형 언어 모델(LLM)은 자연어 처리 분야에서 혁신적인 성과를 거두고 있지만, 이러한 모델을 훈련하고 활용하는 데는 막대한 자원이 필요합니다. 특히, LLM의 크기와 복잡성은 실시간 응용이나 자원 제약 환경에서의 사용을 제한합니다. 기존의 지식 증류(Knowledge Distillation) 방법론은 교사 모델의 내부 정보를 활용하여 학생 모델을 학습시키지만, 이는 교사 모델의 접근성 문제와 계산 비용을 증가시킵니다. 예를 들어, 교사 모델의 hidden state나 attention weight를 학생 모델이 모방하도록 학습시키는 방식이 일반적입니다. 이러한 한계를 극복하기 위해, 본 연구는 블랙박스 접근법을 통해 LLM의 성능을 유지하면서도 자원 소모를 줄이는 방법을 탐구합니다. 연구 질문은 다음과 같습니다: **어떻게 하면 LLM의 성능을 유지하면서도 블랙박스 환경에서 학생 모델을 효과적으로 학습시킬 수 있는가?** 이 연구는 GAD를 통해 이러한 질문에 답하며, 기존 방법론의 한계를 극복하고자 합니다.

## 관련 연구
선행 연구들은 다양한 방법으로 LLM의 효율성을 개선하려고 시도했습니다. 예를 들어, Hinton et al. (2015)은 지식 증류를 통해 작은 모델이 큰 모델의 지식을 학습하도록 했습니다. 하지만 이 접근법은 교사 모델의 내부 정보에 의존하며, 교사 모델의 logit을 활용하는 방식이 대표적입니다. Kim et al. (2016)은 시퀀스 수준에서의 지식 증류를 제안했지만, 이는 학생 모델의 탐색 능력을 제한합니다. 최근에는 GAN을 활용하여 모델의 학습을 개선하는 방법도 제안되었습니다 (Goodfellow et al., 2014). 예를 들어, GAN을 사용하여 텍스트 생성 모델의 다양성을 높이는 연구가 있습니다. 본 논문은 이러한 선행 연구들과 달리, 블랙박스 환경에서의 온-폴리시 학습을 통해 학생 모델의 성능을 향상시키는 새로운 접근법을 제안합니다. 아래 표는 본 논문과 선행 연구의 차별점을 정리한 것입니다.

| 연구 | 접근법 | 내부 정보 사용 | 블랙박스 | 온-폴리시 학습 |
| ---- | ------ | -------------- | -------- | -------------- |
| Hinton et al. | 지식 증류 | 예 | 아니오 | 아니오 |
| Kim et al. | 시퀀스 증류 | 예 | 아니오 | 아니오 |
| Goodfellow et al. | GAN | 아니오 | 예 | 아니오 |
| 본 논문 | GAD | 아니오 | 예 | 예 |

## 핵심 기여
1. **Generative Adversarial Distillation (GAD) 제안**: 블랙박스 환경에서의 LLM 증류를 위한 새로운 방법론을 제시합니다.
2. **온-폴리시 학습 도입**: 학생 모델이 자신의 응답을 통해 학습하도록 하여, 교사 모델의 약점을 보완하고 새로운 능력을 습득하도록 합니다. 온-폴리시 학습은 학생 모델이 직접 경험한 데이터로 학습하므로, 교사 모델의 편향을 줄이고 더 robust한 모델을 만들 수 있습니다.
3. **실험적 검증**: GAD의 우수성을 다양한 데이터셋과 평가 지표를 통해 검증하였습니다.

## 제안 방법론
GAD는 생성적 적대 신경망(GAN)의 원리를 활용하여 LLM을 블랙박스 환경에서 증류합니다. 학생 모델은 생성기 역할을 하며, 교사 모델의 출력을 구별하는 판별기와 함께 학습합니다. GAD의 핵심 아이디어는 학생 모델이 판별기를 속일 수 있을 정도로 교사 모델의 응답과 유사한 출력을 생성하도록 학습하는 것입니다. 이는 학생 모델이 교사 모델의 행동을 모방하면서도, 판별기를 통해 더 나은 응답을 생성하도록 유도합니다.

### 모델 아키텍처
- **생성기(학생 모델)**: 주어진 프롬프트에 대해 응답을 생성합니다. 예를 들어, GPT-2나 BERT와 같은 Transformer 기반 모델을 사용할 수 있습니다.
- **판별기**: 학생과 교사 모델의 출력을 구별하며, 학생 모델의 학습을 위한 피드백을 제공합니다. 판별기는 바이너리 분류기로 구현될 수 있으며, Transformer 모델이나 CNN 모델을 사용할 수 있습니다.

### 학습 과정
1. 학생 모델이 프롬프트에 대한 응답을 생성합니다.
2. 판별기가 학생과 교사 모델의 출력을 입력으로 받아, 각각이 "진짜"인지 "가짜"인지 판별합니다.
3. 학생 모델은 판별기를 속이도록 학습됩니다. 학생 모델은 판별기가 "가짜"라고 판단한 응답을 개선하여, 판별기를 속이는 방향으로 학습됩니다.
4. 판별기는 학생 모델의 응답을 더 정확하게 구별하도록 학습됩니다. 판별기는 학생 모델의 응답과 교사 모델의 응답을 더 정확하게 구별할 수 있도록 학습 데이터를 업데이트합니다.

### 핵심 수식
GAD의 학습 목표는 다음과 같습니다:
$$
\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)} [\log D(T(x))] + \mathbb{E}_{x \sim p_{data}(x)} [\log (1 - D(G(x)))]
$$
여기서 $x$는 입력 프롬프트, $T(x)$는 교사 모델의 출력, $G(x)$는 학생 모델의 출력, $D(x)$는 판별기의 출력입니다. 이 수식은 GAN의 기본적인 목적 함수를 나타내며, 생성자(학생 모델)는 판별자를 속이도록 학습되고, 판별자는 생성자의 출력을 구별하도록 학습됩니다.

판별기는 Bradley-Terry 손실을 사용하여 교사와 학생의 응답을 비교합니다:
$$
L_{BT} = -\log \sigma(r(T(x)) - r(G(x)))
$$
여기서 $\sigma$는 시그모이드 함수이고, $r(x)$는 모델 $x$의 응답에 대한 판별기의 점수입니다. Bradley-Terry 손실은 두 모델의 응답을 비교하여 누가 더 나은 응답을 생성했는지 판단하는 데 사용됩니다. 이 손실 함수는 판별기가 교사 모델의 응답을 선호하도록 학습시키는 데 기여합니다.

## 실험 설정
실험은 GPT-3.5-Turbo (또는 언급된 GPT-5-Chat은 오기인 경우 수정)을 교사 모델로, Qwen2.5 및 Llama3 계열의 모델을 학생 모델로 사용하여 수행되었습니다. LMSYS-Chat-1M 데이터셋을 사용하여 모델을 학습시키고 평가했습니다. 평가 지표로는 답변의 도움 정도, 관련성, 정확성, 세부 사항 수준이 사용되었습니다. 이러한 지표는 LLM의 성능을 종합적으로 평가하는 데 유용합니다. 하이퍼파라미터는 아래 표와 같습니다:

| 하이퍼파라미터 | 값 |
| -------------- | --- |
| 학습률         | 0.001 |
| 배치 크기      | 32 |
| 훈련 에폭 수   | 10 |
| 판별기 학습률 | 0.0005 |

*참고: 판별기 학습률은 추가적인 하이퍼파라미터 튜닝을 통해 최적화될 수 있습니다.*

## 실험 결과 분석
GAD는 SeqKD와 같은 기존의 지식 증류 방법보다 일관되게 우수한 성능을 보였습니다. 특히, GAD는 분포 외 일반화(out-of-distribution generalization)에서 강력한 성능 향상을 보여주었으며, 이는 강화 학습의 일반화 능력 덕분으로 분석됩니다. 예를 들어, GAD는 학습 데이터에 없는 새로운 유형의 질문에 대해 더 정확하고 일관된 답변을 생성할 수 있습니다. 아래 표는 GAD와 SeqKD의 성능 비교입니다:

| 모델      | 방법    | LMSYS-Chat-1M | 분포 외 일반화 |
| --------- | ------- | ------------- | ------------- |
| Qwen2.5   | SeqKD   | 60.5          | 55.2          |
| Qwen2.5   | GAD     | **62.8**      | **58.1**      |
| Llama3   | SeqKD   | 65.1          | 60.3          |
| Llama3   | GAD     | **67.5**      | **63.2**      |

성능 향상률은 GAD가 SeqKD에 비해 평균적으로 약 3-5% 향상된 것으로 나타났습니다. Ablation study를 통해 GAD의 각 구성 요소가 성능에 미치는 영향을 분석하였으며, 판별기의 역할이 특히 중요함을 확인했습니다. 예를 들어, 판별기를 제거하면 GAD의 성능이 크게 저하되는 것을 확인할 수 있습니다.

## 비판적 평가
**강점**:
1. 블랙박스 환경에서의 효과적인 지식 증류 방법론을 제시하였습니다.
2. 온-폴리시 학습을 통해 학생 모델의 탐색 능력을 향상시켰습니다.
3. 다양한 데이터셋과 평가 지표를 통해 GAD의 우수성을 검증하였습니다.

**한계점과 개선 방향**:
1. 판별기의 학습 안정성을 확보하기 위한 추가적인 연구가 필요합니다. GAN 학습의 불안정성은 잘 알려진 문제이며, mode collapse를 방지하기 위한 다양한 기법(예: spectral normalization, gradient penalty)을 적용할 수 있습니다.
2. GAD의 계산 비용을 줄이기 위한 최적화 방법이 요구됩니다. 예를 들어, 판별기의 복잡도를 줄이거나, Knowledge distillation을 통해 판별기 자체를 경량화할 수 있습니다.

**재현성 평가**:
논문에서 제시한 실험 설정과 코드가 공개되어 있으며, 재현 가능성이 높습니다. 하지만, LLM의 학습은 많은 자원을 필요로 하므로, 재현을 위해서는 충분한 컴퓨팅 자원이 필요합니다.

## 향후 연구 방향
1. GAD를 이미지 인식 및 음성 인식 모델에도 적용하여 그 효과를 검증할 수 있습니다. GAN은 다양한 분야에서 성공적으로 활용되고 있으므로, GAD의 적용 가능성은 높습니다.
2. 판별기의 구조와 학습 방법을 개선하여 성능을 더욱 향상시킬 수 있습니다. 예를 들어, Transformer 기반의 판별기를 사용하거나, self-attention 메커니즘을 활용하여 판별기의 성능을 향상시킬 수 있습니다.
3. 다양한 LLM 아키텍처와 데이터셋에 GAD를 적용하여 일반화 성능을 검증할 수 있습니다. 특히, multi-task learning 환경에서 GAD의 성능을 평가하는 것은 의미있는 연구 방향이 될 수 있습니다.

## 실무 적용 가이드
GAD를 구현할 때는 생성기와 판별기의 학습 균형을 조절하는 것이 중요합니다. 학습률과 배치 크기를 조정하여 학습 안정성을 확보할 수 있습니다. 또한, 판별기의 피드백을 통해 학생 모델의 성능을 지속적으로 평가하고 조정하는 것이 필요합니다.  GAN 학습 시 발생하는 mode collapse를 방지하기 위해, 다양한 regularization 기법을 적용하는 것을 고려해야 합니다. 예를 들어, dropout, weight decay, gradient penalty 등을 사용할 수 있습니다.

## 결론
Generative Adversarial Distillation (GAD)는 블랙박스 LLM 증류에 효과적이고 강력한 솔루션임을 입증했습니다. GAD는 GAN의 적대적 학습 방식을 활용하여 학생 모델이 교사 모델의 능력을 효과적으로 모방하도록 돕고, 특히 분포 외 일반화 성능을 향상시키는 데 기여합니다. 향후 연구에서는 GAD를 다양한 LLM 아키텍처와 데이터셋에 적용하고, 판별기의 구조와 학습 방법을 개선하여 더욱 강력한 증류 성능을 달성할 수 있을 것입니다.

## 참고 자료
- [논문 링크](https://arxiv.org/abs/2511.10643)
- [코드 저장소](https://github.com/your-repo-url)
- 관련 자료: Hinton et al. (2015), Kim et al. (2016), Goodfellow et al. (2014)