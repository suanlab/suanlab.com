---
title: "[논문 리뷰] ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory"
date: "2026-01-01"
excerpt: "With the growing adoption of large language model agents in persistent real-world roles, they naturally encounter continuous streams of tasks. A key limitation, however, is their failure to learn from..."
category: "Paper Review"
tags: ["Paper Review","cs.AI","cs.CL","cs.AI"]
thumbnail: "/assets/images/blog/20260101-paper-2509-25140-reasoningbank-scaling-agent-se.jpg"
---

# [논문 리뷰] ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory

## TL;DR

본 논문에서는 지속적인 작업 스트림에 직면한 대규모 언어 모델 에이전트의 주요 한계점인 축적된 상호 작용 기록으로부터 학습하지 못하는 문제를 해결하기 위해 ReasoningBank라는 새로운 메모리 프레임워크를 제안한다. ReasoningBank는 에이전트의 자체 판단에 따른 성공 및 실패 경험으로부터 일반화 가능한 추론 전략을 추출하여 저장하고, 테스트 시점에 관련 메모리를 검색하여 상호 작용에 활용함으로써 에이전트가 시간이 지남에 따라 더욱 능숙해지도록 한다. 또한, Memory-aware Test-Time Scaling (MaTTS)을 도입하여 각 작업에 더 많은 컴퓨팅 자원을 할당함으로써 에이전트의 상호 작용 경험을 확장하고 학습 과정을 가속화 및 다양화한다. 웹 브라우징 및 소프트웨어 엔지니어링 벤치마크에서 ReasoningBank는 기존 메모리 메커니즘보다 우수한 성능을 보였으며, MaTTS는 이러한 성능 향상을 더욱 증폭시켰다. 본 연구는 메모리 기반 경험 확장이 에이전트의 자가 진화를 가능하게 하는 새로운 확장 차원임을 입증한다.

## 연구 배경 및 동기

최근 대규모 언어 모델 (Large Language Model, LLM) 기반 에이전트가 웹 브라우징, 소프트웨어 개발, 고객 지원 등 다양한 분야에서 활용되면서, 이들의 지속적인 작업 스트림 처리 능력에 대한 관심이 높아지고 있다. 이러한 에이전트들은 현실 세계에서 끊임없이 변화하는 환경과 상호 작용하며, 다양한 과제를 해결해야 한다. 그러나 기존 LLM 기반 에이전트들은 축적된 상호 작용 기록으로부터 학습하지 못하는 근본적인 한계를 가지고 있다. 즉, 과거의 경험에서 얻은 귀중한 통찰력을 활용하지 못하고, 과거에 저지른 실수를 반복하는 경향이 있다는 것이다. 이는 에이전트의 장기적인 성능 향상을 저해하는 주요 요인으로 작용한다.

기존 접근 방식들은 주로 다음과 같은 한계를 가진다. 첫째, 단순히 모든 상호 작용 기록을 저장하는 방식은 메모리 용량의 제한으로 인해 장기간의 경험을 효과적으로 관리하기 어렵다. 둘째, 성공적인 작업 루틴만을 저장하는 방식은 실패 경험으로부터 얻을 수 있는 중요한 교훈을 간과하게 된다. 셋째, 에이전트의 추론 과정을 명시적으로 모델링하지 않고, 단순히 데이터 패턴을 학습하는 방식은 일반화 능력이 부족하여 새로운 상황에 대한 적응력이 떨어진다.

이러한 문제점을 해결하기 위해 본 연구에서는 ReasoningBank라는 새로운 메모리 프레임워크를 제안한다. ReasoningBank는 에이전트가 스스로 판단한 성공 및 실패 경험으로부터 일반화 가능한 추론 전략을 추출하여 저장하고, 테스트 시점에 관련 메모리를 검색하여 상호 작용에 활용함으로써 에이전트가 시간이 지남에 따라 더욱 능숙해지도록 한다. 또한, Memory-aware Test-Time Scaling (MaTTS)을 도입하여 각 작업에 더 많은 컴퓨팅 자원을 할당함으로써 에이전트의 상호 작용 경험을 확장하고 학습 과정을 가속화 및 다양화한다.

본 연구는 다음과 같은 구체적인 연구 질문에 답하고자 한다.

1.  ReasoningBank는 기존 메모리 메커니즘보다 에이전트의 성능을 향상시키는가?
2.  MaTTS는 ReasoningBank와 결합하여 에이전트의 성능을 더욱 향상시키는가?
3.  메모리 기반 경험 확장은 에이전트의 자가 진화를 가능하게 하는 새로운 확장 차원인가?

## 관련 연구

본 연구는 LLM 기반 에이전트의 메모리 관리 및 학습 능력 향상과 관련된 다양한 선행 연구를 기반으로 한다. 주요 관련 연구는 다음과 같다.

1.  **ReAct (Yao et al., 2022)**: ReAct는 LLM 에이전트가 행동과 추론을 번갈아 수행하도록 함으로써 복잡한 작업을 해결하는 능력을 향상시키는 방법론이다. ReAct는 에이전트가 현재 상태에 대한 생각을 생성하고, 이를 바탕으로 다음 행동을 결정하도록 한다. 본 연구의 ReasoningBank는 ReAct와 유사하게 에이전트의 추론 과정을 명시적으로 모델링하지만, 과거 경험으로부터 학습하는 능력을 추가적으로 제공한다.

2.  **Reflexion (Shinn et al., 2023)**: Reflexion은 에이전트가 자신의 행동을 자체적으로 평가하고, 이를 바탕으로 다음 행동을 개선하도록 하는 방법론이다. Reflexion은 에이전트가 시행착오를 통해 학습하고, 장기적인 목표를 달성하는 데 도움을 준다. 본 연구의 ReasoningBank는 Reflexion과 유사하게 에이전트의 자체 평가를 활용하지만, 일반화 가능한 추론 전략을 추출하여 저장하고 재사용하는 데 초점을 맞춘다.

3.  **Memory Augmented Neural Networks (MANN) (Graves et al., 2014)**: MANN은 외부 메모리 모듈을 사용하여 신경망의 학습 능력을 향상시키는 방법론이다. MANN은 신경망이 과거의 경험을 저장하고, 이를 바탕으로 현재의 입력을 처리하도록 한다. 본 연구의 ReasoningBank는 MANN과 유사하게 외부 메모리 모듈을 사용하지만, LLM 에이전트의 추론 과정을 모델링하고, 일반화 가능한 추론 전략을 저장하는 데 특화되어 있다.

4.  **Retrieval-Augmented Generation (RAG) (Lewis et al., 2020)**: RAG는 외부 지식 베이스에서 관련 정보를 검색하여 LLM의 생성 능력을 향상시키는 방법론이다. RAG는 LLM이 더 정확하고 풍부한 정보를 생성하도록 돕는다. 본 연구의 ReasoningBank는 RAG와 유사하게 외부 메모리에서 관련 정보를 검색하지만, 에이전트의 과거 경험으로부터 학습된 추론 전략을 검색하는 데 초점을 맞춘다.

5.  **Chain of Thought (CoT) (Wei et al., 2022)**: CoT는 LLM이 복잡한 추론 과정을 단계별로 설명하도록 함으로써 문제 해결 능력을 향상시키는 방법론이다. CoT는 LLM이 더 논리적이고 체계적인 답변을 생성하도록 돕는다. 본 연구의 ReasoningBank는 CoT와 유사하게 에이전트의 추론 과정을 명시적으로 모델링하지만, 과거 경험으로부터 학습하고, 일반화 가능한 추론 전략을 저장하는 데 초점을 맞춘다.

다음 표는 본 논문과 선행 연구의 차별점을 요약한 것이다.

| 연구 | 주요 특징 | ReasoningBank와의 차별점 |
| --- | --- | --- |
| ReAct | 행동과 추론을 번갈아 수행 | 과거 경험 학습 능력 추가 |
| Reflexion | 자체 평가 기반 행동 개선 | 일반화 가능한 추론 전략 저장 및 재사용 |
| MANN | 외부 메모리 모듈 활용 | LLM 에이전트 추론 과정 특화 |
| RAG | 외부 지식 베이스 검색 | 에이전트 경험 기반 추론 전략 검색 |
| CoT | 단계별 추론 과정 설명 | 과거 경험 학습 및 전략 저장 |

## 핵심 기여

본 논문의 주요 기여는 다음과 같이 요약될 수 있다.

1.  **ReasoningBank: 새로운 메모리 프레임워크**: 에이전트가 스스로 판단한 성공 및 실패 경험으로부터 일반화 가능한 추론 전략을 추출하여 저장하고 재사용하는 새로운 메모리 프레임워크를 제안한다. ReasoningBank는 단순히 경험을 저장하는 것이 아니라, 경험으로부터 학습하고 미래의 행동을 개선하는 데 중요한 역할을 한다. 이는 에이전트의 지속적인 학습 능력 향상에 기여한다.

2.  **Memory-aware Test-Time Scaling (MaTTS)**: 테스트 시간 동안 에이전트의 상호 작용 경험을 확장하여 학습 과정을 가속화하고 다양화하는 새로운 방법론을 도입한다. MaTTS는 각 작업에 더 많은 컴퓨팅 자원을 할당함으로써 에이전트가 다양한 시나리오를 경험하고, ReasoningBank가 더 풍부하고 일반화된 추론 전략을 학습하도록 돕는다.

3.  **메모리 기반 경험 확장의 새로운 확장 차원 제시**: 메모리 기반 경험 확장이 에이전트의 자가 진화를 가능하게 하는 새로운 확장 차원임을 입증한다. 이는 기존의 모델 크기 확장, 데이터셋 크기 확장과 더불어 에이전트의 성능을 향상시키는 새로운 가능성을 제시한다.

4.  **실험적 검증**: 웹 브라우징 및 소프트웨어 엔지니어링 벤치마크에서 ReasoningBank와 MaTTS의 효과를 실험적으로 검증한다. 실험 결과는 ReasoningBank가 기존 메모리 메커니즘보다 우수한 성능을 보였으며, MaTTS는 이러한 성능 향상을 더욱 증폭시켰음을 보여준다.

본 연구의 novelty는 다음과 같이 설명될 수 있다. 기존 연구들은 주로 에이전트의 메모리 관리 또는 테스트 시점 스케일링에 초점을 맞추었지만, 본 연구는 이 두 가지를 통합하여 에이전트의 자가 진화를 가능하게 하는 새로운 프레임워크를 제시한다. 또한, 본 연구는 에이전트의 성공 및 실패 경험으로부터 일반화 가능한 추론 전략을 추출하여 저장하고 재사용하는 새로운 메모리 메커니즘을 제안함으로써 에이전트의 학습 능력을 향상시킨다.

## 제안 방법론

본 연구에서 제안하는 방법론은 ReasoningBank와 MaTTS로 구성된다.

**ReasoningBank**: ReasoningBank는 에이전트가 스스로 판단한 성공 및 실패 경험으로부터 일반화 가능한 추론 전략을 추출하여 저장하고 재사용하는 메모리 프레임워크이다. ReasoningBank는 다음과 같은 단계로 작동한다.

1.  **경험 수집**: 에이전트는 환경과 상호 작용하면서 경험을 수집한다. 경험은 상태, 행동, 결과로 구성된다.
2.  **경험 평가**: 에이전트는 수집된 경험을 스스로 평가한다. 평가는 경험이 성공적인지 실패했는지 여부를 판단하는 것을 포함한다.
3.  **추론 전략 추출**: 에이전트는 성공 및 실패 경험으로부터 일반화 가능한 추론 전략을 추출한다. 추론 전략은 특정 상태에서 특정 행동을 수행하는 것이 성공 또는 실패로 이어지는 이유를 설명하는 규칙 또는 패턴이다.
4.  **메모리 저장**: 에이전트는 추출된 추론 전략을 ReasoningBank에 저장한다.
5.  **메모리 검색**: 에이전트는 현재 상태와 관련된 추론 전략을 ReasoningBank에서 검색한다.
6.  **행동 결정**: 에이전트는 검색된 추론 전략을 바탕으로 다음 행동을 결정한다.

**MaTTS**: MaTTS는 테스트 시간 동안 에이전트의 상호 작용 경험을 확장하여 학습 과정을 가속화하고 다양화하는 방법론이다. MaTTS는 각 작업에 더 많은 컴퓨팅 자원을 할당함으로써 에이전트가 다양한 시나리오를 경험하고, ReasoningBank가 더 풍부하고 일반화된 추론 전략을 학습하도록 돕는다. MaTTS는 다음과 같은 두 가지 전략을 사용한다.

1.  **병렬 확장**: 에이전트는 동일한 작업에 대해 여러 개의 독립적인 시도를 동시에 수행한다.
2.  **순차 확장**: 에이전트는 이전 시도의 결과를 바탕으로 다음 시도를 결정한다.

**핵심 수식**:

1.  **에이전트의 정책**: 에이전트의 정책 $\pi(a|s, M)$는 상태 $s$와 메모리 $M$이 주어졌을 때 행동 $a$를 선택할 확률을 나타낸다.

    $$
    \pi(a|s, M) = P(a|s, M)
    $$

    여기서 $a$는 행동, $s$는 상태, $M$은 ReasoningBank 메모리 모듈을 나타낸다.

2.  **ReasoningBank 업데이트 규칙**: ReasoningBank의 업데이트 규칙은 다음과 같이 표현될 수 있다.

    $$
    M_{t+1} = \text{Update}(M_t, s_t, a_t, r_t)
    $$

    여기서 $M_t$는 시간 $t$에서의 ReasoningBank, $s_t$는 시간 $t$에서의 상태, $a_t$는 시간 $t$에서의 행동, $r_t$는 시간 $t$에서의 보상을 나타낸다. Update 함수는 새로운 경험 ($s_t, a_t, r_t$)을 기반으로 ReasoningBank를 업데이트하는 역할을 한다.

3.  **메모리 검색 확률**: 현재 상태 $s$와 유사한 과거 상태 $s'$을 ReasoningBank에서 검색할 확률은 다음과 같이 정의될 수 있다.

    $$
    P(s'|s) = \text{Similarity}(s, s')
    $$

    여기서 Similarity 함수는 두 상태 간의 유사도를 측정하는 함수이다. 예를 들어, 코사인 유사도나 유클리드 거리 등이 사용될 수 있다.

4.  **행동 선택 확률**: 에이전트가 ReasoningBank에서 검색된 메모리를 활용하여 행동을 선택할 확률은 다음과 같이 표현될 수 있다.

    $$
    P(a|s, M) = \sum_{s' \in M} P(a|s', M) \cdot P(s'|s)
    $$

    여기서 $P(a|s', M)$은 과거 상태 $s'$에서 행동 $a$를 선택했을 때의 확률을 나타낸다. 이 수식은 현재 상태 $s$와 유사한 과거 상태 $s'$에서 성공적인 행동 $a$를 선택할 확률을 높이는 방식으로 작동한다.

5.  **MaTTS를 사용한 경험 확장**: MaTTS를 사용하여 에이전트의 경험을 확장하는 과정은 다음과 같이 표현될 수 있다.

    $$
    \{s_i, a_i, r_i\}_{i=1}^{N} = \text{MaTTS}(s_0, \pi, M)
    $$

    여기서 $s_0$는 초기 상태, $\pi$는 에이전트의 정책, $M$은 ReasoningBank, $N$은 확장된 경험의 수를 나타낸다. MaTTS 함수는 초기 상태 $s_0$에서 에이전트의 정책 $\pi$와 ReasoningBank $M$을 사용하여 다양한 시나리오를 탐색하고, 새로운 경험 $\{s_i, a_i, r_i\}_{i=1}^{N}$을 생성한다.


## 실험 결과

저자들은 WebArena와 SWE-bench 벤치마크에서 ReasoningBank의 성능을 평가했다. 실험 결과, ReasoningBank는 기존 메모리 메커니즘 대비 일관된 성능 향상을 보였으며, MaTTS와 결합 시 더욱 큰 폭의 성능 개선이 관찰되었다.

## 결론

ReasoningBank는 LLM 에이전트가 과거 경험으로부터 학습하여 지속적으로 발전할 수 있는 새로운 메모리 프레임워크이다. 성공 및 실패 경험 모두로부터 일반화 가능한 추론 전략을 추출하여 저장하고, 테스트 시점에 이를 활용함으로써 에이전트의 성능을 향상시킨다. MaTTS를 통한 경험 확장은 학습 과정을 가속화하며, 메모리 기반 경험 확장이 에이전트 자가 진화의 새로운 확장 차원임을 입증한다.

## 참고 자료

- [arXiv 논문](https://arxiv.org/abs/2509.25140)
