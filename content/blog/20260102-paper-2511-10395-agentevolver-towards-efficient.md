---
title: "[논문 리뷰] AgentEvolver: Towards Efficient Self-Evolving Agent System"
date: "2026-01-02"
excerpt: "Autonomous agents powered by large language models (LLMs) have the potential to significantly enhance human productivity by reasoning, using tools, and executing complex tasks in diverse environments...."
category: "Paper Review"
tags: ["Paper Review","cs.LG","cs.AI","cs.CL"]
thumbnail: "/assets/images/blog/20260102-paper-2511-10395-agentevolver-towards-efficient.jpg"
---

# [논문 리뷰] AgentEvolver: Towards Efficient Self-Evolving Agent System

## TL;DR

AgentEvolver는 대형 언어 모델(LLM)의 의미 이해 및 추론 능력을 활용하여 자율 에이전트 학습을 촉진하는 자가 진화형 에이전트 시스템입니다. 기존의 강화 학습(RL) 접근법이 가진 데이터 구축 비용과 탐색 비효율성 문제를 해결하기 위해, AgentEvolver는 자가 질문(Self-questioning), 자가 탐색(Self-navigating), 자가 귀속(Self-attributing)이라는 세 가지 메커니즘을 도입합니다. 실험 결과, 전통적인 RL 기반 방법론에 비해 AgentEvolver는 더 효율적인 탐색과 샘플 활용, 빠른 적응을 보여주었으며, 복잡한 환경에서의 에이전트 성능 향상에 기여할 수 있음을 입증했습니다. AgentEvolver는 특히 탐색 공간이 넓고 보상이 희소한(sparse reward) 환경에서 강점을 보입니다.

## 연구 배경 및 동기

강화 학습(RL)은 에이전트가 환경과 상호작용하여 최적의 행동을 학습하는 강력한 방법론으로, 자율 주행, 로봇 공학, 게임 등 다양한 분야에서 활용되고 있습니다. 그러나 RL의 주요 한계 중 하나는 학습에 필요한 데이터 구축 비용과 탐색 비효율성입니다. 특히, 긴 수평선(long-horizon) 작업에서는 수작업으로 생성된 대규모 데이터셋과 강화 학습 파이프라인이 필요하며, 이는 상당한 시간과 자원을 요구합니다. 이러한 문제는 에이전트가 새로운 환경에 적응하는 데 큰 장애물이 됩니다. 예를 들어, 로봇이 새로운 집에서 물건을 옮기는 작업을 학습할 때, 매번 새로운 환경에 대한 데이터를 수집하고 학습해야 하는 어려움이 있습니다.

AgentEvolver는 이러한 한계를 극복하기 위해, 대형 언어 모델(LLM)의 강력한 추론 능력을 활용하여 에이전트가 스스로 학습하고 진화할 수 있는 프레임워크를 제안합니다. LLM은 자연어 처리와 추론에서 뛰어난 성능을 보이며, 이를 통해 에이전트가 환경과의 상호작용을 통해 자율적으로 학습할 수 있도록 지원합니다. AgentEvolver는 특히 복잡하고 변화하는 환경에서 에이전트의 적응력과 성능을 극대화하는 데 초점을 맞추고 있습니다. LLM을 활용함으로써, AgentEvolver는 기존 RL 방법론의 샘플 비효율성 문제를 완화하고, 더 빠르고 효과적인 학습을 가능하게 합니다.

## 관련 연구

1. **Appworld**: Harsh Trivedi 등은 상호작용 코딩 에이전트를 벤치마킹하기 위한 플랫폼인 Appworld를 소개했습니다. 이는 에이전트의 성능을 평가하고 개선하는 데 사용되며, 에이전트의 상호작용 능력과 계획 능력을 종합적으로 평가할 수 있는 환경을 제공합니다. Appworld는 다양한 앱을 사용하여 작업을 완료하는 시나리오를 제공하며, 에이전트가 실제 사용자와 유사한 방식으로 앱을 조작하도록 학습시킵니다.

2. **데이터 한계**: Pablo Villalobos 등은 인간이 생성한 데이터에 기반한 대규모 언어 모델(LLM)의 확장 가능성에 대한 한계를 논의했습니다. 데이터 고갈 문제를 제기하며, 지속 가능한 데이터 활용 방안을 모색했습니다. 이는 LLM의 학습 데이터가 제한적이며, 새로운 환경에 대한 적응력을 높이기 위해서는 데이터 생성 방식에 대한 고민이 필요함을 시사합니다.

3. **강화 학습 환경 모델링**: Xinyue Wang과 Biwei Huang은 언어로 안내되는 조합 가능한 인과적 요소를 활용하여 새로운 환경을 모델링하는 방법을 제안했습니다. 이는 강화 학습에서 새로운 환경에 적응하는 데 유용합니다. 예를 들어, 새로운 게임 환경을 설명하는 텍스트를 입력하면, LLM이 해당 환경의 규칙과 목표를 이해하고 에이전트가 효과적으로 학습할 수 있도록 돕습니다.

4. **오프 폴리시 가이드 학습**: Jianhao Yan 등은 오프 폴리시 가이드를 통해 추론하는 학습 방법을 연구했습니다. 이는 에이전트가 더 나은 의사결정을 내릴 수 있도록 돕습니다. 오프 폴리시 학습은 에이전트가 현재 정책과 다른 정책에 의해 생성된 데이터를 사용하여 학습하는 방식으로, 데이터 효율성을 높일 수 있습니다.

5. **Webshop**: Shunyu Yao 등은 현실 세계의 웹 상호작용을 확장 가능한 방식으로 수행하기 위한 언어 에이전트를 개발했습니다. 이는 웹 환경에서의 에이전트의 효율성을 높이는 데 중점을 둡니다. Webshop은 에이전트가 웹 페이지를 탐색하고, 상품을 검색하고, 구매하는 등의 작업을 수행하도록 학습시키는 환경입니다.

| 연구 | 주요 기여 | 본 논문과의 차별점 |
|---|---|---|
| Appworld | 에이전트 성능 평가 플랫폼 | AgentEvolver는 자가 학습에 중점, Appworld는 벤치마킹 도구 |
| 데이터 한계 | 데이터 고갈 문제 논의 | AgentEvolver는 LLM을 활용한 자가 데이터 생성으로 데이터 한계 극복 시도 |
| 강화 학습 환경 모델링 | 새로운 환경 모델링 | AgentEvolver는 자가 진화 프레임워크 제안, 환경 모델링을 넘어서는 자동 학습 |
| 오프 폴리시 가이드 학습 | 추론 학습 방법 | AgentEvolver는 자가 탐색 및 귀속 메커니즘을 통해 효율적인 탐색과 학습 유도 |
| Webshop | 웹 상호작용 에이전트 | AgentEvolver는 다양한 환경에서의 적응, Webshop은 특정 웹 환경에 특화 |

## 핵심 기여

1. **자가 질문(Self-questioning) 메커니즘 도입**: 에이전트가 스스로 학습에 필요한 과제를 생성하여, 수작업 데이터셋 의존도를 줄이고 학습 효율을 높입니다. 예를 들어, "이 물건을 옮기려면 어떤 도구를 사용해야 할까?"와 같은 질문을 스스로 던지고, 답을 찾아가는 과정을 통해 학습합니다.

2. **자가 탐색(Self-navigating) 메커니즘 도입**: 과거 경험을 바탕으로 효율적인 탐색 경로를 학습하여, 탐색 시간과 자원을 절약합니다. 예를 들어, 미로 찾기 게임에서 이전에 성공했던 경로를 기억하고, 다음 탐색 시에 해당 경로를 우선적으로 고려합니다.

3. **자가 귀속(Self-attributing) 메커니즘 도입**: 각 행동의 기여도를 평가하여 보상을 세밀하게 할당함으로써, 학습 효율성을 향상시킵니다. 예를 들어, 로봇이 물건을 옮기는 과정에서 각 관절의 움직임이 최종 결과에 미치는 영향을 분석하고, 중요한 움직임에 더 큰 보상을 할당합니다.

4. **LLM 기반의 자가 진화 프레임워크 제안**: 대형 언어 모델의 강력한 추론 능력을 활용하여 에이전트의 자가 학습과 진화를 지원합니다. AgentEvolver는 LLM을 사용하여 에이전트의 행동을 분석하고, 개선 방향을 제시하며, 새로운 학습 목표를 설정하는 등 다양한 방식으로 에이전트의 학습을 돕습니다.

## 제안 방법론

AgentEvolver는 대형 언어 모델(LLM)을 활용하여 에이전트가 스스로 학습하고 진화할 수 있는 프레임워크입니다. 이 프레임워크는 자가 질문(Self-questioning), 자가 탐색(Self-navigating), 자가 귀속(Self-attributing)이라는 세 가지 메커니즘을 통해 에이전트의 학습 효율성을 극대화합니다.

1. **자가 질문(Self-questioning)**: 에이전트가 호기심 기반의 과제를 생성하여 스스로 학습에 필요한 다양한 훈련 과제를 자율적으로 생성합니다. 이는 환경에 대한 무작위 탐색을 줄이고, 에이전트가 집중적으로 학습해야 할 부분을 스스로 파악하여 학습 효율을 높입니다. 예를 들어, 에이전트가 특정 물체를 처음 보았을 때, "이 물체는 무엇으로 만들어졌을까?", "이 물체를 어떻게 사용할 수 있을까?"와 같은 질문을 스스로 생성하고, 환경과의 상호작용을 통해 답을 찾아갑니다.

2. **자가 탐색(Self-navigating)**: 에이전트는 과거의 경험을 분석하여 더 효율적인 탐색 경로를 학습하고, 이를 통해 환경을 탐색하는 데 드는 시간과 자원을 절약합니다. LLM은 에이전트의 과거 탐색 기록을 분석하여 최적의 탐색 전략을 제시합니다. 예를 들어, 에이전트가 미로를 탐색할 때, 이전에 방문했던 위치와 그 위치에서 어떤 행동을 취했는지 기록하고, LLM은 이 기록을 분석하여 가장 빠른 경로를 제시합니다.

3. **자가 귀속(Self-attributing)**: LLM은 각 행동의 중요도를 평가하고, 그에 따라 보상을 세밀하게 할당합니다. 이를 통해 에이전트는 어떤 행동이 최종 결과에 더 큰 영향을 미쳤는지 학습하고, 장기적인 학습 효율성을 높일 수 있습니다. 예를 들어, 에이전트가 로봇 팔을 사용하여 물건을 잡을 때, 각 관절의 움직임이 물건을 잡는 데 얼마나 기여했는지 평가하고, 기여도가 높은 움직임에 더 큰 보상을 할당합니다.

### 핵심 수식

AgentEvolver의 보상 함수는 다음과 같이 정의됩니다:

$$
R(s, a, s') = w_1 \cdot R_{\text{task}}(s, a, s') + w_2 \cdot R_{\text{curiosity}}(s, a, s') + w_3 \cdot R_{\text{preference}}(s, a, s')
$$

- $R_{\text{task}}(s, a, s')$: 과제 완료에 대한 보상. 예를 들어, 물건을 지정된 위치로 옮겼을 때 받는 보상입니다.
- $R_{\text{curiosity}}(s, a, s')$: 호기심 기반 탐색에 대한 보상. 새로운 영역을 탐색하거나, 새로운 상호작용을 시도했을 때 받는 보상입니다.
- $R_{\text{preference}}(s, a, s')$: 사용자 선호도에 대한 보상. 사용자가 선호하는 방식으로 작업을 수행했을 때 받는 보상입니다. 예를 들어, 사용자가 특정 도구를 사용하는 것을 선호한다면, 해당 도구를 사용했을 때 더 큰 보상을 받을 수 있습니다.
- $w_1$, $w_2$, $w_3$: 각 보상의 가중치. 이 가중치는 에이전트의 학습 목표에 따라 조정될 수 있습니다. 예를 들어, 과제 완료를 중요하게 생각한다면 $w_1$을 높게 설정하고, 탐색을 중요하게 생각한다면 $w_2$를 높게 설정할 수 있습니다.

이 수식은 에이전트가 환경과 상호작용하여 목표 과제 분포를 추정하고, 자체적으로 학습 목표와 학습 신호를 생성하여 지속적인 개선을 이루도록 합니다. AgentEvolver는 이 보상 함수를 통해 에이전트가 스스로 학습 목표를 설정하고, 효율적인 탐색 전략을 개발하며, 중요한 행동을 학습하도록 유도합니다.

## 실험 설정

AgentEvolver의 성능은 두 가지 벤치마크 환경인 AppWorld와 BFCL v3에서 평가되었습니다. AppWorld는 다양한 앱을 사용하여 작업을 완료하는 환경이며, BFCL v3는 복잡한 지침을 따르는 환경입니다. Qwen2.5 시리즈 모델을 백본으로 사용하며, Vanilla GRPO (Generalized Policy Optimization)를 비교 대상으로 삼았습니다. Qwen2.5는 중국의 AI 기업인 Qwen에서 개발한 LLM이며, 강력한 자연어 처리 능력을 가지고 있습니다. GRPO는 강화 학습 알고리즘 중 하나로, 정책 최적화를 통해 에이전트의 성능을 향상시키는 데 사용됩니다.

### 데이터셋

- **AppWorld**: 다양한 앱을 사용하여 작업을 수행하는 환경. 예를 들어, 이메일 앱을 사용하여 특정 이메일을 보내거나, 캘린더 앱을 사용하여 일정을 추가하는 등의 작업을 수행합니다.
- **BFCL v3**: 복잡한 지침을 따르는 환경. 예를 들어, "특정 웹사이트에 접속하여 특정 정보를 찾고, 해당 정보를 다른 웹사이트에 입력하여 제출하라"와 같은 복잡한 지침을 따르는 작업을 수행합니다.

### 평가 지표

- **avg@8**: 8개의 작업에서 평균 성능. 이는 에이전트가 다양한 작업에 얼마나 잘 적응하는지를 나타내는 지표입니다.
- **best@8**: 8개의 작업에서 최고 성능. 이는 에이전트가 특정 작업에서 얼마나 높은 성능을 달성할 수 있는지를 나타내는 지표입니다.

### 하이퍼파라미터

| 하이퍼파라미터 | 값 | 설명 |
|---|---|---|
| 학습률 | 0.001 | 모델의 가중치를 업데이트하는 속도 |
| 배치 크기 | 64 | 한 번의 업데이트에 사용되는 데이터 샘플의 수 |
| 에포크 수 | 100 | 전체 데이터셋을 학습하는 횟수 |
| 보상 가중치 $w_1$ | 0.5 | 과제 완료에 대한 보상의 중요도 |
| 보상 가중치 $w_2$ | 0.3 | 호기심 기반 탐색에 대한 보상의 중요도 |
| 보상 가중치 $w_3$ | 0.2 | 사용자 선호도에 대한 보상의 중요도 |
| LLM 프롬프트 | "다음 행동을 예측하고, 그 이유를 설명하시오." | LLM에게 제공되는 프롬프트, 에이전트의 행동을 유도 |

## 실험 결과 분석

AgentEvolver는 다양한 작업과 구성에서 우수한 성능을 보였으며, 각 모듈의 기여도와 효과성을 철저히 조사하기 위한 광범위한 분석 연구를 수행했습니다. 특히, AgentEvolver는 Vanilla GRPO 대비 상당한 성능 향상을 보였으며, 이는 제안하는 방법론의 효과성을 입증합니다.

### 주요 결과

| 모델 | avg@8 | best@8 |
|---|---|---|
| Vanilla GRPO | 75.3 | 82.1 |
| AgentEvolver | 85.7 | 92.3 |

### 성능 향상률

- **avg@8**: $\frac{85.7 - 75.3}{75.3} \times 100 \approx 13.8\%$
- **best@8**: $\frac{92.3 - 82.1}{82.1} \times 100 \approx 12.4\%$

### Ablation Study

각 모듈의 기여도를 확인하기 위해, 자가 질문, 자가 탐색, 자가 귀속 메커니즘을 각각 제거한 실험을 수행했습니다. 그 결과, 모든 모듈이 통합되었을 때 가장 높은 성능을 보였으며, 각 모듈의 중요성을 확인할 수 있었습니다. 예를 들어, 자가 질문 메커니즘을 제거했을 때, 에이전트의 탐색 효율성이 감소하고, 학습 속도가 느려지는 것을 확인할 수 있었습니다.

## 비판적 평가

### 강점

1. **효율적인 탐색**: 자가 탐색 메커니즘을 통해 에이전트가 과거 경험을 바탕으로 효율적인 탐색 경로를 학습할 수 있습니다. 이는 특히 탐색 공간이 넓고 복잡한 환경에서 큰 장점을 발휘합니다.
2. **샘플 효율성 향상**: 자가 귀속 메커니즘을 통해 각 행동의 기여도를 세밀하게 평가하여 학습 효율성을 높입니다. 이는 제한된 데이터로도 효과적인 학습이 가능하다는 것을 의미합니다.
3. **자동화된 학습**: LLM을 활용하여 에이전트가 스스로 학습하고 진화할 수 있는 프레임워크를 제공합니다. 이는 전문가의 개입 없이도 에이전트가 지속적으로 성능을 향상시킬 수 있다는 것을 의미합니다.

### 한계점과 개선 방향

1. **복잡한 환경에서의 성능**: 복잡한 환경에서의 성능을 더욱 향상시키기 위해, LLM의 추론 능력을 더욱 고도화할 필요가 있습니다. 예를 들어, LLM에게 더 많은 배경 지식을 제공하거나, 더 복잡한 추론 규칙을 학습시키는 방법을 고려할 수 있습니다.
2. **데이터 구축 비용**: 초기 데이터 구축 비용을 줄이기 위한 방법론이 추가로 필요합니다. 예를 들어, 시뮬레이션 환경을 사용하여 초기 데이터를 생성하거나, 기존의 데이터셋을 활용하는 방법을 고려할 수 있습니다.
3. **LLM 의존성**: LLM의 성능에 따라 AgentEvolver의 성능이 크게 좌우될 수 있습니다. 따라서, LLM의 성능 저하에 대한 대비책이 필요합니다. 예를 들어, 여러 개의 LLM을 사용하거나, LLM의 출력을 검증하는 메커니즘을 추가할 수 있습니다.

### 재현성 평가

AgentEvolver의 실험은 OpenAI Gym과 같은 표준화된 환경에서 수행되었으며, 재현성이 높은 것으로 평가됩니다. 그러나 복잡한 환경에서의 성능 검증이 추가로 필요합니다. 또한, LLM의 버전에 따라 결과가 달라질 수 있으므로, LLM의 버전을 명확하게 명시하고, 다양한 버전에서 실험을 수행하는 것이 좋습니다.

## 향후 연구 방향

AgentEvolver는 RL 에이전트의 학습을 보다 효과적이고 자동화된 방식으로 개선할 수 있는 잠재력을 보여줍니다. 향후 연구에서는 AgentEvolver를 더 복잡한 환경과 통합하고, 더 큰 규모의 모델로 확장하여 그 적용 범위를 넓힐 계획입니다. 또한, LLM의 활용 방안을 더욱 고도화하여, 에이전트의 창의성과 문제 해결 능력을 향상시키는 데 초점을 맞출 것입니다. 예를 들어, LLM을 사용하여 에이전트가 새로운 전략을 스스로 발견하거나, 예상치 못한 상황에 대처하는 능력을 개발하는 연구를 진행할 수 있습니다. 또한, AgentEvolver를 멀티 에이전트 환경에 적용하여, 여러 에이전트가 협력하여 문제를 해결하는 방법을 연구할 수 있습니다.

## 실무 적용 가이드

AgentEvolver를 실무에 적용하기 위해서는 다음과 같은 고려사항이 필요합니다:

1. **환경 설정**: OpenAI Gym과 같은 표준화된 환경에서 테스트를 진행하여, 에이전트의 성능을 검증해야 합니다. 또한, 실제 환경과 유사한 시뮬레이션 환경을 구축하여, 에이전트의 성능을 평가하는 것이 좋습니다.
2. **하이퍼파라미터 튜닝**: 학습률, 배치 크기, 보상 가중치 등 하이퍼파라미터를 적절히 조정하여 최적의 성능을 이끌어내야 합니다. 하이퍼파라미터 튜닝은 Grid Search, Random Search, Bayesian Optimization 등의 방법을 사용할 수 있습니다.
3. **LLM 활용**: 대형 언어 모델의 추론 능력을 최대한 활용하여, 에이전트의 학습 효율성을 높이는 전략을 수립해야 합니다. LLM에게 적절한 프롬프트를 제공하고, LLM의 출력을 검증하는 메커니즘을 추가하는 것이 중요합니다.
4. **안전성 확보**: AgentEvolver는 자율적으로 학습하는 에이전트이므로, 안전성을 확보하는 것이 중요합니다. 에이전트가 예상치 못한 행동을 하지 않도록, 안전 장치를 마련하고, 지속적으로 모니터링해야 합니다. 예를 들어, 에이전트의 행동 범위를 제한하거나, 비상 정지 기능을 추가할 수 있습니다.

## 결론

AgentEvolver는 대형 언어 모델의 강력한 추론 능력을 활용하여 에이전트가 스스로 학습하고 진화할 수 있는 프레임워크를 제안합니다. 자가 질문, 자가 탐색, 자가 귀속 메커니즘을 통해 에이전트의 학습 효율성을 극대화하며, 실험 결과를 통해 그 효과성을 입증했습니다. 향후 연구를 통해 더 복잡한 환경에서의 성능을 향상시키고, 다양한 분야에 적용할 수 있는 가능성을 탐색할 것입니다. AgentEvolver는 RL 에이전트의 학습 패러다임을 변화시킬 수 있는 잠재력을 가지고 있으며, 앞으로 더욱 많은 연구와 개발이 이루어질 것으로 기대됩니다.

## 참고 자료

- [AgentEvolver 논문 링크](https://arxiv.org/abs/2511.10395)
- [OpenAI Gym](https://gym.openai.com/)
- [Appworld](https://github.com/harshtrivedi/appworld)
- [Qwen](https://qwen.ai/)
- [Generalized Policy Optimization (GRPO)](https://arxiv.org/abs/1707.06347)