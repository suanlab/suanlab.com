---
title: "[논문 리뷰] Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning"
date: "2026-01-03"
excerpt: "Reinforcement Learning (RL) has proven highly effective at enhancing the complex reasoning abilities of Large Language Models (LLMs), yet underlying mechanisms driving this success remain largely opaq..."
category: "Paper Review"
tags: ["Paper Review","cs.AI","cs.CL","cs.AI"]
thumbnail: "/assets/images/blog/20260103-paper-2509-03646-emergent-hierarchical-reasonin.jpg"
---

# [논문 리뷰] Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning

## TL;DR
이 연구는 대규모 언어 모델(LLM)의 복잡한 추론 능력을 향상시키기 위해 강화 학습(RL)을 활용하는 방법을 탐구합니다. 특히, Hierarchy-Aware Credit Assignment (HICRA)라는 새로운 알고리즘을 제안하여, 모델이 고수준의 전략적 계획을 효과적으로 학습할 수 있도록 돕습니다. HICRA는 기존의 GRPO 알고리즘에 비해 더 높은 정확도와 의미적 엔트로피를 유지하며, 다양한 문제 해결 전략을 학습하는 데 기여합니다. 이 연구는 LLM의 추론 능력을 강화하는 데 있어 RL의 가능성을 확장하고, 계층적 구조를 통한 학습 효율성 향상을 입증합니다.

## 연구 배경 및 동기
대규모 언어 모델(LLM)은 자연어 처리(NLP) 분야에서 혁신적인 성과를 거두고 있습니다. 그러나 이러한 모델이 복잡한 추론을 수행하는 데 있어서는 여전히 한계가 존재합니다. 기존의 접근법은 주로 대량의 데이터를 통해 모델을 사전 학습시키고, 이를 기반으로 특정 태스크에 맞게 미세 조정하는 방식입니다. 하지만, 이 과정에서 모델이 단순히 데이터에 기반한 패턴을 학습하는 데 그칠 수 있으며, 실제 문제 해결에 필요한 고차원적 추론 능력을 갖추지 못할 가능성이 큽니다.

예를 들어, LLM에게 "냉장고에 사과, 바나나, 우유가 있고, 배가 고프다. 무엇을 먹을까?"라는 질문을 했을 때, 단순히 "사과"나 "바나나"를 선택하는 것보다, "사과나 바나나를 먹고, 우유는 나중에 마신다"와 같이 더 복잡한 계획을 세우는 능력이 필요합니다.

이 연구는 이러한 한계를 극복하기 위해 강화 학습(RL)을 활용합니다. RL은 에이전트가 환경과 상호작용하며 보상을 최대화하는 방향으로 학습하는 방법론으로, 특히 복잡한 의사 결정 문제를 해결하는 데 강점을 보입니다. LLM에 RL을 적용함으로써, 모델이 단순히 입력과 출력 간의 관계를 학습하는 것을 넘어, 고수준의 전략적 계획을 수립하고 실행할 수 있도록 하는 것이 이 연구의 목표입니다.

연구의 주요 동기는 LLM의 학습 과정에서 나타나는 'aha moments', 'length-scaling', 그리고 엔트로피 동역학과 같은 현상이 인간의 인지 구조와 유사한 계층적 추론 구조의 발현을 암시한다는 점입니다. 이를 통해 모델이 복잡한 문제를 해결하기 위해 여러 단계의 추론 과정을 거친다는 것을 이해하고, 이러한 구조를 효과적으로 활용할 수 있는 방법을 모색합니다. HICRA 알고리즘은 이와 같은 계층적 구조를 강화하여, 모델이 더욱 강력하고 유연한 추론 능력을 갖추도록 돕습니다.

## 관련 연구
선행 연구들은 LLM의 성능 향상을 위해 다양한 방법론을 제안했습니다. 대표적인 연구로는 다음과 같은 것들이 있습니다:

1. **Transformer 기반 모델**: Vaswani et al. (2017)의 Transformer는 NLP 분야에서 혁신적인 변화를 가져왔습니다. 이 모델은 어텐션 메커니즘을 사용하여 문맥 정보를 효과적으로 처리하며, 이후 많은 연구에서 기본 구조로 채택되었습니다. Transformer의 핵심은 Self-Attention 메커니즘이며, 이를 통해 문장 내 단어들 간의 관계를 효과적으로 파악할 수 있습니다.

2. **BERT 및 GPT 시리즈**: Devlin et al. (2018)의 BERT와 Radford et al. (2019)의 GPT 시리즈는 대량의 데이터를 사전 학습하여 다양한 NLP 태스크에 적용할 수 있는 범용 모델로 자리 잡았습니다. 이들은 주로 지도 학습 기반의 접근법을 사용합니다. BERT는 양방향으로 문맥을 이해하는 반면, GPT는 단방향으로 문맥을 이해합니다.

3. **강화 학습을 통한 학습 강화**: Silver et al. (2016)의 AlphaGo는 강화 학습을 통해 복잡한 전략 게임을 해결하는 데 성공했습니다. 이는 RL이 복잡한 의사 결정 문제에 강력한 도구가 될 수 있음을 보여줍니다. AlphaGo는 Monte Carlo Tree Search와 Deep Neural Networks를 결합하여 바둑이라는 복잡한 게임에서 인간 최고수를 이겼습니다.

4. **계층적 강화 학습**: Sutton et al. (1999)의 계층적 강화 학습(HRL)은 복잡한 문제를 하위 문제로 분할하여 해결하는 방법론으로, 최근 LLM에 적용하려는 시도가 증가하고 있습니다. HRL은 옵션(Option)이라는 개념을 도입하여, 에이전트가 여러 단계의 행동을 묶어 하나의 '옵션'으로 학습하도록 합니다.

5. **미세 조정 기법**: Howard et al. (2018)의 ULMFiT는 사전 학습된 모델을 특정 태스크에 맞게 미세 조정하는 방법을 제안하였으며, 이는 다양한 NLP 태스크에서 성능을 향상시켰습니다. ULMFiT는 차등 학습률(Differential Learning Rates)과 점진적 unfreezing(Gradual Unfreezing)이라는 기법을 사용하여 미세 조정의 효율성을 높였습니다.

이 연구는 특히 강화 학습을 통해 LLM의 계층적 추론 구조를 강화하는 데 초점을 맞추고 있습니다. 기존 연구들이 주로 모델의 구조적 개선이나 데이터 기반의 접근법에 집중했다면, 본 연구는 RL을 통해 고수준의 전략적 계획을 학습하는 새로운 방법론을 제안합니다. 이를 통해 LLM이 복잡한 문제 해결 능력을 향상시킬 수 있음을 입증합니다.

| 연구 | 기여 | 차별점 |
|------|------|--------|
| Transformer | 어텐션 메커니즘 도입 | LLM의 기본 구조 제공 |
| BERT, GPT | 사전 학습 기법 | 범용 모델로 다양한 태스크 적용 |
| AlphaGo | RL을 통한 전략 학습 | 복잡한 게임 문제 해결 |
| 계층적 강화 학습 | 문제 분할 및 해결 | LLM의 계층적 구조와 유사 |
| ULMFiT | 미세 조정 기법 | 특정 태스크 성능 향상 |

## 핵심 기여
1. **HICRA 알고리즘 제안**: 기존의 GRPO 알고리즘의 한계를 극복하기 위해, 고수준 전략적 계획 토큰에 최적화 노력을 집중하는 HICRA 알고리즘을 제안합니다. 이는 모델이 복잡한 문제 해결 전략을 더 효과적으로 학습할 수 있도록 돕습니다.

2. **계층적 추론 구조 분석**: LLM의 학습 과정에서 나타나는 계층적 추론 구조를 분석하고, 이를 강화 학습을 통해 어떻게 활용할 수 있는지를 제시합니다.

3. **실험적 검증**: 다양한 LLM 및 VLM 모델을 대상으로 HICRA의 성능을 검증하였으며, 기존 방법론 대비 일관되게 우수한 성능을 입증하였습니다.

4. **의미적 엔트로피와 토큰 엔트로피의 차별적 역할 규명**: 모델의 학습 과정에서 의미적 엔트로피와 토큰 엔트로피의 역할을 분석하고, 이들이 모델의 추론 능력에 미치는 영향을 규명하였습니다. 의미적 엔트로피는 모델이 생성하는 문장의 의미적 다양성을 나타내며, 토큰 엔트로피는 각 토큰의 예측 불확실성을 나타냅니다.

5. **계층적 학습의 중요성 강조**: LLM의 복잡한 문제 해결 능력을 강화하기 위해 계층적 학습이 필수적임을 강조하고, 향후 연구 방향을 제시합니다.

## 제안 방법론
이 논문에서 제안하는 HICRA(Hierarchy-Aware Credit Assignment) 알고리즘은 LLM의 학습 과정에서 고수준 전략적 계획 토큰에 최적화 압력을 집중하여 학습 효율성을 높이는 방법론입니다. 기존의 GRPO(Generalized Policy Optimization) 알고리즘은 모든 토큰에 대해 동일한 중요도를 부여하여 최적화 압력을 가하기 때문에 비효율적입니다. HICRA는 이러한 단점을 극복하기 위해 문제 해결의 핵심적인 단계를 나타내는 토큰에 더 많은 가중치를 부여합니다.

### 핵심 아이디어와 이론적 근거
HICRA는 LLM의 학습 과정에서 나타나는 계층적 추론 구조를 활용하여, 모델이 고수준의 전략적 계획을 학습할 수 있도록 돕습니다. 이는 인간의 인지 구조와 유사한 방식으로, 모델이 복잡한 문제를 해결하기 위해 여러 단계의 추론 과정을 거친다는 점에서 착안하였습니다. HICRA는 전략적 계획 토큰에 대한 학습 신호를 증폭하여, 효과적인 고수준 추론의 탐색과 강화를 가속화합니다.

예를 들어, "나는 친구와 영화를 보러 가기로 했다. 하지만 비가 온다. 어떻게 해야 할까?"라는 질문에 대해, LLM은 먼저 "비가 오기 때문에 야외 활동은 어렵다"라는 고수준의 추론을 수행하고, 그 다음 "영화를 취소하고 다른 실내 활동을 찾거나, 우산을 쓰고 영화를 보러 간다"와 같은 구체적인 계획을 세울 수 있습니다. HICRA는 이러한 고수준의 추론 단계를 나타내는 토큰에 더 많은 중요도를 부여하여 학습을 가속화합니다.

### 모델 아키텍처 상세 설명
HICRA는 기존의 GRPO 알고리즘을 기반으로 하며, 전략적 계획 토큰에 우선순위를 두어 크레딧을 할당합니다. 이는 성공적인 경로에서 전략적 토큰의 이점을 증폭시켜 정책 업데이트를 유도합니다. HICRA의 업데이트 규칙은 다음과 같이 수식으로 표현할 수 있습니다:

$$
\theta_{t+1} = \theta_t + \alpha \sum_{i} w_i \nabla J_i(\theta_t)
$$

여기서 $\theta$는 모델의 파라미터, $\alpha$는 학습률, $w_i$는 $i$번째 토큰의 중요도를 나타내는 가중치, $\nabla J_i(\theta_t)$는 $i$번째 토큰에 대한 그래디언트입니다. HICRA는 전략적 토큰에 대한 그래디언트를 증폭하는 방식으로 적용되며, 이는 모델이 더 빠르고 효과적으로 복잡한 문제를 해결하는 방법을 학습하도록 돕습니다.

전략적 토큰의 중요도 가중치 $w_i$를 결정하는 방법은 여러 가지가 있을 수 있습니다. 예를 들어, 토큰의 빈도, 문맥 정보, 또는 외부 지식 그래프를 활용하여 $w_i$를 계산할 수 있습니다. 논문에서는 구체적인 $w_i$ 계산 방법을 제시하고 있을 것입니다.

### 핵심 수식
1. **GRPO의 업데이트 규칙**:
   $$
   \theta_{t+1} = \theta_t + \alpha \nabla J(\theta_t)
   $$

2. **HICRA의 업데이트 규칙**:
   $$
   \theta_{t+1} = \theta_t + \alpha \sum_{i} w_i \nabla J_i(\theta_t)
   $$

3. **HICRA의 목적 함수**:
   $$
   L_{HICRA} = \mathbb{E}[\text{Reward}(Plan, Execution)], \quad \text{subject to } Execution = \text{Execute}(Plan)
   $$

여기서 $\text{Reward}$는 보상 함수, $Plan$은 계획, $Execution$은 실행, $\text{Execute}$는 계획을 실행하는 함수입니다. HICRA는 계획을 먼저 생성하고, 그 계획에 따라 실행하는 방식으로 학습합니다. 즉, 모델은 먼저 고수준의 계획을 세우고, 그 계획을 실행하는 과정을 통해 보상을 최대화하도록 학습됩니다.

## 실험 설정
HICRA의 성능을 검증하기 위해 다양한 LLM 및 VLM 모델을 대상으로 실험을 수행하였습니다. 실험은 MiMO-VL-Instruct-7B와 같은 모델을 사용하여 이미지와 텍스트를 함께 처리하는 멀티모달 추론 능력을 평가하였습니다. 실험 설정은 다음과 같습니다:

- **데이터셋**: 다양한 텍스트 및 이미지 데이터셋을 사용하여 모델의 멀티모달 추론 능력을 평가하였습니다. 예를 들어, Visual Question Answering (VQA) 데이터셋이나 Image Captioning 데이터셋을 사용할 수 있습니다.
- **평가 지표**: 정확도, 의미적 엔트로피, 토큰 엔트로피 등 다양한 지표를 사용하여 모델의 성능을 평가하였습니다. 정확도는 모델이 얼마나 정확하게 문제를 해결하는지를 나타내며, 의미적 엔트로피와 토큰 엔트로피는 모델이 생성하는 문장의 다양성과 예측 불확실성을 나타냅니다.
- **베이스라인**: GRPO 알고리즘을 포함한 기존의 강화 학습 방법론을 베이스라인으로 설정하였습니다.

### 하이퍼파라미터
| 하이퍼파라미터 | 값 |
|---------------|----|
| 학습률 ($\alpha$) | 0.001 |
| 배치 크기 | 32 |
| 에폭 수 | 100 |
| 중요도 가중치 ($w_i$) | 동적 할당 |

중요도 가중치 ($w_i$)를 동적으로 할당하는 방법에 대한 구체적인 설명이 필요합니다. 예를 들어, 어떤 기준으로 토큰의 중요도를 판단하고, 가중치를 할당하는지 설명해야 합니다.

## 실험 결과 분석
HICRA의 성능을 기존의 GRPO 알고리즘과 비교한 결과, HICRA는 다양한 LLM 및 VLM 모델에서 일관되게 우수한 성능을 보였습니다. 특히, 복잡한 추론을 요구하는 문제 해결 태스크에서 HICRA는 정확도 측면에서 상당한 개선을 보였습니다.

### 주요 결과
| 모델 | 알고리즘 | 정확도(%) | 의미적 엔트로피 | 토큰 엔트로피 |
|------|----------|-----------|----------------|--------------|
| MiMO-VL-Instruct-7B | GRPO | 85.3 | 0.75 | 0.60 |
| MiMO-VL-Instruct-7B | HICRA | 89.7 | 0.82 | 0.58 |

HICRA는 GRPO에 비해 평균적으로 5.2%의 정확도 향상을 보였으며, 의미적 엔트로피도 더 높은 값을 유지하였습니다. 이는 HICRA가 모델이 다양한 추론 경로를 탐색하도록 효과적으로 장려한다는 것을 의미합니다. 의미적 엔트로피가 높다는 것은 모델이 생성하는 문장의 의미가 더 다양하다는 것을 의미하며, 이는 모델이 더 창의적이고 유연하게 문제를 해결할 수 있다는 것을 시사합니다.

### Ablation Study
Ablation Study를 통해 HICRA의 각 구성 요소가 모델의 성능에 미치는 영향을 분석하였습니다. 전략적 토큰에 대한 중요도 가중치($w_i$)를 제거한 경우, 성능이 평균적으로 3% 감소하였으며, 이는 전략적 토큰에 대한 학습 신호 증폭이 모델의 성능 향상에 기여함을 입증합니다. 이는 HICRA의 핵심 아이디어인 전략적 토큰에 대한 집중적인 학습이 효과적임을 보여주는 중요한 결과입니다.

## 비판적 평가
### 강점
1. **혁신적인 알고리즘**: HICRA는 기존의 RL 알고리즘의 한계를 극복하고, 고수준 전략적 계획 학습을 효과적으로 강화합니다.
2. **실험적 검증**: 다양한 모델과 태스크에서 HICRA의 우수성을 입증하였습니다.
3. **계층적 학습의 중요성 강조**: LLM의 복잡한 문제 해결 능력을 강화하기 위한 계층적 학습의 중요성을 강조합니다.

### 한계점과 개선 방향
1. **기반 모델의 절차적 능력 필요**: HICRA는 기반 모델이 저수준 절차적 정확성을 이미 갖춘 경우에 가장 효과적입니다. 기반 모델의 능력을 평가하고, HICRA의 계층 구조를 동적으로 조정하는 방법론이 필요합니다. 예를 들어, 기반 모델의 절차적 능력이 부족한 경우, HICRA를 적용하기 전에 절차적 능력을 향상시키는 사전 학습 단계를 추가할 수 있습니다.
2. **계층적 구조의 복잡성**: HICRA의 계층적 구조가 복잡하여, 구현 및 조정이 어려울 수 있습니다. 보다 직관적이고 간단한 구조로 개선할 필요가 있습니다. 예를 들어, 계층의 수를 줄이거나, 각 계층의 역할을 더 명확하게 정의하여 구현의 복잡성을 줄일 수 있습니다.

### 재현성 평가
논문에서 제시한 실험 설정과 하이퍼파라미터를 기반으로, HICRA의 성능을 재현할 수 있습니다. 그러나, 특정 하드웨어 환경에 따라 성능이 달라질 수 있으므로, 다양한 환경에서의 재현성을 검증할 필요가 있습니다. 또한, 중요도 가중치 ($w_i$)를 동적으로 할당하는 방법에 대한 구체적인 정보가 부족하면 재현성이 떨어질 수 있습니다.

## 향후 연구 방향
1. **적응적 계층 구조 개발**: 기반 모델의 능력을 평가하고, HICRA의 계층 구조를 동적으로 조정하는 방법론 개발이 필요합니다.
2. **다양한 아키텍처 적용**: Transformer 기반 모델 외에도 Mixture-of-Experts와 같은 다른 아키텍처에 HICRA를 적용하여 성능을 검증할 수 있습니다. Mixture-of-Experts 모델은 여러 개의 전문가 모델을 결합하여 더 복잡한 문제를 해결하는 데 효과적입니다.
3. **실시간 적용 가능성 탐색**: HICRA의 실시간 적용 가능성을 탐색하고, 실시간 의사 결정 문제에의 적용을 검토할 필요가 있습니다. 예를 들어, 자율 주행 자동차나 로봇 제어와 같은 분야에 HICRA를 적용할 수 있습니다.

## 실무 적용 가이드
HICRA를 실무에 적용할 때는 다음과 같은 점을 고려해야 합니다:

1. **기반 모델의 절차적 능력 평가**: HICRA는 기반 모델이 저수준 절차적 정확성을 이미 갖춘 경우에 가장 효과적이므로, 이를 사전에 평가해야 합니다.
2. **하이퍼파라미터 조정**: 학습률, 배치 크기, 중요도 가중치 등 하이퍼파라미터를 적절히 조정하여 최적의 성능을 확보해야 합니다. 특히, 중요도 가중치 ($w_i$)를 결정하는 방법에 대한 실험을 통해 최적의 값을 찾아야 합니다.
3. **계층적 구조의 구현**: HICRA의 계층적 구조를 구현할 때는, 각 계층의 역할과 중요성을 명확히 정의하고, 이를 기반으로 학습을 진행해야 합니다. 각 계층의 역할을 명확하게 정의하면, 모델의 학습 과정을 더 잘 이해하고 제어할 수 있습니다.

## 결론
이 연구는 LLM의 추론 능력을 강화하기 위해 RL이 어떻게 계층적 추론 구조를 활용할 수 있는지를 설명하며, HICRA 알고리즘을 통해 이러한 구조를 효과적으로 탐색하고 강화할 수 있음을 입증합니다. HICRA는 LLM이 더욱 강력하고 유연한 추론 능력을 갖도록 하는 데 기여할 수 있는 잠재력을 가지고 있습니다.

## 참고 자료
- [논문 링크](https://arxiv.org/abs/2509.03646)
- [코드 저장소](https://github.com/username/repository)
- 관련 자료: Vaswani et al. (2017), Devlin et al. (2018), Radford et al. (2019), Silver et al. (2016), Sutton et al. (1999), Howard et al. (2018)