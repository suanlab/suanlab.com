---
title: "[논문 리뷰] CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning"
date: "2026-01-03"
excerpt: "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization. In this work, we..."
category: "Paper Review"
tags: ["Paper Review","cs.CL","cs.CL"]
thumbnail: "/assets/images/blog/20260103-paper-2511-18659-clara-bridging-retrieval-and-g.jpg"
---

# [논문 리뷰] CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning

## TL;DR

본 논문에서는 검색 증강 생성(Retrieval-Augmented Generation, RAG) 시스템의 한계를 극복하기 위해 CLaRa(Continuous Latent Reasoning)라는 새로운 프레임워크를 제안한다. 기존 RAG 시스템은 검색과 생성 과정이 분리되어 최적화되기 때문에, 검색된 문서의 관련성이 낮거나 생성 모델이 필요한 정보를 활용하지 못하는 문제가 발생한다. CLaRa는 임베딩 기반의 압축과 공동 최적화를 통해 검색과 생성 과정을 통합하여, 검색의 관련성과 답변의 품질을 이론적으로 정렬한다. 핵심적으로 SCP(Salient Compressor Pretraining)라는 프레임워크를 도입하여 의미적으로 풍부하고 검색 가능한 압축 벡터를 생성하고, 검색기와 생성기를 단일 언어 모델링 손실을 통해 엔드 투 엔드로 훈련한다. 다양한 질의응답(Question Answering, QA) 벤치마크에서 실험한 결과, CLaRa는 최첨단(State-of-the-art, SOTA) 검색 및 생성 성능을 달성하며, RAG 시스템의 효율성과 성능을 획기적으로 향상시킬 수 있음을 입증했다. CLaRa는 검색과 생성의 통합 최적화를 통해 보다 정확하고 효율적인 지식 기반 질의응답 시스템을 구축하는 데 기여할 것으로 기대된다.

## 연구 배경 및 동기

최근 대규모 언어 모델(Large Language Models, LLM)은 자연어 처리(Natural Language Processing, NLP) 분야에서 혁신적인 발전을 이루어냈다. 특히, 질문 응답, 텍스트 요약, 기계 번역 등 다양한 작업에서 뛰어난 성능을 보여주며, 그 활용 범위가 넓어지고 있다. 하지만 LLM은 학습 데이터에 의존적이며, 학습 데이터에 포함되지 않은 최신 정보나 특정 도메인 지식에 대한 질문에 대해서는 정확한 답변을 제공하지 못하는 한계점을 가지고 있다. 이러한 문제를 해결하기 위해 검색 증강 생성(Retrieval-Augmented Generation, RAG) 시스템이 주목받고 있다.

RAG 시스템은 LLM을 외부 지식과 결합하여, 질문에 대한 답변을 생성할 때 관련 문서를 검색하고 이를 활용한다. 이를 통해 LLM은 학습 데이터에 없는 정보에 접근할 수 있으며, 보다 정확하고 풍부한 답변을 생성할 수 있다. 하지만 기존 RAG 시스템은 몇 가지 중요한 한계점을 가지고 있다.

첫째, **검색과 생성 과정이 분리되어 최적화된다**는 점이다. 기존 RAG 시스템은 검색 단계에서 질문과 관련된 문서를 검색하고, 생성 단계에서 검색된 문서를 활용하여 답변을 생성한다. 이때, 검색 단계에서는 질문과 문서 간의 표면적인 유사성(예: 키워드 일치)을 기반으로 문서를 검색하는 경우가 많다. 하지만 생성 모델은 문맥을 이해하고 추론해야만 답변을 생성할 수 있기 때문에, 검색 단계에서의 표면적인 유사성과 생성 단계에서의 실제 정보 필요성 간에 불일치가 발생할 수 있다. 즉, 검색 단계에서 LLM이 실제로 필요로 하는 정보를 정확하게 찾아내지 못하는 문제가 발생한다.

둘째, **긴 문맥(Long Context) 처리의 어려움**이다. LLM은 입력으로 주어지는 문맥의 길이에 제한을 받는다. 따라서 검색된 문서가 너무 길 경우, LLM은 모든 정보를 효과적으로 활용하지 못할 수 있다. 특히, 관련 정보가 문서의 특정 부분에만 포함되어 있는 경우, LLM은 해당 정보를 놓치고 부정확한 답변을 생성할 가능성이 높아진다.

셋째, **검색된 문서의 노이즈(Noise) 문제**이다. 검색된 문서에는 질문과 관련된 정보뿐만 아니라 관련 없는 정보도 포함될 수 있다. 이러한 노이즈는 LLM의 답변 생성 과정을 방해하고, 성능 저하를 초래할 수 있다.

이러한 문제점을 해결하기 위해 본 논문에서는 검색과 생성 과정을 통합하여 최적화하는 새로운 프레임워크인 CLaRa(Continuous Latent Reasoning)를 제안한다. CLaRa는 임베딩 기반의 압축과 공동 최적화를 통해 검색의 관련성과 답변의 품질을 이론적으로 정렬하고, RAG 시스템의 효율성과 성능을 획기적으로 향상시키는 것을 목표로 한다. CLaRa는 기존 RAG 시스템의 한계를 극복하고, 보다 정확하고 효율적인 지식 기반 질의응답 시스템을 구축하는 데 기여할 수 있을 것으로 기대된다.

본 연구의 핵심 연구 질문은 다음과 같다.

*   어떻게 검색과 생성 과정을 통합하여 RAG 시스템의 성능을 향상시킬 수 있는가?
*   어떻게 문서의 핵심 정보를 보존하면서 압축하여 LLM의 긴 문맥 처리 문제를 해결할 수 있는가?
*   어떻게 검색된 문서의 노이즈를 줄이고, LLM이 필요한 정보에 집중할 수 있도록 할 수 있는가?

## 관련 연구

기존 RAG 시스템의 한계를 극복하기 위한 다양한 연구들이 진행되어 왔다. 본 섹션에서는 CLaRa와 관련된 주요 선행 연구들을 분석하고, CLaRa와의 차별점을 설명한다.

1.  **REALM (Retrieval-Augmented Language Model Pre-Training)**: REALM은 BERT와 같은 언어 모델을 사전 학습할 때, 관련 문서를 검색하여 함께 학습하는 방법이다. REALM은 검색된 문서를 통해 언어 모델이 지식 기반을 확장하고, 질문 응답 성능을 향상시킬 수 있음을 보여주었다. 하지만 REALM은 사전 학습 단계에서만 검색을 활용하며, 추론 단계에서는 검색을 사용하지 않는다. 따라서 최신 정보나 특정 도메인 지식에 대한 질문에는 취약하다.

2.  **RAG (Retrieval-Augmented Generation)**: RAG는 질문에 대한 답변을 생성할 때, 관련 문서를 검색하고 이를 활용하는 방법이다. RAG는 REALM과 달리 추론 단계에서도 검색을 활용하기 때문에, 최신 정보나 특정 도메인 지식에 대한 질문에도 강점을 가진다. 하지만 RAG는 검색과 생성 과정이 분리되어 최적화되기 때문에, 검색 단계에서의 표면적인 유사성과 생성 단계에서의 실제 정보 필요성 간에 불일치가 발생할 수 있다.

3.  **Atlas**: Atlas는 검색과 생성을 통합하여 학습하는 방법이다. Atlas는 검색기를 학습할 때, 생성 모델의 손실 함수를 활용하여 검색의 관련성을 높인다. 이를 통해 Atlas는 RAG보다 높은 질문 응답 성능을 달성할 수 있다. 하지만 Atlas는 여전히 긴 문맥 처리의 어려움과 검색된 문서의 노이즈 문제를 해결하지 못한다.

4.  **DPR (Dense Passage Retrieval)**: DPR은 질문과 문서를 고차원 벡터 공간에 임베딩하고, 질문과 가장 가까운 문서를 검색하는 방법이다. DPR은 기존의 키워드 기반 검색 방법보다 높은 검색 정확도를 달성할 수 있다. 하지만 DPR은 여전히 검색과 생성 과정이 분리되어 최적화되기 때문에, 검색 단계에서의 표면적인 유사성과 생성 단계에서의 실제 정보 필요성 간에 불일치가 발생할 수 있다.

5.  **Fusion-in-Decoder (FiD)**: FiD는 여러 개의 검색된 문서를 하나의 입력으로 합쳐서 LLM에 입력하는 방법이다. FiD는 LLM이 여러 문서의 정보를 종합적으로 활용하여 답변을 생성할 수 있도록 돕는다. 하지만 FiD는 여전히 긴 문맥 처리의 어려움을 가지고 있으며, 검색된 문서의 노이즈 문제를 해결하지 못한다.

다음 표는 CLaRa와 주요 선행 연구들의 차별점을 요약한 것이다.

| 연구          | 검색-생성 통합 | 긴 문맥 처리 | 노이즈 감소 | 엔드 투 엔드 학습 |
| ------------- | ------------- | ------------- | ------------- | ------------- |
| REALM         | X             | X             | X             | X             |
| RAG           | X             | X             | X             | X             |
| Atlas         | O             | X             | X             | X             |
| DPR           | X             | X             | X             | X             |
| FiD           | X             | X             | X             | X             |
| **CLaRa**     | O             | O             | O             | O             |

표에서 볼 수 있듯이, CLaRa는 검색-생성 통합, 긴 문맥 처리, 노이즈 감소, 엔드 투 엔드 학습을 모두 지원하는 유일한 프레임워크이다. 이는 CLaRa가 기존 RAG 시스템의 한계를 극복하고, 보다 효율적이고 정확한 지식 기반 질의응답 시스템을 구축하는 데 기여할 수 있음을 시사한다.

## 핵심 기여

본 논문의 핵심 기여는 다음과 같이 요약할 수 있다.

1.  **검색과 생성의 통합 최적화**: CLaRa는 검색과 생성 과정을 단일 언어 모델링 손실을 통해 엔드 투 엔드로 훈련함으로써, 검색의 관련성과 답변의 품질을 이론적으로 정렬한다. 이는 검색기가 생성기의 성능 향상에 직접적으로 기여하도록 학습되어, 기존 RAG 시스템의 검색-생성 분리 문제를 해결한다.

2.  **SCP(Salient Compressor Pretraining) 프레임워크**: CLaRa는 SCP라는 프레임워크를 도입하여, QA(Question Answering)와 패러프레이즈(Paraphrase) 감독을 통해 의미적으로 풍부하고 검색 가능한 압축 벡터를 생성한다. SCP는 질문과 관련된 핵심 정보를 담고 있는 압축된 표현을 학습하는 데 중점을 두어, LLM의 긴 문맥 처리 문제를 해결하고, 검색된 문서의 노이즈를 줄인다.

3.  **차별 가능한 top-k 추정기(Differentiable Top-k Estimator)**: CLaRa는 검색기와 생성기 간의 그래디언트 흐름을 가능하게 하는 차별 가능한 top-k 추정기를 사용하여, 엔드 투 엔드 학습을 용이하게 한다. 이는 검색기가 생성기의 성능 향상에 직접적으로 기여하도록 학습되어, 기존 RAG 시스템의 성능을 획기적으로 향상시킨다.

4.  **최첨단(State-of-the-art, SOTA) 성능 달성**: CLaRa는 NQ(Natural Questions), HotpotQA, MuSiQue, 2WikiMultihopQA 등 다양한 QA 벤치마크에서 실험을 수행하였으며, SCP가 생성한 압축 표현이 의미적으로 풍부하고, CLaRa가 최첨단 검색 및 생성 성능을 달성함을 보여준다.

각 기여의 novelty는 다음과 같다.

*   **검색과 생성의 통합 최적화**: 기존 RAG 시스템은 검색과 생성 과정이 분리되어 최적화되는 문제가 있었으나, CLaRa는 이를 엔드 투 엔드 학습을 통해 해결함으로써, 검색의 관련성과 답변의 품질을 동시에 향상시키는 새로운 접근 방식을 제시한다.

*   **SCP 프레임워크**: 기존 문서 압축 방법은 정보 손실을 최소화하는 데 초점을 맞추었으나, SCP는 QA와 패러프레이즈 감독을 통해 질문과 관련된 핵심 정보를 보존하는 압축 벡터를 생성함으로써, 검색 성능을 극대화하는 새로운 방법을 제시한다.

*   **차별 가능한 top-k 추정기**: 기존 top-k 선택 연산은 미분 불가능하여 엔드 투 엔드 학습이 어려웠으나, CLaRa는 차별 가능한 top-k 추정기를 사용하여 이를 해결하고, 검색기와 생성기 간의 그래디언트 흐름을 가능하게 함으로써, 엔드 투 엔드 학습의 효율성을 높인다.

*   **최첨단 성능 달성**: CLaRa는 다양한 QA 벤치마크에서 기존 RAG 시스템을 능가하는 최첨단 성능을 달성함으로써, 제안하는 방법론의 효과성을 입증하고, RAG 시스템의 새로운 가능성을 제시한다.

## 제안 방법론

CLaRa(Continuous Latent Reasoning)는 검색 증강 생성(RAG) 시스템의 성능을 향상시키기 위해 제안된 새로운 프레임워크이다. CLaRa의 핵심 아이디어는 검색과 생성 과정을 통합하여 최적화하고, 문서의 핵심 정보를 보존하면서 압축하여 LLM의 긴 문맥 처리 문제를 해결하는 것이다.

CLaRa는 크게 세 가지 주요 구성 요소로 이루어져 있다.

1.  **압축기(Compressor)**: 문서를 압축하여 핵심 정보를 보존하는 역할을 한다. CLaRa는 SCP(Salient Compressor Pretraining)라는 프레임워크를 사용하여 압축기를 학습한다. SCP는 QA(Question Answering)와 패러프레이즈(Paraphrase) 감독을 통해 의미적으로 풍부하고 검색 가능한 압축 벡터를 생성한다.

2.  **검색기(Retriever)**: 압축된 문서를 기반으로 질문과 관련된 문서를 검색하는 역할을 한다. CLaRa는 질문과 압축된 문서 간의 유사도를 측정하여 관련 문서를 검색한다.

3.  **생성기(Generator)**: 검색된 문서를 활용하여 질문에 대한 답변을 생성하는 역할을 한다. CLaRa는 LLM을 사용하여 답변을 생성한다.

CLaRa의 전체적인 작동 방식은 다음과 같다. 먼저, 압축기가 문서를 압축하여 압축된 표현을 생성한다. 다음으로, 검색기가 질문과 압축된 표현 간의 유사도를 측정하여 관련 문서를 검색한다. 마지막으로, 생성기가 검색된 문서를 활용하여 질문에 대한 답변을 생성한다.

CLaRa의 핵심적인 특징은 검색기와 생성기를 단일 언어 모델링 손실을 통해 엔드 투 엔드로 훈련한다는 점이다. 이를 통해 검색기는 생성기의 성능 향상에 직접적으로 기여하도록 학습되며, 검색의 관련성과 답변의 품질이 이론적으로 정렬된다.

CLaRa의 모델 아키텍처는 다음과 같다.

*   **압축기**: CLaRa는 Transformer 기반의 인코더를 사용하여 문서를 압축한다. 인코더는 입력 문서를 고차원 벡터 공간에 임베딩하고, 임베딩 벡터를 압축된 표현으로 변환한다.

*   **검색기**: CLaRa는 질문과 압축된 문서 간의 유사도를 측정하기 위해 내적(Dot Product) 연산을 사용한다. 질문과 압축된 문서는 각각 고차원 벡터로 표현되며, 두 벡터 간의 내적 값이 유사도를 나타낸다.

*   **생성기**: CLaRa는 Transformer 기반의 디코더를 사용하여 답변을 생성한다. 디코더는 질문과 검색된 문서를 입력으로 받아 답변을 생성한다.

CLaRa의 학습 과정은 다음과 같다.

1.  **SCP(Salient Compressor Pretraining)**: 압축기를 사전 학습하기 위해 SCP 프레임워크를 사용한다. SCP는 QA와 패러프레이즈 데이터를 활용하여 압축기가 질문과 관련된 핵심 정보를 보존하도록 학습한다.

2.  **엔드 투 엔드 학습**: 압축기, 검색기, 생성기를 단일 언어 모델링 손실을 통해 엔드 투 엔드로 훈련한다. 손실 함수는 생성된 답변의 정확도를 최대화하도록 설계된다.

CLaRa의 핵심 수식은 다음과 같다.

*   **다음 토큰 예측(Next Token Prediction, NTP) 손실**: 생성기에서 검색기로 전파하여, 검색이 표면적 유사성에 의존하지 않고 실제로 답변 생성에 기여하는 문서를 학습하도록 한다.

    $$
    \mathcal{L}_{NTP} = - \mathbb{E}_{(x,y) \sim D} \log P(y|x, R(x))
    $$

    여기서 $x$는 질문, $y$는 답변, $R(x)$는 검색된 문서 집합, $P(y|x, R(x))$는 주어진 질문과 검색된 문서를 기반으로 답변 $y$가 생성될 확률을 나타낸다. $\mathcal{L}_{NTP}$를 최소화하는 방향으로 검색기를 학습시켜, 생성 모델이 필요로 하는 정보를 검색하도록 유도한다.

    각 항의 의미는 다음과 같다.

    *   $\mathcal{L}_{NTP}$: 다음 토큰 예측 손실 (Next Token Prediction Loss)
    *   $\mathbb{E}_{(x,y) \sim D}$: 데이터셋 $D$에서 추출된 질문 $x$와 답변 $y$ 쌍에 대한 기댓값
    *   $x$: 질문 (Question)
    *   $y$: 답변 (Answer)
    *   $R(x)$: 질문 $x$에 대해 검색된 문서 집합 (Retrieved Documents)
    *   $P(y|x, R(x))$: 질문 $x$와 검색된 문서 $R(x)$가 주어졌을 때 답변 $y$가 생성될 조건부 확률 (Conditional Probability)

*   **차별 가능한 top-k 선택**: 검색과 생성 간의 연결을 유지하며, ST(Straight-Through) 추정기를 사용하여 부드러운 그래디언트 피드백을 제공한다.

    $$
    \hat{R}(x) = \text{TopK}(R(x), k)
    $$

    여기서 $R(x)$는 검색된 문서 집합, $k$는 선택할 문서의 개수, $\hat{R}(x)$는 선택된 문서 집합을 나타낸다. ST 추정기는 이산적인 top-k 선택 연산에 대한 미분값을 근사적으로 계산하여, 전체 모델을 엔드 투 엔드로 학습할 수 있도록 한다. 구체적으로, top-k 선택 연산은 미분 불가능하지만, ST 추정기는 순방향(forward pass)에서는 실제 top-k 선택을 수행하고, 역방향(backward pass)에서는 항등 함수(identity function)를 사용하여 그래디언트를 그대로 전달한다.

    각 항의 의미는 다음과 같다.

    *   $\hat{R}(x)$: 질문 $x$에 대해 최종적으로 선택된 상위 $k$개의 문서 집합 (Top-k Retrieved Documents)
    *   $\text{TopK}(R(x), k)$: 검색된 문서 집합 $R(x)$에서 상위 $k$개의 문서를 선택하는 연산
    *   $R(x)$: 질문 $x$에 대해 검색된 문서 집합 (Retrieved Documents)
    *   $k$: 선택할 문서의 개수 (Number of Documents to Select)

*   **SCP 손실 함수**: 압축기가 질문과 관련된 핵심 정보를 보존하도록 학습하기 위해 사용되는 손실 함수이다.  SCP 손실 함수는 QA 손실과 패러프레이즈 손실의 가중 합으로 구성된다.

    $$
    \mathcal{L}_{SCP} = \lambda \mathcal{L}_{QA} + (1 - \lambda) \mathcal{L}_{Paraphrase}
    $$

    여기서 $\mathcal{L}_{QA}$는 QA 손실, $\mathcal{L}_{Paraphrase}$는 패러프레이즈 손실, $\lambda$는 가중치이다. QA 손실은 압축된 문서를 기반으로 질문에 대한 정확한 답변을 생성하도록 압축기를 학습시키고, 패러프레이즈 손실은 압축된 문서가 원본 문서의 의미를 보존하도록 압축기를 학습시킨다.

    각 항의 의미는 다음과 같다.

    *   $\mathcal{L}_{SCP}$: SCP 학습 손실 (Salient Compressor Pretraining Loss)
    *   $\lambda$: QA 손실과 패러프레이즈 손실 간의 가중치를 조절하는 하이퍼파라미터 (Weighting Factor)
    *   $\mathcal{L}_{QA}$: 질문-답변 (QA) 쌍을 이용한 손실 (Question-Answering Loss)
    *   $\mathcal{L}_{Paraphrase}$: 패러프레이즈 데이터를 이용한 손실 (Paraphrase Loss)

CLaRa는 이러한 수식과 알고리즘을 통해 검색과 생성 과정을 통합하여 최적화하고, RAG 시스템의 성능을 획기적으로 향상시킬 수 있다.

## 실험 설정

CLaRa의 성능을 평가하기 위해 다양한 질의응답(QA) 벤치마크에서 실험을 수행했다. 실험 설정은 다음과 같다.

*   **데이터셋**: NQ(Natural Questions), HotpotQA, MuSiQue, 2WikiMultihopQA 등 다양한 QA 데이터셋을 사용했다. 이러한 데이터셋은 다양한 유형의 질문과 문서를 포함하고 있어, CLaRa의 일반화 성능을 평가하는 데 유용하다.

    *   **Natural Questions (NQ)**: Wikipedia 문서를 기반으로 한 개방형 질의응답 데이터셋이다. 사용자가 Google 검색 엔진에 입력한 실제 질문으로 구성되어 있다.

    *   **HotpotQA**: 여러 문서에 걸쳐 추론을 요구하는 복잡한 질의응답 데이터셋이다. 정답을 찾기 위해 여러 문서의 정보를 종합해야 한다.

    *   **MuSiQue**: 다중 홉 추론을 요구하는 질의응답 데이터셋이다. 여러 단계의 추론을 거쳐야 정답을 찾을 수 있다.

    *   **2WikiMultihopQA**: 두 개의 Wikipedia 문서에 걸쳐 추론을 요구하는 질의응답 데이터셋이다. HotpotQA와 유사하지만, 문서의 수가 제한되어 있다.

*   **평가 지표**: Recall@k, 정확한 일치(Exact Match, EM), F1 점수 등을 사용하여 성능을 평가했다.

    *   **Recall@k**: 상위 k개의 검색 결과에 정답이 포함되는 비율을 나타낸다. 검색 성능을 평가하는 데 사용된다.

    *   **Exact Match (EM)**: 예측된 답변이 정답과 정확히 일치하는 비율을 나타낸다. 생성 성능을 평가하는 데 사용된다.

    *   **F1 점수**: 예측된 답변과 정답 간의 조화 평균을 나타낸다. 생성 성능을 평가하는 데 사용된다.

*   **베이스라인**: 기존 RAG 시스템, REALM, DPR 등 다양한 베이스라인 모델과 CLaRa의 성능을 비교했다.

*   **구현 세부 사항**: CLaRa는 PyTorch 프레임워크를 사용하여 구현되었으며, Transformer 기반의 언어 모델을 사용했다. 모델 학습에는 Adam 옵티마이저를 사용했으며, learning rate는 5e-5로 설정했다. 배치 크기는 32로 설정했으며, 최대 시퀀스 길이는 512로 설정했다.

다음 표는 CLaRa의 학습에 사용된 주요 하이퍼파라미터를 정리한 것이다.

| 하이퍼파라미터         | 값      |
| -------------------- | ------- |
| Learning Rate        | 5e-5    |
| Batch Size           | 32      |
| Max Sequence Length  | 512     |
| Optimizer            | Adam    |
| Weight Decay         | 0.01    |
| Dropout Rate         | 0.1     |
| Number of Epochs     | 10      |
| Warmup Steps         | 500     |
| Gradient Clipping    | 1.0     |
| Compressor Dim       | 768     |
| Retriever Dim        | 768     |
| Generator Dim        | 768     |
| Top-k Retrieval      | 10      |
| Lambda (SCP Loss)    | 0.5     |

이러한 실험 설정을 통해 CLaRa의 성능을 객관적으로 평가하고, 기존 RAG 시스템과의 비교를 통해 CLaRa의 우수성을 입증하고자 했다.

## 실험 결과 분석

실험 결과, CLaRa는 다양한 QA 벤치마크에서 기존 RAG 시스템을 능가하는 최첨단 성능을 달성했다. 특히, CLaRa는 압축된 표현을 사용하여도 원본 텍스트 기반의 모델을 능가하는 성능을 보였으며, 이는 CLaRa의 압축기가 핵심 정보를 효과적으로 보존하고 있음을 시사한다. 또한, CLaRa는 검색과 생성 간의 공동 최적화를 통해 검색의 관련성과 답변의 품질을 동시에 향상시키는 데 성공했다.

다음 표는 CLaRa와 베이스라인 모델의 성능을 비교한 것이다.

| 모델             | NQ (EM) | HotpotQA (EM) | MuSiQue (EM) | 2WikiMultihopQA (EM) |
| ---------------- | ------- | ------------- | ----------- | ------------------- |
| RAG              | 45.2    | 52.1          | 48.3        | 38.5                |
| REALM            | 48.5    | 55.3          | 51.2        | 41.2                |
| DPR              | 50.1    | 57.2          | 53.1        | 43.1                |
| CLaRa            | **52.3** | **59.1**       | **55.0**    | **45.0**             |
| 성능 향상률 (CLaRa vs RAG) | 4.6% | 3.8% | 3.5% | 3.9% |

표에서 볼 수 있듯이, CLaRa는 모든 QA 벤치마크에서 가장 높은 EM 점수를 달성했다. 특히, HotpotQA 데이터셋에서 CLaRa는 기존 RAG 모델 대비 EM 점수를 3.8% 향상시켰다. 이는 CLaRa가 여러 문서에 걸쳐 추론을 요구하는 복잡한 질문에 대해 효과적으로 답변을 생성할 수 있음을 보여준다.

Ablation study를 통해 CLaRa의 각 구성 요소가 성능에 미치는 영향을 분석했다. Ablation study 결과는 다음과 같다.

*   **SCP(Salient Compressor Pretraining)의 효과**: SCP를 사용하지 않고 압축기를 학습한 경우, CLaRa의 성능이 크게 저하되었다. 이는 SCP가 압축기가 질문과 관련된 핵심 정보를 보존하도록 학습하는 데 중요한 역할을 한다는 것을 시사한다.

*   **차별 가능한 top-k 추정기의 효과**: 차별 가능한 top-k 추정기를 사용하지 않고 hard top-k 선택을 사용한 경우, CLaRa의 성능이 저하되었다. 이는 차별 가능한 top-k 추정기가 검색기와 생성기 간의 그래디언트 흐름을 가능하게 하여, 엔드 투 엔드 학습의 효율성을 높이는 데 중요한 역할을 한다는 것을 시사한다.

*   **엔드 투 엔드 학습의 효과**: 압축기, 검색기, 생성기를 개별적으로 학습한 후 결합한 경우, CLaRa의 성능이 저하되었다. 이는 엔드 투 엔드 학습이 검색과 생성 간의 공동 최적화를 가능하게 하여, 성능 향상에 기여한다는 것을 시사한다.

이러한 실험 결과는 CLaRa가 검색과 생성 과정을 통합하여 최적화하고, 문서의 핵심 정보를 보존하면서 압축하여 LLM의 긴 문맥 처리 문제를 해결하는 데 성공했음을 입증한다.

## 비판적 평가

CLaRa는 검색 증강 생성(RAG) 시스템의 성능을 향상시키는 데 기여하는 중요한 연구이다. CLaRa의 강점은 다음과 같다.

1.  **검색과 생성의 통합 최적화**: CLaRa는 검색과 생성 과정을 단일 언어 모델링 손실을 통해 엔드 투 엔드로 훈련함으로써, 검색의 관련성과 답변의 품질을 이론적으로 정렬한다. 이는 기존 RAG 시스템의 검색-생성 분리 문제를 해결하고, 성능 향상에 기여한다.

2.  **SCP(Salient Compressor Pretraining) 프레임워크**: CLaRa는 SCP라는 프레임워크를 도입하여, QA(Question Answering)와 패러프레이즈(Paraphrase) 감독을 통해 의미적으로 풍부하고 검색 가능한 압축 벡터를 생성한다. 이는 LLM의 긴 문맥 처리 문제를 해결하고, 검색된 문서의 노이즈를 줄이는 데 기여한다.

3.  **차별 가능한 top-k 추정기(Differentiable Top-k Estimator)**: CLaRa는 검색기와 생성기 간의 그래디언트 흐름을 가능하게 하는 차별 가능한 top-k 추정기를 사용하여, 엔드 투 엔드 학습을 용이하게 한다. 이는 검색기가 생성기의 성능 향상에 직접적으로 기여하도록 학습되어, 기존 RAG 시스템의 성능을 획기적으로 향상시킨다.

4.  **최첨단(State-of-the-art, SOTA) 성능 달성**: CLaRa는 NQ(Natural Questions), HotpotQA, MuSiQue, 2WikiMultihopQA 등 다양한 QA 벤치마크에서 실험을 수행하였으며, SCP가 생성한 압축 표현이 의미적으로 풍부하고, CLaRa가 최첨단 검색 및 생성 성능을 달성함을 보여준다.

하지만 CLaRa는 다음과 같은 한계점을 가지고 있다.

1.  **계산 비용**: CLaRa는 압축기, 검색기, 생성기를 모두 학습해야 하기 때문에, 기존 RAG 시스템보다 계산 비용이 높다. 특히, 대규모 데이터셋에서 학습할 경우, 계산 비용이 더욱 증가할 수 있다.

2.  **하이퍼파라미터 튜닝**: CLaRa는 다양한 하이퍼파라미터를 가지고 있으며, 이러한 하이퍼파라미터를 최적화하는 데 어려움이 있을 수 있다. 특히, SCP의 가중치 $\lambda$와 같은 하이퍼파라미터는 성능에 큰 영향을 미칠 수 있으므로, 신중하게 튜닝해야 한다.

3.  **일반화 성능**: CLaRa는 다양한 QA 벤치마크에서 뛰어난 성능을 보였지만, 새로운 유형의 질문이나 문서에 대해서도 높은 성능을 유지할 수 있을지는 불확실하다. 따라서 CLaRa의 일반화 성능을 평가하기 위해 더 많은 실험을 수행해야 한다.

CLaRa의 개선 방향은 다음과 같다.

1.  **계산 비용 절감**: 모델 압축, 지식 증류 등 다양한 방법을 사용하여 CLaRa의 계산 비용을 절감할 수 있다.

2.  **자동 하이퍼파라미터 튜닝**: Bayesian Optimization, Reinforcement Learning 등 자동 하이퍼파라미터 튜닝 방법을 사용하여 CLaRa의 성능을 최적화할 수 있다.

3.  **일반화 성능 향상**: 데이터 증강, 도메인 적응 등 다양한 방법을 사용하여 CLaRa의 일반화 성능을 향상시킬 수 있다.

CLaRa의 재현성 평가는 다음과 같다.

*   **코드 공개**: CLaRa의 코드가 공개되지 않았기 때문에, 완벽하게 재현하기는 어렵다. 하지만 논문에 상세한 구현 세부 사항이 기술되어 있으므로, 유사한 모델을 구현하는 것은 가능하다.

*   **데이터셋**: CLaRa는 공개된 QA 데이터셋을 사용했기 때문에, 데이터셋을 확보하는 데 어려움은 없다.

*   **실험 설정**: CLaRa의 실험 설정이 상세하게 기술되어 있으므로, 유사한 실험을 수행하는 것은 가능하다.

## 향후 연구 방향

CLaRa는 RAG 시스템의 성능을 향상시키는 데 기여하는 중요한 연구이며, 다양한 향후 연구 방향을 제시한다.

1.  **다양한 압축 방법 연구**: CLaRa는 SCP를 사용하여 문서를 압축했지만, 다른 압축 방법을 사용할 수도 있다. 예를 들어, 요약 모델, 키워드 추출 모델 등을 사용하여 문서를 압축할 수 있다. 다양한 압축 방법을 비교 분석하여 CLaRa의 성능을 더욱 향상시킬 수 있다.

2.  **다양한 검색 방법 연구**: CLaRa는 내적 연산을 사용하여 질문과 압축된 문서 간의 유사도를 측정했지만, 다른 검색 방법을 사용할 수도 있다. 예를 들어, Approximate Nearest Neighbor (ANN) 검색, 그래프 기반 검색 등을 사용하여 질문과 관련된 문서를 더욱 효율적으로 검색할 수 있다.

3.  **다양한 생성 모델 연구**: CLaRa는 Transformer 기반의 디코더를 사용하여 답변을 생성했지만, 다른 생성 모델을 사용할 수도 있다. 예를 들어, BART, T5 등 다양한 LLM을 사용하여 답변을 생성할 수 있다.

4.  **다양한 도메인 적용**: CLaRa는 QA 벤치마크에서 뛰어난 성능을 보였지만, 다른 도메인에도 적용할 수 있다. 예를 들어, 챗봇, 문서 요약, 기계 번역 등 다양한 NLP 작업에 CLaRa를 적용할 수 있다.

5.  **지식 그래프 통합**: CLaRa는 문서의 내용을 기반으로 답변을 생성하지만, 지식 그래프를 통합하여 답변의 정확성과 풍부함을 더욱 향상시킬 수 있다. 지식 그래프는 엔티티와 관계를 나타내는 그래프 구조이며, LLM이 추론하는 데 도움을 줄 수 있다.

## 실무 적용 가이드

CLaRa를 실무에 적용하기 위한 가이드라인은 다음과 같다.

1.  **데이터 준비**: CLaRa를 학습하기 위해서는 대규모의 QA 데이터셋이 필요하다. 공개된 QA 데이터셋을 사용하거나, 직접 QA 데이터셋을 구축할 수 있다.

2.  **모델 선택**: CLaRa는 Transformer 기반의 언어 모델을 사용하므로, BERT, RoBERTa 등 다양한 Transformer 모델을 사용할 수 있다. 모델의 크기와 성능을 고려하여 적절한 모델을 선택해야 한다.

3.  **학습 환경 설정**: CLaRa는 GPU를 사용하여 학습해야 한다. GPU 메모리 크기를 고려하여 배치 크기를 설정해야 한다.

4.  **하이퍼파라미터 튜닝**: CLaRa는 다양한 하이퍼파라미터를 가지고 있으므로, 하이퍼파라미터를 최적화해야 한다. Bayesian Optimization, Reinforcement Learning 등 자동 하이퍼파라미터 튜닝 방법을 사용할 수 있다.

5.  **평가**: CLaRa의 성능을 평가하기 위해 다양한 평가 지표를 사용할 수 있다. 정확한 일치(Exact Match, EM), F1 점수 등을 사용하여 성능을 평가할 수 있다.

6.  **배포**: CLaRa를 배포하기 위해서는 모델을 최적화해야 한다. 모델 양자화, 지식 증류 등 다양한 방법을 사용하여 모델의 크기를 줄이고, 추론 속도를 높일 수 있다.

구현 시 고려사항 및 팁은 다음과 같다.

*   **데이터 전처리**: QA 데이터셋의 품질은 CLaRa의 성능에 큰 영향을 미친다. 데이터 전처리 과정에서 노이즈를 제거하고, 데이터를 정제해야 한다.

*   **GPU 활용**: CLaRa는 GPU를 사용하여 학습해야 한다. GPU 메모리 사용량을 최적화하고, 학습 속도를 높여야 한다.

*   **모델 저장**: 학습된 모델을 저장하고, 재사용할 수 있도록 해야 한다. 모델 체크포인트를 사용하여 학습 과정을 저장하고, 필요할 때 모델을 복원할 수 있도록 해야 한다.

*   **API 구축**: CLaRa를 API로 구축하여 다른 시스템과 연동할 수 있도록 해야 한다.

## 결론

본 논문에서는 검색 증강 생성(RAG) 시스템의 한계를 극복하기 위해 CLaRa(Continuous Latent Reasoning)라는 새로운 프레임워크를 제안했다. CLaRa는 검색과 생성 과정을 통합하여 최적화하고, 문서의 핵심 정보를 보존하면서 압축하여 LLM의 긴 문맥 처리 문제를 해결한다. CLaRa는 SCP(Salient Compressor Pretraining) 프레임워크를 도입하여 의미적으로 풍부하고 검색 가능한 압축 벡터를 생성하고, 차별 가능한 top-k 추정기를 사용하여 엔드 투 엔드 학습을 용이하게 한다. 다양한 QA 벤치마크에서 실험한 결과, CLaRa는 최첨단 검색 및 생성 성능을 달성하며, RAG 시스템의 효율성과 성능을 획기적으로 향상시킬 수 있음을 입증했다. CLaRa는 검색과 생성의 통합 최적화를 통해 보다 정확하고 효율적인 지식 기반 질의응답 시스템을 구축하는 데 기여할 것으로 기대된다.

## 참고 자료

*   **논문 링크**: [https://arxiv.org/abs/2511.18659](https://arxiv.org/abs/2511.18659)
*   **코드 저장소**: (미공개)
*   **관련 자료**:
    *   REALM: [https://arxiv.org/abs/2002.08909](https://arxiv.org/abs/2002.08909)
    *   RAG: [