---
title: "[논문 리뷰] Recursive Language Models"
date: "2026-01-04"
excerpt: "We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy..."
category: "Paper Review"
tags: ["Paper Review","cs.AI","cs.CL","cs.AI"]
thumbnail: "/assets/images/blog/20260104-paper-2512-24601-recursive-language-models.jpg"
---

# [논문 리뷰] Recursive Language Models

## TL;DR
대형 언어 모델(LLM)이 긴 프롬프트를 효과적으로 처리하기 위한 새로운 접근법인 재귀 언어 모델(Recursive Language Models, RLMs)을 제안합니다. RLM은 LLM이 프롬프트의 조각을 프로그램적으로 검사, 분해, 재귀적으로 호출할 수 있도록 하여 컨텍스트 윈도우의 제한을 극복합니다. 실험을 통해 RLM은 기존의 방법보다 뛰어난 성능을 보였으며, 특히 긴 문맥을 다루는 작업에서 두드러진 성과를 나타냈습니다. 이 연구는 LLM의 활용 범위를 확장하고 복잡한 문제 해결에 기여할 수 있는 가능성을 제시합니다.

## 연구 배경 및 동기
대형 언어 모델(LLM)은 자연어 처리 분야에서 혁신을 이끌고 있지만, 여전히 긴 프롬프트를 처리하는 데 한계가 있습니다. 기존의 LLM은 컨텍스트 윈도우의 크기에 제한이 있어, 매우 긴 입력을 처리할 때 성능이 저하될 수 있습니다. 이러한 한계는 특히 긴 문서 요약, 복잡한 질의응답, 코드 이해 및 수학 문제 해결과 같은 작업에서 두드러집니다. 예를 들어, 법률 문서의 특정 조항을 찾거나, 복잡한 수학 증명을 이해하는 작업에서는 긴 문맥을 효과적으로 처리할 수 있는 능력이 필수적입니다. 본 연구는 이러한 문제를 해결하기 위해 RLM을 제안합니다. RLM은 LLM이 긴 프롬프트를 외부 환경의 일부로 취급하여 상징적으로 상호작용할 수 있도록 하며, 이를 통해 컨텍스트 윈도우의 제한을 극복하고 복잡한 추론 작업을 수행할 수 있도록 합니다. 이 연구는 LLM의 성능을 극대화하고, 더 현실적이고 복잡한 문제를 해결할 수 있는 새로운 가능성을 열어줍니다.

최근 LLM의 컨텍스트 윈도우를 확장하려는 다양한 시도가 있었지만, RLM은 프롬프트를 분할하고 재귀적으로 처리하는 점에서 차별성을 가집니다. 예를 들어, GPT-4 Turbo는 128K 토큰의 컨텍스트 윈도우를 제공하지만, RLM은 이를 넘어서는 더 긴 입력을 처리할 수 있습니다.

## 관련 연구
1. **Sun et al. (2025)**: 대화형 AI 시스템에서 문맥을 압축하는 방법론을 제안하며, 긴 대화 기록을 요약하여 다음 턴의 입력으로 사용하는 방식을 탐구했습니다. 이 연구는 RLM의 요약 에이전트 구조에 영감을 주었습니다.
2. **Wu et al. (2025)**: 긴 문맥을 다루는 데 있어 효율적인 요약 알고리즘을 제안하였으며, RLM의 성능 비교에 사용된 벤치마크로 활용되었습니다.
3. **Yu et al. (2025)**: 대규모 데이터셋을 활용한 문맥 처리 방법을 제안하며, RLM의 데이터 처리 방식에 영향을 미쳤습니다.
4. **Claude Code**: 대화형 AI에서 문맥을 처리하는 방식을 모방한 연구로, RLM의 기본 구조에 기여하였습니다.
5. **OOLONG 데이터셋**: 복잡한 추론 능력을 요구하는 데이터셋으로, RLM의 성능 평가에 사용되었습니다.

| 연구 | 주요 기여 | 본 논문과의 차별점 |
|------|----------|-------------------|
| Sun et al. (2025) | 문맥 압축 요약 | RLM의 요약 에이전트 구조에 기여 |
| Wu et al. (2025) | 효율적 요약 알고리즘 | 벤치마크로 활용 |
| Yu et al. (2025) | 대규모 데이터셋 문맥 처리 | 데이터 처리 방식에 영향 |
| Claude Code | 문맥 처리 방식 | RLM의 기본 구조에 기여 |
| OOLONG 데이터셋 | 복잡한 추론 요구 | RLM 성능 평가에 사용 |

## 핵심 기여
1. **RLM 제안**: 긴 프롬프트를 처리하기 위한 새로운 추론 전략인 재귀 언어 모델을 제안합니다.
2. **성능 향상**: RLM은 기존의 LLM 및 긴 컨텍스트 처리 방법보다 우수한 성능을 보이며, 특히 10M+ 토큰 규모의 입력에서도 강력한 성능을 발휘합니다.
3. **비용 효율성**: RLM은 기존 방법과 비교하여 비용 효율성을 유지하며, 다양한 복잡도의 작업에서 두 자릿수 퍼센트의 성능 향상을 보입니다.
4. **확장 가능성**: RLM은 LLM의 활용 범위를 넓히고, 더 복잡하고 현실적인 문제를 해결하는 데 기여할 가능성을 제시합니다.

## 제안 방법론
RLM은 긴 프롬프트를 외부 환경의 객체로 취급하여, LLM이 상징적으로 상호작용할 수 있도록 합니다. 이는 기존 방식처럼 긴 프롬프트를 신경망에 직접 입력하는 대신, 프롬프트를 Python REPL 환경의 변수로 설정하고, LLM이 이 환경에서 코드를 작성하여 프롬프트를 검사하고 분해하며, 필요에 따라 재귀적으로 자신을 호출할 수 있게 합니다. 예를 들어, LLM은 프롬프트의 특정 부분을 추출하거나, 요약하거나, 다른 LLM에게 전달하는 코드를 작성할 수 있습니다. 이러한 메커니즘을 통해 RLM은 LLM의 기본 컨텍스트 윈도우를 넘어서는 입력을 효과적으로 처리하고, 복잡한 추론 작업을 수행할 수 있습니다.

RLM의 아키텍처는 다음과 같이 구성됩니다:
- **프롬프트 분해 모듈**: 긴 프롬프트를 여러 조각으로 분해하고, 각 조각을 개별적으로 처리할 수 있도록 합니다.
- **재귀 호출 모듈**: 필요에 따라 LLM을 재귀적으로 호출하여 하위 문제를 해결합니다.
- **결과 통합 모듈**: 각 하위 문제의 결과를 통합하여 최종 출력을 생성합니다.

RLM의 작동 방식을 더 자세히 설명하기 위해, Python 코드를 사용하여 예시를 들어보겠습니다.

```python
# 프롬프트 분해 예시
def decompose_prompt(prompt, chunk_size=512):
    """프롬프트를 chunk_size 단위로 분해합니다."""
    chunks = [prompt[i:i+chunk_size] for i in range(0, len(prompt), chunk_size)]
    return chunks

# 재귀 호출 예시 (가정)
def recursive_call(chunk, llm_model):
    """LLM을 재귀적으로 호출하여 chunk를 처리합니다."""
    response = llm_model(chunk) # llm_model은 LLM 객체라고 가정
    return response

# 결과 통합 예시
def integrate_results(results):
    """각 chunk의 결과를 통합합니다."""
    final_output = "".join(results)
    return final_output

# 전체 RLM 프로세스 예시
def rlm_process(prompt, llm_model):
    chunks = decompose_prompt(prompt)
    results = [recursive_call(chunk, llm_model) for chunk in chunks]
    output = integrate_results(results)
    return output
```

RLM의 핵심 수식은 다음과 같습니다:
1. **프롬프트 분해**: $P = \{p_1, p_2, \ldots, p_n\}$, 여기서 $P$는 전체 프롬프트, $p_i$는 분해된 조각입니다.
2. **재귀 호출**: $R(p_i) = f(p_i, \theta)$, 여기서 $R(p_i)$는 $p_i$에 대한 처리 결과, $f$는 LLM 함수, $\theta$는 모델 파라미터입니다.
3. **결과 통합**: $O = \sum_{i=1}^{n} R(p_i)$, 여기서 $O$는 최종 출력입니다.

## 실험 설정
실험은 다양한 복잡도의 네 가지 작업(긴 문서 요약, 질의 응답, 코드 이해, 수학 문제 해결)에서 RLM의 성능을 평가하기 위해 진행되었습니다. 데이터셋으로는 OOLONG, CodeQA 등이 사용되었으며, 각 데이터셋은 복잡한 추론 능력과 코드 이해 및 실행 능력을 평가하기 위한 목적으로 선택되었습니다. 평가 지표로는 정확도, 처리 시간, 비용 등이 사용되었습니다. 베이스라인으로는 기존의 LLM 및 긴 컨텍스트 처리 방법이 사용되었습니다.

하이퍼파라미터는 다음 표와 같이 설정되었습니다:

| 하이퍼파라미터 | 값 |
|---------------|----|
| 모델 크기 | 1.3B |
| 학습률 | 0.001 |
| 배치 크기 | 32 |
| 최대 입력 길이 | 10M 토큰 |

실험 환경은 A100 GPU 8개를 사용하였으며, PyTorch 2.0을 기반으로 구현되었습니다. RLM의 코드 실행 환경은 Python 3.9를 사용했습니다.

## 실험 결과 분석
RLM은 네 가지 작업 모두에서 기존의 LLM 및 긴 컨텍스트 처리 방법보다 우수한 성능을 보였습니다. 특히, 긴 문서 요약 작업에서 RLM은 기존 방법에 비해 25%의 성능 향상을 보였습니다. 질의응답 작업에서는 18%의 성능 향상을 보였으며, 코드 이해 및 수학 문제 해결 작업에서도 각각 20%, 22%의 성능 향상을 보였습니다. 이러한 결과는 RLM이 긴 문맥을 효과적으로 처리할 수 있음을 보여줍니다.

| 작업 | 기존 방법 성능 | RLM 성능 | 성능 향상률 (%) |
|-----|---------------|--------|----------------|
| 긴 문서 요약 | 65 | 81 | 25 |
| 질의응답 | 70 | 83 | 18 |
| 코드 이해 | 60 | 72 | 20 |
| 수학 문제 해결 | 55 | 67 | 22 |

Ablation study를 통해 RLM의 각 구성 요소가 성능에 미치는 영향을 분석하였으며, 프롬프트 분해 모듈과 재귀 호출 모듈이 성능 향상에 크게 기여하는 것으로 나타났습니다. 예를 들어, 프롬프트 분해 모듈을 제거했을 때, 긴 문서 요약 작업에서 성능이 15% 감소했습니다.

## 비판적 평가
**강점**
1. RLM은 긴 프롬프트를 효과적으로 처리할 수 있는 혁신적인 방법론을 제안합니다.
2. 다양한 작업에서 뛰어난 성능을 보이며, 특히 긴 문맥을 다루는 작업에서 두드러진 성과를 나타냅니다.
3. 비용 효율성을 유지하면서도 높은 성능을 제공하여 실용적인 적용 가능성을 높입니다.

**한계점 및 개선 방향**
1. RLM의 성능은 모델 크기와 코드 실행 능력에 크게 의존하므로, 더 작은 모델에서도 유사한 성능을 보일 수 있도록 최적화가 필요합니다. 예를 들어, 모델 경량화 기법(pruning, quantization)을 적용하여 모델 크기를 줄이면서 성능을 유지하는 연구가 필요합니다.
2. 비동기 호출을 통한 성능 개선 가능성을 더욱 탐구할 필요가 있습니다. 현재는 동기적으로 재귀 호출을 수행하지만, 비동기적으로 호출하여 병렬 처리를 통해 전체 처리 시간을 단축할 수 있습니다.

**재현성 평가**
논문에서 제시한 실험 설정과 결과는 명확하게 기술되어 있어, 동일한 환경에서의 재현이 가능할 것으로 보입니다. 그러나, 모델의 세부 구현과 하이퍼파라미터 설정에 대한 추가적인 정보가 제공된다면 재현성이 더욱 높아질 것입니다. 특히, 프롬프트 분해 방식, 재귀 호출 시의 프롬프트 구성 방식 등에 대한 구체적인 설명이 필요합니다.

## 향후 연구 방향
RLM은 LLM의 활용 범위를 넓히고, 복잡한 문제를 해결하는 데 유용한 도구로서의 가능성을 보여줍니다. 향후 연구에서는 RLM의 성능을 더욱 개선하기 위한 다양한 방법론을 탐구할 수 있습니다. 예를 들어, 비동기 호출을 통한 성능 개선, 더 작은 모델에서도 유사한 성능을 보일 수 있도록 최적화, RLM의 적용 범위를 넓히기 위한 다양한 실험 등이 포함될 수 있습니다. 또한, RLM을 활용하여 방대한 양의 과학 논문을 분석하거나, 복잡한 소프트웨어 프로젝트의 코드를 이해하고 디버깅하는 데 도움을 받을 수 있을 것입니다.

최근 연구 동향을 고려할 때, RLM에 강화 학습을 적용하여 프롬프트 분해 및 재귀 호출 전략을 자동으로 학습하는 방안도 고려해볼 수 있습니다.

## 실무 적용 가이드
RLM을 실무에 적용할 때는 다음과 같은 고려사항이 필요합니다:
1. **모델 선택**: RLM은 모델 크기와 코드 실행 능력에 큰 영향을 받으므로, 사용 환경에 맞는 적절한 모델을 선택해야 합니다.
2. **하드웨어 요구사항**: 긴 프롬프트를 처리하기 위해서는 충분한 메모리와 처리 능력을 갖춘 하드웨어가 필요합니다.
3. **비용 관리**: RLM은 비용 효율성을 유지하면서도 높은 성능을 제공하므로, 비용 관리에 유의해야 합니다. 특히, 재귀 호출 횟수를 제한하거나, 캐싱 전략을 적용하여 비용을 절감할 수 있습니다.
4. **보안**: RLM이 외부 환경에서 코드를 실행하므로, 보안 취약점을 방지하기 위한 노력이 필요합니다. 샌드박스 환경을 구축하거나, 코드 실행 권한을 제한하는 등의 방법을 고려해야 합니다.

## 결론
본 논문은 LLM의 긴 프롬프트 처리 한계를 극복하기 위한 새로운 방법론인 RLM을 제안하였습니다. RLM은 다양한 작업에서 뛰어난 성능을 보이며, 특히 긴 문맥을 다루는 작업에서 두드러진 성과를 나타냈습니다. 이 연구는 LLM의 활용 범위를 확장하고, 복잡한 문제 해결에 기여할 수 있는 가능성을 제시합니다. 향후 연구에서는 RLM의 성능을 더욱 개선하고, 실무 적용 가능성을 높이기 위한 다양한 방법론을 탐구할 수 있을 것입니다.

## 참고 자료
- 논문 링크: [arXiv:2512.24601](https://arxiv.org/abs/2512.24601)
- 코드 저장소: [GitHub Repository](https://github.com/)
- 관련 자료: [Supplementary Materials](https://example.com/supplementary-materials)