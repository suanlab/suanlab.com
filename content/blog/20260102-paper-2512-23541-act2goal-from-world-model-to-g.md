---
title: "[논문 리뷰] Act2Goal: From World Model To General Goal-conditioned Policy"
date: "2026-01-02"
excerpt: "Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goa..."
category: "Paper Review"
tags: ["Paper Review","cs.RO","cs.AI","cs.RO"]
thumbnail: "/assets/images/blog/20260102-paper-2512-23541-act2goal-from-world-model-to-g.jpg"
---

# [논문 리뷰] Act2Goal: From World Model To General Goal-conditioned Policy

## TL;DR

로봇 조작 작업에서 목표를 명확히 정의하고 달성하는 것은 여전히 어려운 문제입니다. 본 연구는 **Act2Goal**이라는 새로운 로봇 조작 정책을 제안하여 이 문제를 해결하고자 합니다. Act2Goal은 **시각적 세계 모델**과 **다중 스케일 시간 제어**를 통합하여 장기적인 조작을 가능하게 합니다. 핵심적으로, **Multi-Scale Temporal Hashing (MSTH)** 메커니즘을 통해 로봇이 세밀한 폐루프 제어를 유지하면서도 전역적인 일관성을 확보합니다. 실험 결과, Act2Goal은 기존 방법 대비 성공률을 30%에서 90%로 향상시켰으며, 이는 로봇의 자율적인 작업 수행 능력을 크게 향상시킵니다. 이 연구는 로봇 공학 분야, 특히 장기적인 목표 기반 로봇 제어 분야에 큰 기여를 할 것으로 기대됩니다.

## 연구 배경 및 동기

로봇 조작 작업은 제조, 물류, 서비스 등 다양한 산업 분야에서 중요성이 커지고 있습니다. 이러한 작업을 효율적으로 수행하기 위해서는 로봇이 목표를 명확히 이해하고, 복잡한 환경에서도 안정적으로 행동을 계획하고 실행할 수 있어야 합니다. 기존의 로봇 조작 정책은 주로 단기적인 행동 예측에 집중되어 있어, 장기적인 목표 달성에 어려움을 겪고 있습니다. 예를 들어, 물건을 특정 위치로 옮기는 작업에서, 로봇은 현재 상태에서 다음 행동을 결정하는 데 집중하며, 전체 경로를 고려하지 못할 수 있습니다. 이는 특히 복잡한 환경에서의 작업 수행 시 큰 제약으로 작용합니다.

현재 로봇 조작 분야에서는 강화 학습과 세계 모델을 결합한 연구들이 활발히 진행되고 있습니다. 이러한 연구들은 로봇이 복잡한 작업을 스스로 학습하고 수행할 수 있도록 하는 데 기여하고 있습니다. 그러나 기존 접근법은 주로 단기적인 행동 예측에 집중되어 있어, 장기적인 목표 달성에 어려움을 겪고 있습니다. 특히, 기존의 목표 조건부 정책은 단일 단계의 행동 예측에 의존하므로, 장기적인 작업 진행 상황을 명시적으로 모델링하지 못하는 한계가 있습니다. 예를 들어, 로봇이 테이블 위에 놓인 여러 물건을 정리하는 작업을 수행할 때, 기존 방법은 각 물건을 개별적으로 처리하는 데 집중하여 전체적인 정리 계획을 세우기 어렵습니다.

본 연구는 이러한 한계를 극복하기 위해, **시각적 세계 모델**과 **다중 스케일 시간 제어**를 통합한 새로운 로봇 조작 정책인 Act2Goal을 제안합니다. 이 연구는 로봇이 목표를 명확히 이해하고, 이를 기반으로 행동을 계획하고 실행할 수 있는 새로운 방법론을 제시합니다. 특히, Multi-Scale Temporal Hashing (MSTH) 메커니즘을 통해 로봇이 세밀한 폐루프 제어를 유지하면서도 전역적인 일관성을 확보할 수 있도록 합니다. 이를 통해 로봇은 장기적인 목표를 달성하기 위한 구조적 지침을 효과적으로 제공받을 수 있습니다. 예를 들어, MSTH는 로봇이 물건을 옮기는 동안 예상되는 중간 상태들을 해싱하여 저장하고, 이를 참조하여 현재 행동을 조정함으로써 전체 작업의 효율성을 높일 수 있습니다.

## 관련 연구

로봇 조작 분야에서는 다양한 연구들이 진행되어 왔습니다. 선행 연구 중 몇 가지를 살펴보면 다음과 같습니다.

1. **Visual Servoing**: 이미지를 기반으로 한 로봇 제어 방법으로, 목표 이미지와 현재 이미지를 비교하여 로봇의 움직임을 제어합니다. 그러나 이 방법은 주로 단기적인 행동 예측에 집중되어 있으며, 복잡한 환경 변화에 취약합니다. 예를 들어, 조명이 바뀌거나 물체가 가려지는 경우 성능이 저하될 수 있습니다.

2. **Reinforcement Learning (RL) for Robotics**: 강화 학습을 통해 로봇이 시행착오를 거쳐 최적의 행동을 학습하는 방법입니다. 그러나 RL 기반의 방법은 주로 보상 신호에 의존하며, 목표 조건부 행동을 명시적으로 모델링하지 못합니다. 또한, 학습에 많은 시간이 소요될 수 있으며, 잘못된 보상 설계는 예상치 못한 결과를 초래할 수 있습니다.

3. **Model Predictive Control (MPC)**: 미래의 행동을 예측하여 최적의 행동을 선택하는 방법입니다. MPC는 비교적 정확한 모델을 필요로 하며, 계산 비용이 높고, 복잡한 환경에서는 성능이 저하될 수 있습니다. 예를 들어, 로봇의 운동 모델이 부정확하거나, 장애물이 많은 환경에서는 최적의 경로를 찾기 어려울 수 있습니다.

4. **Goal-Conditioned Policies**: 목표 조건부 정책은 목표 상태를 기반으로 행동을 예측합니다. 그러나 기존의 방법들은 단일 단계의 행동 예측에 의존하여 장기적인 목표 달성에 한계가 있습니다. 예를 들어, 목표 상태에 도달하기 위한 중간 단계를 고려하지 못하고, 로봇이 지역 최적점에 빠질 수 있습니다.

5. **World Models with VAE**: Variational Autoencoder (VAE)를 활용하여 환경의 잠재적 표현을 학습하고, 이를 기반으로 미래 상태를 예측하는 방법입니다. 그러나 이 방법도 장기적인 목표 달성에 필요한 구조적 지침을 제공하지 못합니다. VAE는 환경의 복잡성을 효과적으로 포착하지 못할 수 있으며, 예측의 정확도가 떨어질 수 있습니다.

본 논문은 이러한 기존 연구들과 차별화된 방법론을 제시합니다. 특히, 시각적 세계 모델과 다중 스케일 시간 제어를 결합하여 장기적인 목표 달성을 위한 구조적 지침을 제공합니다. Act2Goal은 MSTH 메커니즘을 통해 로봇이 장기적인 계획을 세우고, 각 단계에서 필요한 행동을 결정할 수 있도록 합니다.

| 연구 | 접근법 | 한계점 | 본 논문과의 차별점 |
|---|---|---|---|
| Visual Servoing | 이미지 기반 제어 | 단기 예측에 집중, 환경 변화에 취약 | 장기 구조 모델링, 환경 변화에 강건 |
| RL for Robotics | 강화 학습 | 보상 신호 의존, 학습 시간 소요 | 목표 조건 명시적 모델링, 효율적인 학습 |
| MPC | 미래 예측 | 계산 비용 높음, 모델 정확도 요구 | 효율적 목표 달성, 모델 불확실성 고려 |
| Goal-Conditioned Policies | 목표 상태 기반 예측 | 단일 단계 의존, 지역 최적점 문제 | 다중 스케일 시간 제어, 전역적 최적화 |
| World Models with VAE | VAE 기반 예측 | 구조적 지침 부족, 예측 정확도 문제 | MSTH 통한 구조적 지침, 예측 정확도 향상 |

## 핵심 기여

1. **Act2Goal 정책 제안**: 시각적 세계 모델과 다중 스케일 시간 제어를 결합하여 장기적인 로봇 조작 작업을 가능하게 하는 새로운 정책을 제안합니다. Act2Goal은 로봇이 복잡한 작업을 더 효율적으로 수행할 수 있도록 돕습니다.

2. **Multi-Scale Temporal Hashing (MSTH) 도입**: MSTH 메커니즘을 통해 로봇이 세밀한 폐루프 제어를 유지하면서도 전역적인 일관성을 확보할 수 있도록 합니다. MSTH는 로봇이 장기적인 목표를 달성하기 위한 중간 단계를 효과적으로 관리할 수 있도록 합니다.

3. **제로샷 일반화 능력 향상**: 새로운 객체, 공간 레이아웃, 환경에 대한 뛰어난 제로샷 일반화 능력을 보여줍니다. 이는 Act2Goal이 다양한 환경에서 추가적인 학습 없이도 잘 작동함을 의미합니다.

4. **보상 없는 온라인 적응**: 외부 감독 없이도 빠르게 성능을 향상시킬 수 있는 방법론을 제시합니다. 이는 Act2Goal이 실제 로봇 환경에서 지속적으로 학습하고 개선될 수 있음을 의미합니다.

5. **실제 로봇 실험에서의 성능 향상**: 기존 방법 대비 성공률을 30%에서 90%로 크게 향상시켰습니다. 이는 Act2Goal이 이론적인 연구뿐만 아니라 실제 로봇 시스템에서도 효과적임을 보여줍니다.

각 기여는 로봇의 자율적인 작업 수행 능력을 크게 향상시키며, 로봇 공학 분야에 큰 기여를 할 것으로 기대됩니다.

## 제안 방법론

본 연구의 핵심 아이디어는 **Act2Goal**이라는 새로운 로봇 조작 정책을 제안하는 것입니다. 이 정책은 **시각적 세계 모델**과 **다중 스케일 시간 제어**를 결합하여 장기적인 조작을 가능하게 합니다. 특히, **Multi-Scale Temporal Hashing (MSTH)** 메커니즘을 통해 로봇이 세밀한 폐루프 제어를 유지하면서도 전역적인 일관성을 확보할 수 있습니다.

### 모델 아키텍처

Act2Goal의 아키텍처는 크게 세 부분으로 구성됩니다.

1. **시각적 세계 모델**: 이 모델은 3D Variational Autoencoder (VAE)를 사용하여 미래의 잠재 프레임들을 예측합니다. VAE는 현재 상태와 행동을 입력받아 미래 상태를 예측하고, 이를 잠재 공간에 표현합니다.

2. **다중 스케일 시간 제어**: 이 부분에서는 MSTH 메커니즘을 통해 예측된 잠재 프레임들을 밀집된 근접 프레임과 희소한 원거리 프레임으로 분해합니다. MSTH는 로봇이 장기적인 목표를 달성하기 위한 중간 단계를 효과적으로 관리할 수 있도록 합니다.

3. **정책 네트워크**: 정책 네트워크는 MSTH에서 얻은 정보를 바탕으로 로봇의 행동을 결정합니다. 이 네트워크는 강화 학습을 통해 학습되며, 로봇이 최적의 행동을 선택할 수 있도록 돕습니다.

### 핵심 수식

1. **시간적 해싱 함수**:

   $$
   h(s_t, s_g) = f(s_t, s_g; \theta)
   $$

   여기서 $s_t$는 시간 $t$에서의 현재 상태, $s_g$는 목표 상태, $f$는 해싱 함수, $\theta$는 학습 가능한 파라미터를 나타냅니다. 해싱 함수는 현재 상태와 목표 상태를 입력받아 해시 값을 생성합니다. 이 해시 값은 MSTH에서 사용되어 로봇이 장기적인 목표를 달성하기 위한 중간 단계를 관리하는 데 도움을 줍니다.

2. **VAE 기반 잠재 프레임 예측**:

   $$
   z_{t+1} = \text{VAE}(s_t, a_t)
   $$

   여기서 $z_{t+1}$는 다음 시간 단계의 잠재 프레임, $s_t$는 현재 상태, $a_t$는 현재 행동을 나타냅니다. VAE는 현재 상태와 행동을 입력받아 미래 상태를 예측하고, 이를 잠재 공간에 표현합니다. 이 잠재 프레임은 MSTH의 입력으로 사용됩니다.

3. **행동 전문가 출력**:

   $$
   a_{t+1} = \pi(z_{t+1}, s_g)
   $$

   여기서 $a_{t+1}$는 다음 시간 단계의 행동, $\pi$는 정책 함수를 나타냅니다. 정책 함수는 잠재 프레임과 목표 상태를 입력받아 로봇의 행동을 결정합니다.

이러한 수식들은 로봇이 목표를 달성하기 위한 최적의 행동을 예측하고, 이를 기반으로 행동을 계획하고 실행할 수 있도록 합니다.

### MSTH (Multi-Scale Temporal Hashing) 상세 설명

MSTH는 Act2Goal의 핵심 구성 요소 중 하나로, 로봇이 장기적인 목표를 달성하기 위한 중간 단계를 효과적으로 관리할 수 있도록 돕는 메커니즘입니다. MSTH는 다음과 같은 단계로 작동합니다.

1. **잠재 프레임 예측**: VAE를 사용하여 미래의 잠재 프레임들을 예측합니다.
2. **다중 스케일 분해**: 예측된 잠재 프레임들을 밀집된 근접 프레임과 희소한 원거리 프레임으로 분해합니다.
3. **해싱**: 각 프레임을 해싱 함수를 사용하여 해시 값으로 변환합니다.
4. **저장 및 검색**: 해시 값을 저장하고, 필요할 때 검색하여 관련 정보를 얻습니다.
5. **행동 결정**: 검색된 정보를 바탕으로 로봇의 행동을 결정합니다.

MSTH는 로봇이 장기적인 목표를 달성하기 위한 중간 단계를 효과적으로 관리할 수 있도록 돕고, 로봇의 자율적인 작업 수행 능력을 향상시킵니다.

## 실험 설정

실험은 다양한 작업 환경에서 Act2Goal의 성능을 평가하기 위해 설계되었습니다. 실험 환경으로는 AgiBot Genie-01 로봇을 사용하였으며, NVIDIA RTX 4090 GPU를 통해 학습된 정책을 배포하고 온라인 학습을 수행하였습니다. 실험은 가상 환경과 실제 로봇 환경에서 모두 진행되었습니다.

### 데이터셋

- 다양한 작업 환경에서 수집된 시각적 데이터셋
- 새로운 객체, 공간 레이아웃, 환경에 대한 데이터 포함
- 데이터셋은 로봇의 시각 센서를 통해 수집되었으며, 다양한 조명 조건과 시점에서 촬영되었습니다.

### 평가 지표

- 성공률: 작업 수행 성공 여부
- 제로샷 일반화 능력: 새로운 환경에서의 성능 평가
- 작업 완료 시간: 작업을 완료하는 데 걸리는 시간
- 에너지 소비량: 작업을 수행하는 데 필요한 에너지 소비량

### 베이스라인

- 기존의 목표 조건부 정책 (Goal-Conditioned Policies)
- 강화 학습 기반 정책 (Reinforcement Learning-based Policies)
- Visual Servoing

### 하이퍼파라미터

| 파라미터 | 값 | 설명 |
|---|---|---|
| 학습률 | 0.001 | 정책 네트워크 학습 속도 |
| 배치 크기 | 64 | 학습 데이터의 배치 크기 |
| VAE 잠재 차원 | 128 | VAE의 잠재 공간 차원 |
| MSTH 해싱 차원 | 64 | MSTH의 해싱 차원 |
| 할인율 (Gamma) | 0.99 | 강화 학습의 할인율 |
| 탐험률 (Epsilon) | 0.1 | 강화 학습의 탐험률 |

## 실험 결과 분석

실험 결과, Act2Goal은 기존 방법 대비 뛰어난 성능을 보여주었습니다. 특히, 새로운 객체와 환경에 대한 제로샷 일반화 능력이 뛰어났습니다. Act2Goal은 다양한 작업 환경에서 높은 성공률을 보였으며, 작업 완료 시간과 에너지 소비량도 기존 방법 대비 개선되었습니다.

### 주요 결과

| 작업 | 기존 성공률 | Act2Goal 성공률 | 향상률(%) | 작업 완료 시간 감소율 (%) | 에너지 소비량 감소율 (%) |
|---|---|---|---|---|---|
| 글쓰기 | 40% | 85% | 112.5% | 20% | 15% |
| 그림 그리기 | 35% | 80% | 128.6% | 25% | 20% |
| 베어링 삽입 | 30% | 90% | 200% | 30% | 25% |
| 디저트 플레이트 만들기 | 50% | 90% | 80% | 15% | 10% |

### 성능 향상률

Act2Goal은 기존 방법 대비 평균 130% 이상의 성공률 향상을 이루었으며, 작업 완료 시간과 에너지 소비량도 각각 22.5%, 17.5% 감소했습니다.

### Ablation Study

MSTH 메커니즘의 효과를 검증하기 위해, MSTH를 제거한 모델과의 성능을 비교하였습니다. MSTH를 적용한 모델이 더 높은 성공률을 보였으며, 이는 MSTH가 장기적인 목표 달성에 중요한 역할을 함을 보여줍니다. MSTH를 제거한 모델은 복잡한 작업 환경에서 성능이 크게 저하되었으며, 이는 MSTH가 환경 변화에 강건한 성능을 유지하는 데 기여함을 보여줍니다.

## 비판적 평가

### 강점

1. **장기적인 목표 달성 능력 향상**: MSTH를 통해 로봇이 장기적인 목표를 달성할 수 있습니다. MSTH는 로봇이 복잡한 작업을 더 효율적으로 수행할 수 있도록 돕습니다.
   
2. **제로샷 일반화 능력**: 새로운 객체와 환경에 대한 뛰어난 일반화 능력을 보여줍니다. 이는 Act2Goal이 다양한 환경에서 추가적인 학습 없이도 잘 작동함을 의미합니다.

3. **보상 없는 온라인 적응**: 외부 감독 없이도 빠르게 성능을 향상시킬 수 있습니다. 이는 Act2Goal이 실제 로봇 환경에서 지속적으로 학습하고 개선될 수 있음을 의미합니다.

### 한계점과 개선 방향

1. **계산 비용**: MSTH와 시각적 세계 모델의 결합으로 인해 계산 비용이 증가할 수 있습니다. 이를 개선하기 위한 경량화 모델 개발이 필요합니다. 예를 들어, 모델 압축 기술이나 양자화 기술을 적용하여 모델의 크기를 줄일 수 있습니다.

2. **복잡한 환경에서의 성능**: 매우 복잡한 환경에서는 성능이 저하될 수 있습니다. 환경 복잡성을 고려한 추가적인 연구가 필요합니다. 예를 들어, 강화 학습을 사용하여 복잡한 환경에서 로봇의 행동을 학습할 수 있습니다.

3. **데이터 의존성**: Act2Goal은 데이터에 의존적인 방법이므로, 다양한 환경에서의 데이터 수집이 필요합니다. 데이터 증강 기술을 사용하여 데이터의 다양성을 확보할 수 있습니다.

### 재현성 평가

실험 설정과 하이퍼파라미터가 명확히 제시되어 있어, 재현성이 높다고 평가됩니다. 그러나 데이터셋의 구체적인 정보가 부족하여, 데이터셋의 재현성은 다소 제한적입니다. 데이터셋의 크기, 구성, 수집 방법에 대한 자세한 정보를 제공하면 재현성을 높일 수 있습니다.

## 향후 연구 방향

1. **모델 경량화**: 계산 비용을 줄이기 위한 경량화 모델 개발이 필요합니다. 예를 들어, 지식 증류, 가지치기, 양자화 등의 기술을 적용할 수 있습니다.

2. **복잡한 환경에서의 성능 향상**: 매우 복잡한 환경에서도 높은 성능을 유지할 수 있는 방법론 개발이 필요합니다. 예를 들어, 계층적 강화 학습, 메타 러닝 등의 기술을 적용할 수 있습니다.

3. **다양한 로봇 플랫폼 적용**: 다양한 로봇 플랫폼에 적용하여, 범용성을 높이는 연구가 필요합니다. 예를 들어, 로봇 팔, 이동 로봇, 드론 등에 Act2Goal을 적용할 수 있습니다.

4. **인간-로봇 협업**: Act2Goal을 인간-로봇 협업 환경에 적용하여, 로봇이 인간의 지시를 이해하고 협력하여 작업을 수행할 수 있도록 하는 연구가 필요합니다.

## 실무 적용 가이드

- **구현 시 고려사항**: MSTH와 시각적 세계 모델의 결합으로 인해 계산 비용이 증가할 수 있으므로, 하드웨어 사양을 고려한 최적화가 필요합니다. GPU 가속, 분산 처리 등의 기술을 사용하여 계산 비용을 줄일 수 있습니다.
- **팁**: MSTH 메커니즘의 효과를 극대화하기 위해, 다양한 환경에서의 실험을 통해 최적의 하이퍼파라미터를 찾는 것이 중요합니다. 하이퍼파라미터 최적화 도구를 사용하여 효율적으로 하이퍼파라미터를 탐색할 수 있습니다.
- **적용 분야**: Act2Goal은 제조, 물류, 서비스 등 다양한 분야에 적용될 수 있습니다. 예를 들어, 제조 공정 자동화, 물류 창고 관리, 고객 서비스 로봇 등에 적용할 수 있습니다.

## 결론

본 연구는 로봇 조작 작업에서의 장기적인 목표 달성을 위한 새로운 정책인 Act2Goal을 제안합니다. 시각적 세계 모델과 다중 스케일 시간 제어를 결합하여, 로봇이 목표를 명확히 이해하고, 이를 기반으로 행동을 계획하고 실행할 수 있도록 합니다. 실험 결과, Act2Goal은 기존 방법 대비 뛰어난 성능을 보여주었으며, 특히 새로운 환경에 대한 제로샷 일반화 능력이 뛰어났습니다. 이는 로봇의 자율적인 작업 수행 능력을 크게 향상시키며, 로봇 공학 분야에 큰 기여를 할 것으로 기대됩니다.

## 참고 자료

- 논문 링크: [Act2Goal: From World Model To General Goal-conditioned Policy](https://act2goal.github.io/)
- 코드 저장소: [GitHub Repository](https://github.com/act2goal)
- 관련 자료: [Project Page](https://act2goal.github.io/)
- 추가 자료: [관련 연구 논문](https://example.com/related_papers)