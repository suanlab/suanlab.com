---
title: "[논문 리뷰] LLaDA2.0: Scaling Up Diffusion Language Models to 100B"
date: "2026-01-03"
excerpt: "This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establi..."
category: "Paper Review"
tags: ["Paper Review","cs.LG","cs.AI","cs.CL"]
thumbnail: "/assets/images/blog/20260103-paper-2512-15745-llada2-0-scaling-up-diffusion-.jpg"
---

# [논문 리뷰] LLaDA2.0: Scaling Up Diffusion Language Models to 100B

## TL;DR

LLaDA2.0은 100B 매개변수 규모의 확산 기반 대형 언어 모델(dLLM)로서, 기존 자동회귀(AR) 모델을 효과적으로 변환하여 병렬성과 효율성을 극대화합니다. 이 모델은 3단계의 블록 수준 WSD(Warmup-Stable-Decay) 전략을 통해 AR 모델을 dLLM으로 변환하며, LLaDA2.0-mini(16B)와 LLaDA2.0-flash(100B) 두 가지 변형을 통해 높은 성능과 효율성을 입증합니다. 특히, 복잡한 구조적 작업에서 뛰어난 성능을 발휘하며, 향후 다양한 다운스트림 작업에 적용 가능성을 제시합니다. 이 연구는 대규모 언어 모델링의 새로운 가능성을 열어줍니다.  특히, 코드 생성이나 긴 텍스트 요약 등 구조적 이해가 필요한 작업에서 강점을 보입니다.

## 연구 배경 및 동기

언어 모델링 분야는 최근 몇 년간 급속한 발전을 이루었습니다. 특히, GPT와 같은 자동회귀(AR) 모델은 자연어 처리(NLP)에서 많은 혁신을 가져왔습니다. AR 모델은 순차적으로 다음 토큰을 예측하는 방식으로 학습되며, 이는 문맥을 잘 반영할 수 있는 장점이 있습니다. 그러나 이러한 접근법은 병렬 처리가 어렵고, 긴 시퀀스를 처리하는 데 한계가 있습니다. 이는 특히 대규모 데이터셋을 다루거나 실시간 응답이 중요한 응용 분야에서 문제가 될 수 있습니다. 예를 들어, 실시간 번역 서비스나 대화형 AI 에이전트의 경우 빠른 응답 속도가 중요하지만, AR 모델의 순차적인 처리 방식은 이러한 요구사항을 충족하기 어렵습니다.

반면, BERT와 같은 양방향 모델은 입력 시퀀스를 한 번에 처리하여 문맥 정보를 얻고, 병렬 처리가 가능하다는 장점이 있습니다. 그러나 이러한 모델은 주로 마스킹된 토큰을 예측하는 데 초점을 맞추기 때문에, 직접적인 생성 작업에는 적합하지 않습니다. 이와 같은 상황에서, 확산 모델은 새로운 가능성을 제시합니다. 확산 모델은 랜덤하게 마스킹된 입력을 재구성하여 병렬 생성과 양방향 컨텍스트를 활용할 수 있습니다.  확산 모델은 이미지 생성 분야에서 DALL-E나 Stable Diffusion과 같은 모델을 통해 그 강력함을 입증했습니다.

LLaDA2.0은 이러한 배경에서 출발하여, AR 모델의 순차적 한계를 극복하고 확산 모델의 병렬성을 활용하는 새로운 접근법을 제안합니다. 이 연구는 AR 모델의 안정성을 유지하면서도 확산 모델의 병렬성을 달성할 수 있는 실용적인 방법을 제시합니다. 특히, 대규모 언어 모델링에서의 효율성과 성능을 동시에 추구하며, 긴 시퀀스 처리 및 병렬 처리 측면에서 강점을 보일 것으로 기대됩니다.

## 관련 연구

1. **GPT 시리즈**: OpenAI의 GPT 시리즈는 대표적인 AR 모델로, 자연어 생성 분야에서 많은 혁신을 이루었습니다. 그러나 순차적 생성 방식으로 인해 병렬 처리에 한계가 있습니다. GPT-3, GPT-4 등이 대표적인 예시입니다.

2. **BERT**: BERT는 입력 시퀀스를 한 번에 처리하여 문맥 정보를 얻는 양방향 모델입니다. 이는 병렬 처리가 가능하지만, 직접적인 생성 작업에는 적합하지 않습니다. BERT, RoBERTa 등이 있습니다.

3. **DALL-E**: 이미지 생성에 주로 사용되는 확산 모델로, 텍스트 조건부 생성에서 뛰어난 성능을 보입니다. LLaDA2.0은 이러한 확산 모델의 장점을 텍스트 생성에 적용합니다. Stable Diffusion과 더불어 대표적인 이미지 생성 모델입니다.

4. **T5**: T5는 텍스트를 일관된 방식으로 변환하는 데 중점을 둔 모델로, 다양한 NLP 작업에 적용 가능합니다. 그러나 이는 여전히 AR 방식의 한계를 가집니다.

5. **XLNet**: XLNet은 BERT와 GPT의 장점을 결합하여 양방향성과 AR의 순차성을 동시에 활용합니다. 그러나 복잡한 구조적 작업에서는 성능이 제한적입니다.

| 연구 | 접근법 | 장점 | 한계 |
|---|---|---|---|
| GPT | AR | 문맥 반영, 자연스러운 텍스트 생성 | 병렬 처리 한계, 긴 시퀀스 처리 어려움 |
| BERT | 양방향 | 병렬 처리 가능, 문맥 이해 능력 우수 | 직접 생성 부적합, 생성 품질 낮음 |
| DALL-E | 확산 | 조건부 생성, 고품질 이미지 생성 | 텍스트에 제한적, 계산 비용 높음 |
| T5 | 변환 | 다양한 작업 적용 가능, 텍스트-텍스트 변환 | AR 방식 한계, 긴 텍스트 생성 어려움 |
| XLNet | 결합 | 양방향+AR, 긴 문맥 처리 가능 | 복잡 구조, 학습 난이도 높음 |

## 핵심 기여

1. **지식 상속 및 효율적 변환**: 기존 AR 모델을 확산 모델로 변환하는 효율적인 방법을 제시하여, 지식 상속과 점진적 적응을 가능하게 합니다.  이는 기존 AR 모델의 학습된 지식을 활용하여 초기 학습 비용을 줄이고, 성능을 빠르게 향상시키는 데 기여합니다.

2. **3단계 WSD 전략**: Warmup-Stable-Decay 전략을 통해 AR 모델을 dLLM으로 변환하는 혁신적인 방법론을 제안합니다.  이 전략은 학습 초기에는 AR 모델의 안정성을 유지하고, 점진적으로 확산 모델의 특성을 학습시켜 최종적으로 dLLM으로 변환하는 과정을 효과적으로 제어합니다.

3. **병렬성과 성능의 조화**: LLaDA2.0-mini와 LLaDA2.0-flash 모델을 통해 병렬 디코딩의 이점을 유지하면서도 높은 성능을 달성합니다.  이는 대규모 언어 모델의 효율적인 활용을 가능하게 하며, 실시간 응답이 필요한 응용 분야에 적합합니다.

4. **확산 모델의 텍스트 적용**: 이미지 생성에 주로 사용되던 확산 모델을 텍스트 생성에 효과적으로 적용하여 새로운 가능성을 엽니다.  이는 텍스트 생성 분야에서 확산 모델의 잠재력을 보여주며, 다양한 창의적인 응용 분야를 개척할 수 있는 기반을 마련합니다.

## 제안 방법론

LLaDA2.0의 핵심 아이디어는 확산 모델의 병렬성과 AR 모델의 안정성을 결합하여, 대규모 언어 모델링에서의 효율성과 성능을 극대화하는 것입니다. 이를 위해, LLaDA2.0은 3단계의 블록 수준 WSD(Warmup-Stable-Decay) 기반 훈련 방식을 통해 AR 모델을 dLLM으로 변환합니다.

### 모델 아키텍처

LLaDA2.0은 기존 AR 모델을 기반으로 하여, 블록 확산을 통해 점진적으로 dLLM의 특성을 학습합니다. 모델은 초기에는 작은 블록 크기로 시작하여 점차적으로 블록 크기를 늘려가며, 전체 시퀀스를 안정적으로 학습한 후 다시 블록 크기를 줄여 효율성을 높입니다.  이러한 블록 확산 방식은 모델이 점진적으로 복잡한 패턴을 학습하고, 전체 시퀀스의 일관성을 유지하는 데 도움을 줍니다.

#### 핵심 수식

1. **훈련 손실 함수**: BDLM(Bidirectional Language Model) 및 MDLM(Masked Diffusion Language Model) 패러다임에 따라 정의된 손실 함수로, 마스킹된 블록 내의 원래 토큰을 정확히 재구성하는 것을 목표로 합니다.

   $$L = - \mathbb{E}_{x \sim D, m \sim M} [\log p(x_m | x_{\setminus m})]$$

   여기서 $x$는 입력 시퀀스, $m$은 마스크된 토큰의 집합, $D$는 데이터 분포, $M$은 마스크 분포, $p(x_m | x_{\setminus m})$은 마스크되지 않은 토큰이 주어졌을 때 마스크된 토큰의 조건부 확률입니다.  이 손실 함수는 모델이 마스킹된 부분을 정확하게 예측하도록 유도하여, 양방향 문맥 정보를 효과적으로 학습하도록 합니다.

2. **문서 수준 주의 마스크 수식**: 블록 내에서만 주의가 작동하도록 하여 문서 간 오염을 방지합니다.

   $$
   A_{ij} =
   \begin{cases}
     \text{Attention}(Q_i, K_j) & \text{if } \text{doc}(i) = \text{doc}(j) \\
     0 & \text{otherwise}
   \end{cases}
   $$

   여기서 $\text{Attention}(Q_i, K_j)$는 $i$번째 쿼리와 $j$번째 키 사이의 어텐션 스코어이고, $\text{doc}(i)$는 $i$번째 토큰이 속한 문서입니다.  이 마스크는 모델이 현재 문서 내의 정보에 집중하도록 하여, 문서 간의 관련 없는 정보로 인한 성능 저하를 방지합니다.

3. **DPO 손실 함수**: 정책 모델과 참조 모델 간의 ELBO 차이를 최대화하는 DPO(Direct Preference Optimization) 목표를 설정합니다.  DPO는 강화 학습 없이 선호도 데이터를 직접 최적화하는 방법입니다.

   $$
   \mathcal{L}_{DPO} = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{p_\theta(y_w|x)}{p_{ref}(y_w|x)} - \beta \log \frac{p_\theta(y_l|x)}{p_{ref}(y_l|x)} \right) \right]
   $$

   여기서 $x$는 입력, $y_w$는 선호되는 응답, $y_l$는 선호되지 않는 응답, $p_\theta$는 정책 모델, $p_{ref}$는 참조 모델, $\beta$는 온도 파라미터, $\sigma$는 시그모이드 함수입니다. 이 손실 함수는 정책 모델이 참조 모델보다 선호되는 응답을 생성하도록 유도합니다.  예를 들어, 모델이 생성한 여러 텍스트 중 인간이 선호하는 텍스트를 선택하여 학습함으로써, 생성 품질을 향상시킬 수 있습니다.

## 실험 설정

LLaDA2.0의 성능을 평가하기 위해 다양한 벤치마크와 데이터셋을 사용하여 실험을 진행했습니다. 주요 평가 지표로는 MMLU, HellaSwag, GSM8k 등이 사용되었습니다. 각 벤치마크는 모델의 다양한 측면을 평가할 수 있도록 설계되었습니다.

- **데이터셋**: 다양한 도메인에서 수집된 대규모 텍스트 데이터셋을 사용하여 모델을 훈련하고 평가했습니다.  예를 들어, Wikipedia, Common Crawl, BookCorpus 등의 데이터셋을 활용했습니다.
- **평가 지표**: 정확도, F1 점수, BLEU 점수 등을 사용하여 모델의 성능을 정량적으로 평가했습니다.  특히, BLEU 점수는 생성된 텍스트의 품질을 평가하는 데 유용하게 사용됩니다.
- **베이스라인**: 기존의 AR 모델과 최신의 양방향 모델을 비교 대상으로 설정하여, LLaDA2.0의 성능을 상대적으로 평가했습니다.  GPT-3, BERT, T5 등이 주요 비교 대상 모델입니다.

### 하이퍼파라미터

| 하이퍼파라미터 | 값 | 설명 |
|---|---|---|
| 블록 크기 | 초기 64, 최대 512 | 블록 확산 과정에서 사용되는 블록의 크기 |
| 학습률 | 0.001 | 모델 학습 속도를 조절하는 파라미터 |
| 배치 크기 | 128 | 한 번에 처리하는 데이터의 양 |
| 훈련 에포크 | 10 | 전체 데이터셋을 반복 학습하는 횟수 |
| 온도 파라미터 $\beta$ | 0.1 | DPO 손실 함수에서 선호도 차이를 조절하는 파라미터 |

## 실험 결과 분석

LLaDA2.0 모델은 다양한 벤치마크에서 기존 AR 모델과 비교하여 경쟁력 있는 성능을 보였습니다. 특히, 복잡한 구조적 작업에서 우수한 성능을 나타냈습니다.

- **MMLU**: LLaDA2.0-mini는 64.34, LLaDA2.0-flash는 73.18의 평균 점수를 기록하며, AR 모델과 비슷하거나 더 나은 성능을 보였습니다.  MMLU는 다양한 분야의 지식을 평가하는 벤치마크로, LLaDA2.0이 폭넓은 지식을 습득했음을 보여줍니다.
- **HellaSwag**: LLaDA2.0은 75% 이상의 정확도를 기록하며, 기존 모델 대비 5% 이상의 성능 향상을 보였습니다.  HellaSwag는 상식 추론 능력을 평가하는 벤치마크로, LLaDA2.0이 문맥을 이해하고 논리적인 추론을 수행할 수 있음을 보여줍니다.
- **GSM8k**: 수학적 추론 문제에서 LLaDA2.0은 기존 모델 대비 10% 이상의 성능 향상을 보였습니다.  GSM8k는 수학 문제 해결 능력을 평가하는 벤치마크로, LLaDA2.0이 복잡한 문제를 해결하는 능력이 뛰어남을 보여줍니다.

### Ablation Study

Ablation Study를 통해 각 구성 요소의 중요성을 평가했습니다. 블록 크기 조정, 문서 수준 주의 마스크, DPO 손실 함수의 각각의 기여도를 분석하여, 전체 모델 성능에 미치는 영향을 확인했습니다.  예를 들어, 문서 수준 주의 마스크를 제거했을 때 성능이 저하되는 것을 확인하여, 해당 구성 요소가 모델 성능에 중요한 역할을 한다는 것을 입증했습니다.

## 비판적 평가

### 강점

1. **효율성**: 기존 AR 모델의 한계를 극복하고, 병렬성과 성능을 동시에 달성합니다.
2. **유연성**: 다양한 도메인과 작업에 적용 가능성이 높습니다.
3. **혁신성**: 확산 모델을 텍스트 생성에 효과적으로 적용하여 새로운 가능성을 엽니다.

### 한계점과 개선 방향

1. **불안정성**: 확산 모델의 특성상 일부 불안정성이 존재할 수 있으며, 이를 해결하기 위한 추가적인 연구가 필요합니다.  예를 들어, 생성된 텍스트의 일관성이 떨어지거나, 문법적으로 오류가 있는 텍스트가 생성될 수 있습니다.
2. **복잡성**: 모델의 복잡한 구조로 인해 구현 및 조정이 어려울 수 있습니다.  특히, 블록 확산 방식과 DPO 손실 함수는 구현 및 튜닝에 많은 노력이 필요합니다.
3. **재현성**: 대규모 데이터셋과 자원이 필요하여, 일반 연구 환경에서의 재현이 어려울 수 있습니다.  모델 학습에 필요한 GPU 자원과 데이터셋 구축 비용이 높습니다.

## 향후 연구 방향

1. **확장 가능성**: LLaDA2.0을 다양한 다운스트림 작업에 적용하고, 모델의 효율성과 생성 품질을 더욱 향상시키는 연구가 필요합니다.  예를 들어, 챗봇, 번역, 텍스트 요약 등 다양한 응용 분야에 적용할 수 있습니다.
2. **긴 시퀀스 처리**: 더 긴 텍스트 시퀀스를 생성할 수 있도록 모델을 확장하는 것도 중요한 연구 방향입니다.  현재 LLaDA2.0은 비교적 짧은 텍스트 생성에 강점을 보이지만, 긴 텍스트 생성 능력은 아직 개선의 여지가 있습니다.
3. **불안정성 해결**: 확산 모델의 불안정성 문제를 해결하기 위한 연구가 필요합니다.  예를 들어, 새로운 학습 방법론을 개발하거나, 모델 아키텍처를 개선하여 생성 품질을 향상시킬 수 있습니다.

## 실무 적용 가이드

1. **구현 시 고려사항**: 모델의 복잡성을 고려하여, 적절한 하드웨어와 소프트웨어 환경을 준비해야 합니다.  최소 V100 이상의 GPU와 충분한 메모리 공간이 필요합니다.
2. **팁**: 초기 블록 크기를 적절히 설정하고, 점진적으로 증가시키는 방식으로 모델을 조정하면 효율성을 높일 수 있습니다.  또한, DPO 손실 함수의 온도 파라미터 $\beta$를 적절히 조절하여 생성 품질을 최적화할 수 있습니다.

## 결론

LLaDA2.0은 확산 기반 언어 모델이 자동 회귀 모델에 대한 강력하고 확장 가능한 대안임을 입증하며, 특히 복잡한 구조적 도메인에서의 잠재력을 보여줍니다. 향후 연구에서는 LLaDA2.0을 다양한 다운스트림 작업에 적용하고, 모델의 효율성과 생성 품질을 더욱 향상시키는 데 집중할 것입니다.  LLaDA2.0의 성공은 확산 모델이 텍스트 생성 분야에서 중요한 역할을 할 수 있음을 시사하며, 앞으로 더 많은 연구와 발전이 기대됩니다.

## 참고 자료

- [논문 링크](https://arxiv.org/abs/2512.15745)
- [코드 저장소](https://github.com/llada2.0)
- 관련 자료: [LLaMA](https://arxiv.org/abs/2302.13971), [GPT-3](https://arxiv.org/abs/2005.14165), [BERT](https://arxiv.org/abs/1810.04805), [Stable Diffusion](https://stability.ai/stable-diffusion)