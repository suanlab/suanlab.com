---
title: "[논문 리뷰] Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge"
date: "2026-01-22"
excerpt: "Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly..."
category: "Paper Review"
tags: ["Paper Review","cs.CL","cs.AI","cs.LG"]
thumbnail: "/assets/images/blog/20260122-paper-2601-08808-multiplex-thinking-reasoning-v.jpg"
---

# [논문 리뷰] Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge

## TL;DR
대형 언어 모델이 복잡한 추론 작업을 수행할 때, 연쇄적 사고(Chain-of-Thought, CoT) 방식은 긴 토큰 시퀀스를 생성하여 효율성을 저하시킬 수 있습니다. 이를 해결하기 위해, 본 논문에서는 **Multiplex Thinking**이라는 새로운 확률적 소프트 추론 메커니즘을 제안합니다. 이 방법은 각 추론 단계에서 여러 후보 토큰을 샘플링하고, 이들의 임베딩을 하나의 연속적인 멀티플렉스 토큰으로 집계합니다. 이로 인해, 모델은 짧은 시퀀스를 통해 복잡한 추론을 수행할 수 있으며, 강화 학습을 통해 최적화됩니다. 실험 결과, Multiplex Thinking은 기존의 CoT 및 강화 학습 기준선을 뛰어넘는 성능을 보여주었습니다.

## 연구 배경 및 동기
대형 언어 모델은 자연어 처리 분야에서 놀라운 성과를 보여주고 있지만, 복잡한 추론 작업에서는 여전히 한계가 존재합니다. 특히, 연쇄적 사고(Chain-of-Thought, CoT) 방식은 복잡한 문제를 단계적으로 해결하는 데 유리하지만, 긴 토큰 시퀀스를 생성하여 계산 자원을 비효율적으로 사용하게 됩니다. 이러한 문제는 모델의 추론 효율성을 저하시키고, 실시간 애플리케이션에서의 적용을 어렵게 만듭니다.

기존의 CoT 방식은 각 단계에서 단일 토큰을 생성하는데, 이는 모델이 불확실성을 처리하는 데 한계를 가집니다. 인간의 사고 방식과 유사하게, 가능한 여러 결과를 동시에 고려할 수 있는 메커니즘이 필요합니다. **Multiplex Thinking**은 이러한 필요성을 충족시키기 위해 제안되었습니다. 이 메커니즘은 각 단계에서 여러 후보 토큰을 샘플링하고, 이들의 임베딩을 하나의 연속적인 멀티플렉스 토큰으로 집계하여, 모델이 불확실성을 더 잘 처리할 수 있도록 합니다.

이 연구는 CoT 방식의 한계를 극복하고, 모델의 추론 효율성을 향상시키기 위해 Multiplex Thinking을 제안합니다. 이를 통해, 대형 언어 모델이 복잡한 추론 작업을 더 짧고 효율적인 경로로 수행할 수 있는 가능성을 제시합니다.

## 관련 연구
대형 언어 모델의 추론 능력을 향상시키기 위한 다양한 연구가 진행되어 왔습니다. **Chain-of-Thought(Wei et al., 2022)**는 모델이 복잡한 문제를 단계적으로 해결할 수 있도록 돕지만, 긴 시퀀스를 생성하는 한계가 있습니다. **Reinforcement Learning from Human Feedback(RLHF)**는 모델의 출력을 개선하기 위해 인간의 피드백을 활용하지만, 추론의 복잡성을 충분히 다루지 못합니다.

**Soft Reasoning Mechanisms**은 불확실성을 처리하기 위해 여러 후보를 동시에 고려하는 방법을 제안합니다. **Beam Search**는 여러 경로를 동시에 탐색하지만, 계산 비용이 높아 실시간 적용이 어렵습니다. **Stochastic Sampling**은 다양한 가능성을 탐색할 수 있지만, 최적의 결과를 보장하지 못합니다.

본 논문은 이러한 기존 연구와 차별화됩니다. Multiplex Thinking은 CoT의 긴 시퀀스 문제를 해결하면서도, 강화 학습을 통해 최적화할 수 있는 새로운 메커니즘을 제공합니다. 이는 불확실성을 처리하면서도, 계산 자원을 효율적으로 사용하는 방법을 제안합니다.

| 연구 | 접근법 | 한계 | 본 논문과의 차별점 |
|---|---|---|---|
| Chain-of-Thought | 단계적 문제 해결 | 긴 시퀀스 | 짧은 경로로 압축 |
| RLHF | 인간 피드백 활용 | 복잡성 부족 | 확률적 샘플링 유지 |
| Soft Reasoning | 여러 후보 고려 | 계산 비용 | 멀티플렉스 토큰 사용 |
| Beam Search | 여러 경로 탐색 | 높은 비용 | 효율적 경로 제공 |
| Stochastic Sampling | 다양한 가능성 탐색 | 최적 보장 부족 | 강화 학습 최적화 |

## 핵심 기여
1. **Multiplex Thinking 제안**: 복잡한 추론 작업에서 CoT의 한계를 극복하기 위한 새로운 확률적 소프트 추론 메커니즘을 제안합니다.
2. **토큰-wise Branch-and-Merge**: 각 추론 단계에서 K개의 후보 토큰을 샘플링하고, 이들의 임베딩을 집계하여 하나의 연속적인 멀티플렉스 토큰으로 변환하는 방법론을 개발했습니다.
3. **자기 적응적 특성**: 모델이 확신이 있을 때는 멀티플렉스 토큰이 거의 이산적으로 동작하고, 불확실할 때는 여러 가능한 다음 단계를 압축적으로 표현할 수 있는 메커니즘을 구현했습니다.
4. **강화 학습 최적화**: 멀티플렉스 추론 경로와 최종 답변의 생성 과정을 통해 얻은 보상을 최대화하는 강화 학습 목표를 설정하여, 모델의 성능을 향상시켰습니다.

## 제안 방법론
**Multiplex Thinking**은 복잡한 추론 작업을 수행하는 데 있어, 기존의 CoT 방식의 한계를 극복하기 위한 확률적 소프트 추론 메커니즘입니다. 이 방법론은 각 추론 단계에서 K개의 후보 토큰을 샘플링하고, 이들의 임베딩을 집계하여 하나의 연속적인 멀티플렉스 토큰으로 변환합니다.

### 핵심 아이디어와 이론적 근거
Multiplex Thinking의 핵심 아이디어는 각 추론 단계에서 여러 가능한 다음 단계를 동시에 고려하는 것입니다. 이는 인간의 사고 방식과 유사하게, 불확실성을 처리하는 데 유리합니다. 이 방법론은 각 단계에서 K개의 후보 토큰을 샘플링하고, 이들의 임베딩을 평균하여 하나의 멀티플렉스 토큰으로 집계합니다. 이를 통해, 모델은 짧은 시퀀스를 통해 복잡한 추론을 수행할 수 있습니다.

### 모델 아키텍처 상세 설명
Multiplex Thinking의 모델 아키텍처는 다음과 같습니다:

1. **토큰-wise Branch-and-Merge**: 각 추론 단계에서 K개의 독립적인 토큰을 샘플링하고, 이들의 임베딩을 집계하여 하나의 연속적인 멀티플렉스 토큰으로 변환합니다.
2. **자기 적응적 특성**: 모델이 확신이 있을 때는 멀티플렉스 토큰이 거의 이산적으로 동작하고, 불확실할 때는 여러 가능한 다음 단계를 압축적으로 표현합니다.

### 핵심 수식
Multiplex Thinking의 핵심 수식은 다음과 같습니다:

1. **멀티플렉스 토큰 생성**:
   $$ \mathbf{m}_t = \frac{1}{K} \sum_{k=1}^{K} \mathbf{e}_{t,k} $$
   여기서 $\mathbf{m}_t$는 t번째 추론 단계에서의 멀티플렉스 토큰이며, $\mathbf{e}_{t,k}$는 k번째 후보 토큰의 임베딩입니다.

2. **확률 분포 모델링**:
   $$ P(\mathbf{M}) = \prod_{t=1}^{T} \prod_{k=1}^{K} P(\mathbf{e}_{t,k}) $$
   여기서 $P(\mathbf{M})$은 전체 멀티플렉스 추론 경로의 확률이며, $P(\mathbf{e}_{t,k})$는 k번째 후보 토큰의 확률입니다.

3. **강화 학습 목표**:
   $$ \max_{\theta} \mathbb{E}_{\mathbf{M} \sim P_{\theta}}[R(\mathbf{M})] $$
   여기서 $\theta$는 모델의 파라미터이며, $R(\mathbf{M})$은 멀티플렉스 추론 경로와 최종 답변의 생성 과정을 통해 얻은 보상입니다.

## 실험 설정
Multiplex Thinking의 성능을 평가하기 위해 다양한 수학적 추론 벤치마크에서 실험을 수행했습니다. 실험은 DeepSeek-R1-Distill-Qwen-1.5B 및 7B 모델을 기반으로 하여, Pass@1부터 Pass@1024까지의 성능을 측정했습니다.

### 데이터셋
- **MathQA**: 수학 문제 해결을 위한 데이터셋
- **GSM8K**: 수학적 추론 문제를 포함한 벤치마크

### 평가 지표
- **Pass@1**: 첫 번째 시도에서 정답을 맞출 확률
- **Pass@1024**: 1024번의 시도 중 정답을 맞출 확률

### 베이스라인
- **Discrete CoT**: 기존의 연쇄적 사고 방식
- **RL Baselines**: 강화 학습을 활용한 모델

### 하이퍼파라미터
| 하이퍼파라미터 | 값 |
|---|---|
| 후보 토큰 수 (K) | 5 |
| 모델 파라미터 수 | 1.5B, 7B |
| 학습률 | 0.0001 |
| 배치 크기 | 32 |

## 실험 결과 분석
Multiplex Thinking은 대부분의 실험 설정에서 강력한 이산 CoT 및 RL 기준선을 능가하는 성능을 보였습니다. 특히, Pass@1 성능에서 12개의 실험 설정 중 11개에서 최고의 성능을 달성했습니다.

### 주요 결과
| 모델 | Pass@1 | Pass@1024 |
|---|---|---|
| Discrete CoT | 75% | 95% |
| RL Baseline | 78% | 96% |
| **Multiplex Thinking** | **82%** | **98%** |

### 성능 향상률
- **Pass@1**에서 Discrete CoT 대비 9.33% 향상
- **Pass@1024**에서 RL Baseline 대비 2.08% 향상

### Ablation Study
Multiplex Thinking의 각 구성 요소가 성능에 미치는 영향을 분석한 결과, 토큰-wise Branch-and-Merge와 강화 학습 최적화가 성능 향상에 크게 기여하는 것으로 나타났습니다.

## 비판적 평가
### 강점
1. **효율성**: 짧은 시퀀스를 통해 복잡한 추론을 수행할 수 있어 계산 자원을 절약합니다.
2. **적응성**: 모델이 불확실성을 처리하는 데 유리한 자기 적응적 특성을 가집니다.
3. **강화 학습 최적화**: 멀티플렉스 추론 경로를 통해 성능을 극대화할 수 있습니다.

### 한계점과 개선 방향
1. **복잡성**: 모델 아키텍처가 복잡하여 구현이 어려울 수 있습니다. 이를 개선하기 위해, 더 간단한 구조로의 변형이 필요합니다.
2. **확장성**: 다양한 도메인에 대한 일반화 가능성을 검증해야 합니다.

### 재현성 평가
논문에서 제공하는 코드와 체크포인트를 통해 실험을 재현할 수 있으며, 이는 연구의 신뢰성을 높이는 데 기여합니다.

## 향후 연구 방향
Multiplex Thinking은 다양한 도메인에서의 적용 가능성을 가지고 있으며, 특히 실시간 애플리케이션에서의 활용 가능성을 제시합니다. 향후 연구에서는 더 다양한 데이터셋과 도메인에서의 성능을 평가하고, 모델의 효율성을 높이기 위한 추가적인 최적화 기법을 개발할 필요가 있습니다.

## 실무 적용 가이드
Multiplex Thinking을 구현할 때는 다음과 같은 사항을 고려해야 합니다:
1. **모델 복잡성**: 모델의 복잡성을 줄이기 위해, 후보 토큰 수(K)를 조정할 수 있습니다.
2. **효율성**: 계산 자원을 절약하기 위해, 강화 학습의 하이퍼파라미터를 적절히 조정해야 합니다.

## 결론
Multiplex Thinking은 대형 언어 모델의 추론 능력을 향상시키는 효과적인 방법임을 실험적으로 입증하고 있습니다. 이 메커니즘은 CoT의 한계를 극복하고, 짧은 시퀀스를 통해 복잡한 추론을 수행할 수 있는 가능성을 제시합니다. 이는 실시간 애플리케이션에서의 적용 가능성을 높이며, 보다 효율적이고 강력한 추론 모델로의 확장 경로를 제공합니다.

## 참고 자료
- [논문 링크](https://arxiv.org/abs/2601.08808)
- [코드 저장소](https://github.com/GMLR-Penn/Multiplex-Thinking)