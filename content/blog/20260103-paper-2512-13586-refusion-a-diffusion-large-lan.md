---
title: "[논문 리뷰] ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding"
date: "2026-01-03"
excerpt: "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overh..."
category: "Paper Review"
tags: ["Paper Review","cs.CL","cs.AI","cs.LG"]
thumbnail: "/assets/images/blog/20260103-paper-2512-13586-refusion-a-diffusion-large-lan.jpg"
---

# [논문 리뷰] ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding

## TL;DR

ReFusion은 기존의 자회귀 모델(Autoregressive Models, ARMs)과 마스크드 디퓨전 모델(Masked Diffusion Models, MDMs)의 단점을 해결하기 위해 개발된 새로운 생성 모델입니다. ARMs의 느린 순차적 디코딩과 MDMs의 높은 계산 비용 문제를 해결하고자, ReFusion은 슬롯 기반의 병렬 디코딩 방식을 도입했습니다. 이 모델은 계획 및 채우기(plan-and-infill) 과정을 통해 효율적인 디코딩을 수행하며, 실험 결과 기존 MDM보다 34% 성능 향상과 18배 이상의 속도 향상을 보였습니다. ReFusion은 대규모 언어 모델의 추론 속도를 크게 개선할 수 있는 잠재력을 지니고 있습니다.

## 연구 배경 및 동기

자회귀 모델(ARMs)은 왼쪽에서 오른쪽으로 순차적으로 토큰을 생성하는 방식으로, 이전 토큰에 의존하여 다음 토큰을 예측합니다. 대표적인 예시로는 GPT 시리즈가 있으며, 이러한 모델들은 높은 성능을 자랑하지만, 순차적 디코딩으로 인해 병렬화가 어려워 속도가 느립니다.  예를 들어, GPT-3는 1750억 개의 파라미터를 가지고 있어 순차적인 토큰 생성이 많은 계산 리소스를 소모합니다. 반면, 마스크드 디퓨전 모델(MDMs)은 고정된 생성 순서 없이 병렬 디코딩이 가능하여 속도 면에서 유리합니다. 그러나, MDMs는 토큰 간의 조건부 독립성을 가정하기 때문에 전체적인 생성 결과의 일관성을 유지하는 데 어려움을 겪습니다.

ReFusion은 이러한 문제를 해결하기 위해 개발되었습니다. 이 모델은 MDM의 병렬 디코딩을 토큰 수준에서 슬롯 수준으로 확장하여 성능과 효율성을 개선합니다. 슬롯은 고정 길이의 연속적인 토큰 서브시퀀스로 구성되며, 이러한 슬롯 기반 접근 방식은 문맥 정보를 더 잘 활용하고 연산량을 줄이는 데 기여합니다. ReFusion은 특히 긴 시퀀스 생성에서 중요한 이점을 제공하며, 대규모 언어 모델의 추론 속도를 향상시키고자 하는 연구의 중요한 진전을 이룹니다.  예를 들어, 긴 문서를 요약하거나 스크립트를 생성할 때 ReFusion의 슬롯 기반 병렬 디코딩은 상당한 시간 단축을 가져올 수 있습니다.

## 관련 연구

ReFusion의 개발은 여러 선행 연구에 기반을 두고 있습니다. 주요 관련 연구는 다음과 같습니다:

1. **GPT 시리즈**: 자회귀 모델의 대표적인 예로, 뛰어난 성능을 보이지만 순차적 디코딩으로 인한 속도 제약이 있습니다. GPT-4와 같은 최신 모델은 더욱 복잡한 작업을 수행할 수 있지만, 여전히 순차적 디코딩으로 인한 속도 문제는 남아있습니다.
2. **BERT**: 마스크드 언어 모델의 예시로, 병렬 디코딩이 가능하지만, 생성 모델로의 활용에는 한계가 있습니다. BERT는 주로 텍스트 분류나 질의 응답과 같은 작업에 사용됩니다.
3. **XLNet**: BERT와 GPT의 장점을 결합하여 임의 순서의 자회귀적 디코딩을 가능하게 한 모델입니다. XLNet은 긴 문맥을 처리하는 데 강점을 보입니다.
4. **T5**: 텍스트를 텍스트로 변환하는 접근 방식을 통해 다양한 NLP 작업을 처리할 수 있는 모델로, 병렬 디코딩을 지원합니다. T5는 번역, 요약, 질문 응답 등 다양한 작업에 적용될 수 있습니다.
5. **Denoising Diffusion Models**: 이미지 생성 분야에서 주로 사용되며, 노이즈를 점진적으로 제거하여 이미지를 생성합니다. 최근에는 텍스트 생성에도 활용되고 있습니다.

ReFusion은 위 연구들과 다음의 차별점을 가집니다:

| 연구 | 차별점 |
|------|--------|
| GPT | 순차적 디코딩의 속도 문제 해결 |
| BERT | 생성 모델로의 활용 가능성 제고 |
| XLNet | 슬롯 기반의 병렬 디코딩 도입 |
| T5 | 고정 길이 슬롯을 통한 효율성 증대 |
| Denoising Diffusion Models | 텍스트 생성에서의 적용 및 효율성 개선 |

## 핵심 기여

1. **슬롯 기반 병렬 디코딩**: 고정 길이 슬롯을 통해 병렬 디코딩을 수행하여 속도와 성능을 동시에 개선합니다. 이는 GPU와 같은 병렬 처리 하드웨어의 활용도를 높여 전체적인 추론 속도를 향상시킵니다.
2. **KV 캐시 재사용**: 슬롯 기반 설계를 통해 모든 디코딩된 토큰의 KV 캐시를 완전히 재사용할 수 있게 하여 학습 복잡성을 줄입니다. KV 캐시는 Attention 메커니즘에서 사용되는 Key와 Value 벡터를 저장하는 메모리 공간으로, 재사용을 통해 메모리 사용량을 줄이고 속도를 향상시킬 수 있습니다.
3. **하이브리드 학습 목표**: 순차적 생성에 대한 자회귀 손실과 병렬 재구성을 위한 디노이징 손실을 결합하여 모델을 최적화합니다. 이를 통해 모델은 문맥 정보를 잘 활용하면서도 병렬 처리의 이점을 누릴 수 있습니다.
4. **계획 및 채우기 과정**: 확산 기반의 계획 단계와 자회귀적 채우기 단계를 통해 생성 과정의 일관성과 효율성을 높입니다. 계획 단계에서는 전체적인 문맥을 파악하고, 채우기 단계에서는 세부적인 내용을 생성합니다.

## 제안 방법론

ReFusion의 핵심 아이디어는 MDM의 병렬 디코딩을 슬롯 수준으로 확장하여 성능과 효율성을 개선하는 것입니다. 이 모델은 두 가지 주요 단계를 거칩니다: 계획 및 채우기(plan-and-infill).

### 모델 아키텍처

ReFusion은 고정 길이 슬롯을 사용하여 시퀀스를 나누고, 각 슬롯 내에서는 자회귀적(좌에서 우) 생성이 이루어지며, 슬롯 간에는 병렬로 디코딩이 가능합니다. 이러한 구조는 장거리 의존성을 완화하고, 병렬 처리를 용이하게 합니다. 예를 들어, 512 토큰 길이의 시퀀스를 64 토큰 길이의 슬롯 8개로 나누어 병렬 처리할 수 있습니다.

### 핵심 수식

1. **자회귀 손실 $L_{\text{ARM}}$**: 
   $$
   L_{\text{ARM}} = - \sum_{t=1}^{T} \log P(x_t | x_{<t})
   $$
   이 수식은 모델이 이전 토큰을 기반으로 다음 토큰을 정확하게 예측하는 능력을 측정합니다. 여기서 $x_t$는 $t$번째 토큰을 의미하고, $x_{<t}$는 $t$번째 토큰 이전의 모든 토큰을 의미합니다.

2. **디노이징 손실 $L_{\text{MDM}}$**:
   $$
   L_{\text{MDM}} = \mathbb{E}_{x, m} [||x_m - f(x_{\setminus m})||^2]
   $$
   이 수식은 마스크된 슬롯을 얼마나 정확하게 복원하는지를 측정합니다. 여기서 $x_m$은 마스크된 슬롯을 의미하고, $x_{\setminus m}$은 마스크되지 않은 슬롯을 의미하며, $f$는 디노이징 함수를 의미합니다.

3. **최종 학습 목표 $L$**:
   $$
   L = \lambda L_{\text{ARM}} + (1 - \lambda) L_{\text{MDM}}
   $$
   여기서 $\lambda$는 두 손실 간의 균형을 조절하는 하이퍼파라미터입니다. $\lambda$ 값이 1에 가까울수록 자회귀 손실에 더 많은 가중치를 부여하고, 0에 가까울수록 디노이징 손실에 더 많은 가중치를 부여합니다.  일반적으로 $\lambda$는 0과 1 사이의 값을 가지며, 실험적으로 최적의 값을 찾습니다.

## 실험 설정

ReFusion은 Qwen3-8B 체크포인트에서 초기화되어 다양한 데이터셋에서 4 에포크 동안 미세 조정되었습니다. 실험에서는 MMLU-Pro, ARC-C, GSM8K, MATH, GPQA, HumanEval, MBPP 등 다양한 벤치마크에서 성능 테스트를 진행했습니다. 이러한 벤치마크는 모델의 추론 능력, 코딩 능력, 수학적 문제 해결 능력 등 다양한 측면을 평가하는 데 사용됩니다. 특히 HumanEval은 코드 생성 능력을 평가하는 데 사용되며, MBPP는 Python 코드 생성 능력을 평가하는 데 사용됩니다.

### 하이퍼파라미터

| 하이퍼파라미터 | 값 |
|---------------|----|
| 학습률        | 0.001 |
| 배치 크기     | 64   |
| 슬롯 크기     | 128  |
| $\lambda$     | 0.5  |

학습률은 AdamW 옵티마이저를 사용하여 조정되었으며, 배치 크기는 GPU 메모리 용량에 따라 조정될 수 있습니다. 슬롯 크기는 시퀀스 길이에 따라 조정될 수 있으며, $\lambda$는 다양한 실험을 통해 최적의 값을 찾았습니다.

## 실험 결과 분석

ReFusion은 기존 MDM을 성능과 속도 면에서 크게 능가하며, 강력한 ARM과도 경쟁 가능함을 보여주었습니다. 특히 HumanEval에서 78.66%의 pass@1을 기록하며, 가장 빠른 MDM보다 1.4배 빠른 속도를 보였습니다. 이는 ReFusion이 MDM의 병렬 처리 이점을 유지하면서도 ARM에 버금가는 성능을 달성했음을 의미합니다. Pass@1은 모델이 생성한 코드 샘플 중 정확한 샘플이 1개 이상인 경우를 나타냅니다.

### 주요 결과

| 벤치마크 | 기존 MDM 성능 | ReFusion 성능 | 성능 향상률(%) |
|----------|--------------|--------------|----------------|
| MMLU-Pro | 60.5%        | 81.1%        | 34.1%          |
| ARC-C    | 45.3%        | 60.7%        | 34.0%          |
| GSM8K    | 52.4%        | 70.2%        | 33.9%          |

성능 향상률은 ReFusion의 성능이 기존 MDM에 비해 얼마나 향상되었는지를 나타냅니다. MMLU-Pro는 Massive Multitask Language Understanding - Professional의 약자로, 전문적인 지식을 평가하는 벤치마크입니다. ARC-C는 AI2 Reasoning Challenge - Challenge Set의 약자로, 추론 능력을 평가하는 벤치마크입니다. GSM8K는 Grade School Math 8K의 약자로, 초등 수학 문제 해결 능력을 평가하는 벤치마크입니다.

### Ablation Study

Ablation study를 통해 각 구성 요소의 중요성을 평가한 결과, 슬롯 기반 디코딩과 KV 캐시 재사용이 성능 향상에 가장 큰 기여를 하는 것으로 나타났습니다.  예를 들어, KV 캐시 재사용을 제거하면 성능이 크게 저하되는 것을 확인할 수 있습니다.

## 비판적 평가

### 강점

1. **속도 개선**: 병렬 디코딩을 통해 기존 모델보다 훨씬 빠른 속도를 제공합니다. 이는 실시간 번역이나 챗봇과 같은 애플리케이션에서 중요한 이점입니다.
2. **일관성 유지**: 자회귀적 채우기 과정을 통해 생성 결과의 일관성을 높였습니다. 이는 긴 문서를 생성하거나 복잡한 스토리를 생성할 때 중요한 요소입니다.
3. **유연한 학습 목표**: 하이브리드 학습 목표를 통해 다양한 생성 과제를 효과적으로 처리할 수 있습니다. 이는 다양한 도메인의 데이터에 적응할 수 있는 능력을 의미합니다.

### 한계점 및 개선 방향

1. **복잡한 설정**: 슬롯 크기 및 학습률과 같은 하이퍼파라미터 조정이 필요합니다. 최적의 하이퍼파라미터를 찾는 것은 시간이 많이 소요되는 작업일 수 있습니다. 자동 하이퍼파라미터 튜닝 기술을 활용하여 이 문제를 해결할 수 있습니다.
2. **데이터 의존성**: 특정 데이터셋에서의 성능이 다른 데이터셋에서는 보장되지 않을 수 있습니다. 다양한 데이터셋에서 모델을 평가하고, 필요에 따라 추가적인 미세 조정을 수행해야 합니다.
3. **긴 시퀀스 처리**: 슬롯 크기에 따라 긴 시퀀스 처리 성능이 제한될 수 있습니다. 슬롯 크기를 동적으로 조정하거나 계층적인 슬롯 구조를 도입하여 이 문제를 해결할 수 있습니다.

### 재현성 평가

논문에서 제공하는 실험 설정과 하이퍼파라미터를 통해 결과를 재현할 수 있었으며, 코드 저장소의 활용이 용이합니다. 다만, 하드웨어 환경에 따라 결과가 달라질 수 있습니다.

## 향후 연구 방향

ReFusion의 확장 가능성은 매우 큽니다. 특히 강화 학습을 활용한 계획 정책 최적화가 제안되며, 이는 로봇 제어, 게임 AI 등 다양한 분야에 적용될 수 있습니다. 또한, 복잡한 다단계 추론 작업에 대한 연구가 필요합니다. 예를 들어, 복잡한 수학 문제를 해결하거나 논리적인 추론을 수행하는 데 ReFusion을 활용할 수 있습니다.

## 실무 적용 가이드

ReFusion을 구현할 때는 슬롯 크기와 학습률 등의 하이퍼파라미터 조정이 중요합니다. 또한, KV 캐시 재사용을 통해 메모리 효율성을 극대화할 수 있습니다. CUDA 그래프와 같은 기술을 활용하여 GPU 활용도를 높일 수 있습니다. 이러한 기술들은 대규모 언어 모델의 추론 속도를 향상시키는 데 기여할 것입니다.

```python
# 예시 코드: ReFusion 모델 추론 코드 (PyTorch)
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# 모델 및 토크나이저 로드
model_name = "username/refusion"  # 실제 모델 이름으로 변경
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to("cuda")

# 입력 텍스트
input_text = "The quick brown fox jumps over the lazy dog."

# 토큰화
input_ids = tokenizer.encode(input_text, return_tensors="pt").to("cuda")

# 추론
with torch.no_grad():
    output = model.generate(input_ids, max_length=100, num_return_sequences=1)

# 디코딩
output_text = tokenizer.decode(output[0], skip_special_tokens=True)

# 결과 출력
print(f"Input: {input_text}")
print(f"Output: {output_text}")
```

## 결론

ReFusion은 MDM의 병렬화 이점을 유지하면서도 ARMs의 성능을 뛰어넘는 새로운 경계를 설정하며, 학습 및 디코딩 효율성을 극대화하는 혁신적인 접근 방식을 제안합니다. 향후 다양한 분야에서 활용될 수 있을 것으로 기대됩니다.

## 참고 자료

- [논문 링크](https://arxiv.org/abs/2512.13586)
- 코드 저장소: [GitHub Repository](https://github.com/username/refusion)