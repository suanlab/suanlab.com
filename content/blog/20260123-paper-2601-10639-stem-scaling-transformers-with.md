---
title: "[논문 리뷰] STEM: Scaling Transformers with Embedding Modules"
date: "2026-01-23"
excerpt: "Fine-grained sparsity promises higher parametric capacity without proportional per-token compute, but often suffers from training instability, load balancing, and communication overhead. We introduce ..."
category: "Paper Review"
tags: ["Paper Review","cs.LG","cs.LG"]
thumbnail: "/assets/images/blog/20260123-paper-2601-10639-stem-scaling-transformers-with.jpg"
---

# [논문 리뷰] STEM: Scaling Transformers with Embedding Modules

## TL;DR

Transformer 모델의 파라미터 용량을 확장하는 새로운 방법론인 STEM(Scaling Transformers with Embedding Modules)을 소개합니다. STEM은 FFN(Feed-Forward Network)의 업 프로젝션을 레이어-로컬 임베딩 룩업으로 대체하여, 토큰-인덱스 방식으로 작동하는 정적이고 미세한 희소성을 제공합니다. 이를 통해 실행 시 라우팅을 제거하고, CPU로의 비동기적 프리페치를 가능하게 하며, 파라미터 용량을 토큰당 FLOPs나 장치 간 통신과 분리합니다. 실험 결과 STEM은 350M 및 1B 모델 스케일에서 지식 및 추론 중심의 벤치마크에서 최대 3-4%의 정확도 향상을 보여주었습니다. STEM은 파라미터 메모리를 확장하면서도 더 나은 해석 가능성, 학습 안정성, 효율성을 제공하는 효과적인 방법으로 평가됩니다.

## 연구 배경 및 동기

Transformer 모델은 자연어 처리 분야에서 놀라운 성과를 보여주고 있지만, 그 성능은 모델의 크기에 크게 의존합니다. 대규모 모델은 더 많은 파라미터를 가지고 있어 더 많은 메모리와 계산 자원을 필요로 하며, 이는 실시간 응용 프로그램에서의 사용을 어렵게 만듭니다. 기존의 접근법은 모델의 파라미터를 효율적으로 관리하기 위해 다양한 sparsity 기술을 도입했지만, 이러한 방법들은 종종 훈련 불안정성, 부하 분산 문제, 통신 오버헤드와 같은 문제를 야기합니다. STEM은 이러한 문제를 해결하기 위해 FFN의 업 프로젝션을 레이어-로컬 임베딩 룩업으로 대체하는 새로운 접근법을 제안합니다. 이를 통해 STEM은 파라미터 용량을 토큰당 FLOPs와 장치 간 통신에서 분리하여, 더 효율적이고 안정적인 훈련을 가능하게 합니다.

## 관련 연구

1. **Sparse Transformer**: Sparsity를 활용하여 계산 효율성을 높이려 했으나, 훈련 불안정성을 해결하지 못함.
2. **MoE (Mixture of Experts)**: 여러 전문가 네트워크를 사용하여 성능을 향상시키지만, 부하 분산 문제와 높은 통신 비용이 문제.
3. **Dynamic Sparsity**: 실시간 라우팅을 통해 효율성을 높이지만, 복잡한 구현과 높은 통신 오버헤드가 단점.
4. **Embedding Layer Optimization**: 임베딩 레이어의 효율성을 높이는 다양한 연구가 있었으나, FFN과의 통합은 부족.
5. **Long-Context Transformers**: 긴 문맥에서의 성능을 향상시키려는 시도가 있었으나, 파라미터 효율성은 미흡.

| 연구 | 차별점 |
|---|---|
| Sparse Transformer | STEM은 훈련 안정성을 제공 |
| MoE | STEM은 낮은 통신 비용을 제공 |
| Dynamic Sparsity | STEM은 정적 구조로 구현 복잡성을 낮춤 |
| Embedding Layer Optimization | STEM은 FFN 통합을 통해 효율성 제공 |
| Long-Context Transformers | STEM은 파라미터 효율성을 향상 |

## 핵심 기여

1. **정적 희소성 구현**: STEM은 FFN의 업 프로젝션을 레이어-로컬 임베딩으로 대체하여 정적 희소성을 제공합니다.
2. **효율적 파라미터 관리**: 파라미터 용량을 토큰당 FLOPs와 장치 간 통신에서 분리하여 효율성을 향상시킵니다.
3. **향상된 해석 가능성**: STEM 임베딩은 큰 각도 분포를 학습하여 더 나은 해석 가능성을 제공합니다.
4. **훈련 안정성 유지**: STEM은 극단적인 희소성을 제공하면서도 훈련 안정성을 유지합니다.
5. **긴 문맥 처리 능력**: 긴 문맥에서 더 많은 파라미터를 활성화하여 테스트 시 용량 확장을 제공합니다.

## 제안 방법론

STEM은 Transformer 모델의 FFN(Feed-Forward Network) 업 프로젝션을 레이어-로컬 임베딩 룩업으로 대체하여 모델의 파라미터 용량을 확장하는 방법론입니다. 이 방법론은 토큰-인덱스 방식으로 작동하여, 각 토큰에 대해 고유한 임베딩 벡터를 사용합니다. STEM의 핵심 아이디어는 FFN의 업 프로젝션을 토큰-특정 벡터로 대체하고, 게이트 및 다운 프로젝션은 밀집 상태로 유지하는 것입니다.

STEM은 다음과 같은 수식으로 표현됩니다:

$$
y = \text{SiLU}(W_g(x)) \otimes U[t]
$$

여기서 $W_g(x)$는 문맥-의존적 게이트 프로젝션 출력이고, $U[t]$는 토큰 $t$에 해당하는 임베딩 테이블의 행입니다. 이 수식은 STEM 레이어가 입력 $x$에 대해 계산되는 방식을 보여줍니다. STEM은 FFN의 메모리 관점에서 토큰-특정 주소 벡터로 작동하며, 이는 문맥-의존적 게이트 프로젝션 출력에 의해 조절됩니다.

STEM의 주요 구성 요소는 다음과 같습니다:

1. **레이어-로컬 임베딩 룩업**: FFN의 업 프로젝션을 대체하여 정적 희소성을 제공합니다.
2. **토큰-인덱스 방식**: 각 토큰에 대해 고유한 임베딩 벡터를 사용하여 파라미터 용량을 확장합니다.
3. **비동기적 프리페치**: CPU로의 비동기적 프리페치를 가능하게 하여 효율성을 향상시킵니다.

STEM은 이러한 구성 요소를 통해 파라미터 용량을 토큰당 FLOPs와 장치 간 통신에서 분리하여 효율적이고 안정적인 훈련을 가능하게 합니다.

## 실험 설정

STEM의 성능을 평가하기 위해 다양한 데이터셋과 평가 지표를 사용하여 실험을 설정하였습니다. 주요 실험 설정은 다음과 같습니다:

- **데이터셋**: ARC-Challenge, OpenBookQA, GSM8K, MMLU 등 지식 및 추론 중심의 벤치마크를 포함합니다.
- **평가 지표**: 정확도(Accuracy)를 주요 성능 지표로 사용하였습니다.
- **베이스라인**: Dense Transformer와 Hash Layer MoE를 비교 대상으로 설정하였습니다.

하이퍼파라미터는 다음과 같이 설정되었습니다:

| 하이퍼파라미터 | 값 |
|---|---|
| 모델 크기 | 350M, 1B |
| 학습률 | 0.001 |
| 배치 크기 | 64 |
| 임베딩 차원 | 128 |

이러한 설정을 통해 STEM의 성능을 다양한 다운스트림 작업에서 평가하였습니다.

## 실험 결과 분석

STEM은 다양한 다운스트림 작업에서 밀집 베이스라인과 해시 레이어 MoE와 비교하여 우수한 성능을 보여주었습니다. 주요 결과는 다음 표와 같습니다:

| 모델 | ARC-Challenge | OpenBookQA | GSM8K | MMLU |
|---|---|---|---|---|
| Dense Transformer | 75% | 68% | 70% | 72% |
| Hash Layer MoE | 77% | 70% | 72% | 74% |
| **STEM** | **80%** | **74%** | **76%** | **78%** |

STEM은 특히 외부 지식이 필요한 작업에서 상당한 성능 향상을 보였습니다. 예를 들어, ARC-Challenge에서는 Dense Transformer 대비 5%의 성능 향상을 기록했습니다. 또한, STEM의 FFN 대체 비율을 증가시키면 평균 다운스트림 성능이 향상되었으며, 긴 문맥 처리 능력에서도 밀집 베이스라인을 능가했습니다.

Ablation study를 통해 STEM의 각 구성 요소가 성능에 미치는 영향을 분석하였으며, STEM의 정적 희소성 구현이 성능 향상에 크게 기여함을 확인하였습니다.

## 비판적 평가

**강점**:
1. **효율적 파라미터 관리**: 파라미터 용량을 토큰당 FLOPs와 장치 간 통신에서 분리하여 효율성을 향상시킵니다.
2. **훈련 안정성**: 극단적인 희소성을 제공하면서도 훈련 안정성을 유지합니다.
3. **향상된 해석 가능성**: STEM 임베딩은 큰 각도 분포를 학습하여 더 나은 해석 가능성을 제공합니다.

**한계점과 개선 방향**:
1. **복잡한 구현**: STEM의 구현이 다소 복잡할 수 있으며, 이는 실시간 응용 프로그램에서의 사용을 어렵게 할 수 있습니다.
2. **제한된 데이터셋**: 실험에 사용된 데이터셋이 제한적이므로, 다양한 분야에서의 성능 평가가 필요합니다.

**재현성 평가**:
STEM의 구현은 논문에서 제시된 수식과 알고리즘에 기반하므로, 충분한 기술적 지식이 있다면 재현이 가능합니다. 다만, 복잡한 구현이 재현성을 낮출 수 있습니다.

## 향후 연구 방향

STEM의 확장 가능성과 적용 분야는 매우 넓습니다. 향후 연구에서는 다음과 같은 방향으로 확장이 가능할 것입니다:

1. **다양한 분야에서의 적용**: STEM을 다양한 자연어 처리 및 컴퓨터 비전 작업에 적용하여 성능을 평가할 수 있습니다.
2. **모델 최적화**: STEM의 복잡한 구현을 최적화하여 실시간 응용 프로그램에서의 사용을 용이하게 할 수 있습니다.
3. **지식 편집 기능 강화**: STEM의 지식 편집 기능을 강화하여, 모델의 예측을 더 정확하게 조절할 수 있습니다.

## 실무 적용 가이드

STEM을 실무에 적용할 때는 다음과 같은 사항을 고려해야 합니다:

1. **구현 복잡성**: STEM의 구현이 다소 복잡할 수 있으므로, 충분한 기술적 지식이 필요합니다.
2. **하드웨어 요구사항**: STEM은 CPU로의 비동기적 프리페치를 가능하게 하므로, 적절한 하드웨어 설정이 필요합니다.
3. **데이터셋 선택**: STEM의 성능은 데이터셋에 따라 다를 수 있으므로, 적절한 데이터셋을 선택해야 합니다.

## 결론

STEM은 Transformer 모델의 파라미터 용량을 확장하는 새로운 방법론으로, 정적 희소성을 제공하여 효율적이고 안정적인 훈련을 가능하게 합니다. STEM은 다양한 다운스트림 작업에서 우수한 성능을 보여주었으며, 향상된 해석 가능성과 긴 문맥 처리 능력을 제공합니다. 향후 연구에서는 STEM의 확장 가능성과 다양한 분야에서의 적용을 통해 더 많은 성과를 기대할 수 있습니다.

## 참고 자료

- 논문 링크: [arXiv:2601.10639](https://arxiv.org/abs/2601.10639)
- 코드 저장소: [GitHub Repository](https://github.com/your-repo/STEM)
- 관련 자료: [Transformer Models](https://paperswithcode.com/methods/category/transformers)