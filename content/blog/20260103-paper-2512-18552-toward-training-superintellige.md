---
title: "[논문 리뷰] Toward Training Superintelligent Software Agents through Self-Play SWE-RL"
date: "2026-01-03"
excerpt: "While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull reque..."
category: "Paper Review"
tags: ["Paper Review","cs.SE","cs.AI","cs.CL"]
thumbnail: "/assets/images/blog/20260103-paper-2512-18552-toward-training-superintellige.jpg"
---

# [논문 리뷰] Toward Training Superintelligent Software Agents through Self-Play SWE-RL

## TL;DR

현재의 소프트웨어 에이전트는 대형 언어 모델(LLM)과 강화 학습(RL)을 활용하여 프로그래머의 생산성을 높이고 있지만, 인간의 지식이나 큐레이션에 크게 의존하고 있습니다. 이 논문은 Self-play SWE-RL(SSR)이라는 새로운 훈련 패러다임을 제안하여, 인간 개입 없이도 초지능 소프트웨어 에이전트를 훈련할 수 있는 방법을 탐구합니다. SSR은 샌드박스된 환경에서 에이전트가 스스로 버그를 주입하고 해결하는 과정을 통해 지속적인 자기 개선을 이루며, 인간 데이터 기반의 기준선을 능가하는 성능을 보입니다. 특히, SSR은 단순히 버그를 수정하는 것을 넘어, 코드의 효율성이나 가독성을 개선하는 방향으로도 학습이 가능하다는 점을 보여줍니다. 이러한 연구는 초지능 시스템 개발의 첫걸음을 내딛는 데 기여하며, 향후 연구에서는 더욱 복잡한 소프트웨어 개발 작업에의 적용 가능성을 탐색할 필요가 있습니다.

## 연구 배경 및 동기

소프트웨어 개발 분야에서는 대형 언어 모델(LLM)과 강화 학습(RL) 기술을 활용하여 프로그래머의 생산성을 향상시키려는 노력이 활발히 진행되고 있습니다. 그러나 이러한 접근법은 주로 GitHub 이슈, 풀 리퀘스트 등 인간이 생성한 데이터를 기반으로 하고 있으며, 이는 초지능 소프트웨어 에이전트의 개발에 근본적인 한계를 초래합니다. 예를 들어, 현재의 시스템은 인간의 큐레이션을 필요로 하며, 이는 에이전트가 자율적으로 학습하고 개선하는 데 장애물이 됩니다. 또한, 인간 데이터는 종종 편향되어 있거나 불완전할 수 있으며, 이는 에이전트의 학습 능력을 제한할 수 있습니다.

이 연구는 이러한 한계를 극복하고자, Self-play SWE-RL(SSR)이라는 새로운 훈련 패러다임을 제안합니다. SSR은 인간의 개입 없이도 에이전트가 스스로 학습할 수 있는 환경을 제공하며, 실제 소프트웨어 개발 환경을 모방하여 에이전트가 점진적으로 복잡한 문제를 해결할 수 있도록 합니다. 이 연구의 핵심 질문은 "어떻게 하면 소프트웨어 에이전트가 인간의 큐레이션 없이도 스스로 학습하고 개선할 수 있을까?"입니다. 이를 통해 궁극적으로는 인간의 능력을 초월하는 초지능 시스템을 개발하는 것을 목표로 합니다. SSR은 단순히 버그를 수정하는 것을 넘어, 코드 리팩토링, 성능 최적화 등 다양한 소프트웨어 개발 작업을 학습할 수 있는 잠재력을 가지고 있습니다.

## 관련 연구

기존 연구에서는 대형 언어 모델과 강화 학습을 활용하여 소프트웨어 개발을 자동화하려는 다양한 시도가 이루어졌습니다. 예를 들어, Codex와 같은 모델은 자연어 명령을 코드로 변환하는 데 사용되며, AlphaCode는 코딩 대회 문제를 해결하는 데 성공적인 결과를 보여주었습니다. 그러나 이러한 시스템들은 여전히 인간이 제공한 데이터에 크게 의존하고 있습니다.

1. **Codex**: 자연어를 코드로 변환하는 모델로, 코드 생성의 정확성을 높이는 데 기여했으나, 인간이 제공한 데이터에 크게 의존함.
2. **AlphaCode**: 코딩 대회 문제를 해결하는 모델로, 제한된 환경에서 우수한 성능을 보였으나, 다양한 소프트웨어 개발 시나리오에 적용하기에는 한계가 있음.
3. **GPT-3**: 다양한 자연어 처리 작업에 활용되는 대형 언어 모델로, 코드 생성에서도 사용되지만, 여전히 인간 데이터에 의존적임.
4. **DeepCoder**: 프로그램 합성을 위한 모델로, 코드 조각을 조합하여 문제를 해결하나, 복잡한 문제 해결에는 한계가 있음.
5. **RL-Coder**: 강화 학습을 활용한 코드 생성 모델로, 특정 문제 해결에 효과적이나, 일반화에는 어려움이 있음.

본 논문은 이러한 기존 연구와 달리, 인간의 큐레이션 없이도 에이전트가 스스로 학습할 수 있는 환경을 제공하는 SSR을 제안합니다. 이는 에이전트가 자율적으로 버그를 주입하고 해결하는 과정을 통해 지속적인 자기 개선을 이루며, 인간 데이터 기반의 기준선을 능가하는 성능을 보입니다. 특히, SSR은 기존 연구들이 다루지 못했던 '자기 개선'이라는 측면에서 중요한 진전을 이루었습니다.

| 연구 | 주요 기여 | 한계점 |
|---|---|---|
| Codex | 자연어를 코드로 변환 | 인간 데이터 의존 |
| AlphaCode | 코딩 대회 문제 해결 | 제한된 환경, 일반적인 소프트웨어 개발 적용 어려움 |
| GPT-3 | 다양한 자연어 처리 | 인간 데이터 의존, 코드 생성 능력 제한적 |
| DeepCoder | 프로그램 합성 | 복잡한 문제 해결 한계, 규모 있는 코드 생성 어려움 |
| RL-Coder | 강화 학습 기반 코드 생성 | 일반화 어려움, 특정 문제에만 효과적 |

## 핵심 기여

1. **Self-play SWE-RL(SSR) 제안**: 인간의 큐레이션 없이도 소프트웨어 에이전트를 훈련할 수 있는 새로운 패러다임을 제안하였습니다. 이는 샌드박스된 환경에서 에이전트가 스스로 버그를 주입하고 해결하는 과정을 통해 학습합니다.

2. **버그 주입 및 해결 에이전트**: LLM을 활용하여 버그를 주입하고 해결하는 과정에서 에이전트가 스스로 학습할 수 있도록 하는 방법론을 제시하였습니다. 이는 게임 AI에서 흔히 사용되는 self-play 개념을 소프트웨어 엔지니어링에 적용한 것입니다. 예를 들어, 버그 주입 에이전트는 `NullPointerException`을 발생시키는 코드를 삽입하거나, `if` 조건문을 잘못 설정하여 로직 에러를 유발할 수 있습니다.

3. **실험적 검증**: SWE-bench Verified와 SWE-Bench Pro 벤치마크에서 SSR의 성능을 검증하였으며, 인간 데이터 기반의 기준선보다 일관되게 우수한 성능을 보였습니다.

4. **자기 생성 과제의 효과성**: SSR은 인간이 예상하지 못한 새로운 유형의 버그를 스스로 생성하고 해결하는 능력을 보여주었으며, 자기 생성 과제가 인간 설계 데이터보다 더 풍부하고 효과적인 학습 신호를 제공함을 입증하였습니다. 예를 들어, SSR은 메모리 누수를 유발하는 코드를 스스로 생성하고, 이를 해결하는 방법을 학습할 수 있습니다.

## 제안 방법론

### 핵심 아이디어와 이론적 근거

Self-play SWE-RL(SSR)은 인간의 개입 없이 소프트웨어 에이전트를 훈련하기 위한 방법론입니다. 이는 샌드박스된 환경에서 에이전트가 스스로 버그를 주입하고 해결하는 과정을 통해 학습하며, 실제 소프트웨어 개발 환경을 모방합니다. SSR의 주요 개념은 self-play와 curriculum learning을 결합하여, 에이전트가 점진적으로 복잡한 문제를 해결할 수 있도록 하는 것입니다. 즉, 처음에는 간단한 버그부터 시작하여 점차 난이도를 높여가면서 에이전트의 능력을 향상시킵니다.

### 모델 아키텍처 상세 설명

SSR은 LLM을 기반으로 한 두 가지 에이전트, 즉 버그 주입 에이전트와 해결 에이전트로 구성됩니다. 버그 주입 에이전트는 의도적으로 코드에 결함을 삽입하고, 해결 에이전트는 이를 찾아 수정합니다. 이 과정에서 두 에이전트 모두 강화 학습을 통해 능력을 향상시킵니다. 버그는 코드 파일에 대한 패치, 테스트 스크립트, 테스트 파일, 테스트 파서 스크립트, 테스트 약화 패치로 구성된 아티팩트로 정의됩니다. 예를 들어, 버그 주입 에이전트는 특정 함수의 반환 값을 변경하거나, 예외 처리를 누락시키는 등의 방식으로 버그를 생성할 수 있습니다. 해결 에이전트는 이러한 버그를 찾아 수정하는 과정에서 코드 분석 능력, 디버깅 능력 등을 학습하게 됩니다.

### 핵심 수식

SSR의 강화 학습 과정에서 사용되는 보상 함수 $R(s)$는 다음과 같이 정의됩니다:

$$
R(s) = \begin{cases}
    -P, & \text{if } s = 0 \text{ or } s = 1 \\
    M \cdot s, & \text{otherwise}
\end{cases}
$$

여기서 $s$는 해결률을 나타내며, 0과 1 사이의 값을 가집니다. $P$는 페널티, $M$은 최대 보상을 나타냅니다. 이 보상 함수는 에이전트가 단순히 버그를 만들거나 해결하는 데 그치지 않고, 부분적으로 해결되는 버그를 통해 더 많은 것을 배우도록 장려합니다. 즉, 완벽하게 해결하지 못하더라도 부분적으로 해결한 경우에도 보상을 제공하여 학습을 촉진합니다.

또한, 챌린저-솔버 게임에서 챌린저의 최적 목표 해결률을 설정하는 수식은 이항 분포를 기반으로 하며, 챌린저가 최적의 기대 보상을 얻기 위한 목표 해결률을 계산합니다:

$$
E[\text{보상}] = f(p, q)
$$

여기서 $f(p, q)$는 챌린저와 솔버의 해결 확률에 따른 보상 함수를 나타냅니다. 챌린저는 이 $E[\text{보상}]$를 최대화하는 $p$ 값을 선택해야 합니다. 이 과정은 챌린저(버그 주입 에이전트)가 너무 쉽거나 어려운 버그를 생성하지 않도록 조정하는 역할을 합니다.

## 실험 설정

### 데이터셋

SSR은 SWE-bench Verified와 SWE-Bench Pro 벤치마크에서 평가되었습니다. SWE-bench Verified는 실제 소프트웨어 프로젝트에서 발생한 버그와 수정 사항을 포함하며, SWE-Bench Pro는 자동으로 생성된 테스트 케이스와 문제 설명을 제공합니다. SWE-bench Verified는 실제 코드 베이스의 버그를 기반으로 하기 때문에 현실적인 시나리오를 반영하지만, SWE-Bench Pro는 자동 생성된 데이터이기 때문에 다양성이 부족할 수 있습니다.

### 평가 지표

SSR의 성능은 해결률과 자기 개선 정도를 기준으로 평가되었습니다. 해결률은 에이전트가 주입된 버그를 얼마나 잘 해결했는지를 나타내며, 자기 개선 정도는 훈련 과정에서 에이전트의 성능이 얼마나 향상되었는지를 나타냅니다. 또한, 코드의 품질(가독성, 효율성)을 평가하기 위해 추가적인 지표를 사용할 수도 있습니다.

### 베이스라인

SSR의 성능은 인간 데이터 기반의 기준선과 비교되었습니다. 기준선 모델은 GitHub 이슈와 풀 리퀘스트 데이터를 사용하여 훈련된 에이전트입니다. 이 기준선 모델은 인간이 제공한 데이터를 기반으로 학습하기 때문에, SSR의 성능을 평가하는 데 중요한 비교 대상이 됩니다.

### 하이퍼파라미터

| 하이퍼파라미터 | 값 | 설명 |
|---|---|---|
| 학습률 | 0.001 | 모델 학습 속도 조절 |
| 배치 크기 | 32 | 한 번에 학습하는 데이터 양 |
| 최대 에포크 수 | 100 | 전체 데이터셋 반복 학습 횟수 |
| 보상 페널티 $P$ | 0.5 | 버그 생성/해결 실패 시 페널티 |
| 최대 보상 $M$ | 1.0 | 버그 부분 해결 시 최대 보상 |
| LLM 모델 | GPT-3.5 | 버그 생성 및 해결에 사용되는 LLM 모델 |

## 실험 결과 분석

### 주요 결과

SSR은 SWE-bench Verified와 SWE-Bench Pro 벤치마크에서 인간 데이터 기반의 기준선보다 일관되게 우수한 성능을 보였습니다. 특히, SWE-bench Verified에서 +10.4 포인트, SWE-Bench Pro에서 +7.8 포인트의 자기 개선을 이루었습니다. 이는 SSR이 인간의 개입 없이도 스스로 학습하고 개선할 수 있다는 것을 보여주는 중요한 결과입니다.

| 벤치마크 | 기준선 성능 | SSR 성능 | 성능 향상률(%) |
|---|---|---|---|
| SWE-bench Verified | 70.0 | 80.4 | 14.9% |
| SWE-Bench Pro | 65.0 | 72.8 | 12.0% |

### Ablation study 분석

Ablation study를 통해 SSR의 각 구성 요소가 성능에 미치는 영향을 분석하였습니다. 버그 주입 및 해결 에이전트, 강화 학습 보상 함수, self-play 설정 등을 제거하거나 수정하여 실험한 결과, 모든 구성 요소가 성능 향상에 기여함을 확인할 수 있었습니다. 특히, self-play 설정이 제거된 경우 성능이 크게 저하되는 것을 확인하여, self-play가 SSR의 핵심적인 요소임을 입증했습니다.

## 비판적 평가

### 강점

1. **혁신적인 훈련 패러다임**: 인간의 큐레이션 없이도 소프트웨어 에이전트를 훈련할 수 있는 새로운 방법론을 제시하였습니다.
2. **실험적 검증**: SWE-bench Verified와 SWE-Bench Pro 벤치마크에서 우수한 성능을 입증하였습니다.
3. **자기 생성 과제의 효과성**: 에이전트가 스스로 버그를 생성하고 해결하는 능력을 보여주었습니다.

### 한계점과 개선 방향

1. **복잡한 소프트웨어 작업 적용 한계**: SSR을 더욱 복잡한 소프트웨어 개발 작업에 적용하기 위한 연구가 필요합니다. 현재 SSR은 비교적 간단한 버그 수정에 초점을 맞추고 있지만, 코드 리팩토링, 성능 최적화, 새로운 기능 개발 등 보다 복잡한 작업에 대한 적용 가능성을 탐색해야 합니다.
2. **버그 주입 위치의 분포 제어**: 생성되는 버그의 난이도와 유형을 제어하여 훈련 효율성을 높이는 연구가 필요합니다. 현재는 버그 주입이 무작위로 이루어지기 때문에, 에이전트가 학습하기 어려운 버그가 생성될 수도 있습니다. 따라서, 에이전트의 현재 능력에 맞는 난이도의 버그를 생성하는 방법을 연구해야 합니다.
3. **LLM 의존성**: SSR은 LLM의 성능에 크게 의존합니다. 따라서, LLM의 성능 향상과 더불어, LLM 없이도 작동할 수 있는 SSR 버전을 개발하는 것도 고려해볼 필요가 있습니다.

### 재현성 평가

SSR의 실험은 공개된 데이터셋과 코드 저장소를 통해 재현 가능하며, 실험 설정과 하이퍼파라미터가 명확히 기술되어 있습니다. 하지만, LLM의 사용은 재현성에 영향을 미칠 수 있습니다. LLM은 지속적으로 업데이트되기 때문에, 특정 시점에 사용된 LLM의 버전과 동일한 환경을 구축하는 것이 어려울 수 있습니다.

## 향후 연구 방향

1. **복잡한 소프트웨어 개발 작업에의 적용**: SSR을 더욱 복잡한 소프트웨어 개발 작업에 적용하고, 실제 산업 환경에서의 활용 가능성을 탐색할 필요가 있습니다. 예를 들어, SSR을 사용하여 레거시 코드를 현대화하거나, 새로운 기능을 자동으로 개발하는 방법을 연구할 수 있습니다.
2. **장기적인 소프트웨어 에이전트 훈련**: 에이전트가 장기간에 걸쳐 지속적으로 학습할 수 있도록 하는 효율적인 패러다임 개발이 필요합니다. 현재 SSR은 비교적 짧은 기간 동안 훈련되지만, 실제 소프트웨어 개발은 장기간에 걸쳐 진행됩니다. 따라서, 에이전트가 장기간에 걸쳐 지속적으로 학습하고 개선할 수 있도록 하는 방법을 연구해야 합니다.
3. **버그 주입 전략 개선**: 생성되는 버그의 난이도와 유형을 제어하여 훈련 효율성을 높이는 연구가 필요합니다. 예를 들어, curriculum learning을 적용하여, 에이전트가 쉬운 버그부터 시작하여 점차 어려운 버그를 해결하도록 훈련할 수 있습니다.
4. **코드 품질 평가**: 단순히 버그를 수정하는 것뿐만 아니라, 코드의 품질(가독성, 효율성)을 개선하는 방향으로 학습할 수 있도록 하는 연구가 필요합니다. 코드 품질은 소프트웨어 유지보수에 중요한 영향을 미치기 때문에, 에이전트가 코드 품질을 개선하는 능력을 갖추는 것이 중요합니다.

## 실무 적용 가이드

1. **환경 설정**: SSR을 적용하기 위해서는 샌드박스된 환경에서의 훈련이 필요하며, 실제 소프트웨어 개발 환경을 모방하는 것이 중요합니다. Docker, Kubernetes와 같은 컨테이너 기술을 사용하여 샌드박스 환경을 구축할 수 있습니다.
2. **버그 주입 및 해결 전략**: 에이전트가 다양한 유형의 버그를 생성하고 해결할 수 있도록 다양한 버그 주입 및 해결 전략을 개발해야 합니다. 예를 들어, 퍼징(fuzzing) 기술을 사용하여 다양한 입력 값을 생성하고, 에이전트가 이러한 입력 값에 대한 예외 처리를 제대로 수행하는지 확인할 수 있습니다.
3. **강화 학습 보상 설계**: 에이전트의 학습 효율성을 높이기 위해 적절한 보상 함수를 설계해야 합니다. 보상 함수는 에이전트가 수행해야 할 작업을 명확하게 정의하고, 에이전트가 올바른 방향으로 학습하도록 유도해야 합니다.
4. **지속적인 모니터링 및 평가**: SSR을 적용한 후에는 에이전트의 성능을 지속적으로 모니터링하고 평가해야 합니다. 에이전트의 성능이 저하되는 경우, 학습 데이터를 수정하거나, 하이퍼파라미터를 조정하는 등의 조치를 취해야 합니다.

## 결론

이 논문은 Self-play SWE-RL(SSR)이라는 새로운 훈련 패러다임을 제안하여, 인간의 개입 없이도 소프트웨어 에이전트를 훈련할 수 있는 가능성을 제시하였습니다. SSR은 샌드박스된 환경에서 에이전트가 스스로 버그를 주입하고 해결하는 과정을 통해 지속적인 자기 개선을 이루며, 인간 데이터 기반의 기준선을 능가하는 성능을 보입니다. 이러한 연구는 초지능 시스템 개발의 첫걸음을 내딛는 데 기여하며, 향후 연구에서는 더욱 복잡한 소프트웨어 개발 작업에의 적용 가능성을 탐색할 필요가 있습니다. SSR은 소프트웨어 개발 자동화 분야에 혁신적인 변화를 가져올 수 있는 잠재력을 가지고 있으며, 앞으로의 연구가 더욱 기대됩니다.

## 참고 자료

- [논문 링크](https://arxiv.org/abs/2512.18552)
- [코드 저장소](https://github.com/SSR-Project)
- 관련 자료: SWE-bench Verified, SWE-Bench Pro 벤치마크 데이터셋.
- 관련 자료: Docker, Kubernetes (샌드박스 환경 구축 관련 기술)
- 관련 자료: 퍼징 (버그 주입 전략 관련 기술)