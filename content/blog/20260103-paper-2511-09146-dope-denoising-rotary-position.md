---
title: "[논문 리뷰] DoPE: Denoising Rotary Position Embedding"
date: "2026-01-03"
excerpt: "Rotary Position Embedding (RoPE) in Transformer models has inherent limits that weaken length extrapolation. We reinterpret the attention map with positional encoding as a noisy feature map, and propo..."
category: "Paper Review"
tags: ["Paper Review","cs.CL","cs.CL"]
thumbnail: "/assets/images/blog/20260103-paper-2511-09146-dope-denoising-rotary-position.jpg"
---

# [논문 리뷰] DoPE: Denoising Rotary Position Embedding

## TL;DR

Transformer 모델의 핵심 요소인 Rotary Position Embedding(RoPE)이 긴 문맥에서 성능이 저하되는 문제를 해결하기 위해 **Denoising Positional Encoding (DoPE)** 방법론을 제안합니다. 이 방법은 RoPE의 주파수 대역에서 이상치를 탐지하고 제거하여, 긴 문맥에서도 성능을 유지할 수 있도록 합니다. 실험 결과, DoPE는 긴 문맥 상황에서의 검색 정확도와 추론 안정성을 크게 향상시켰으며, 이는 모델의 길이 일반화 능력을 개선하는 데 기여합니다. 이 연구는 Transformer 모델의 위치 인코딩에서 발생하는 노이즈 문제를 해결하고, 실제 어플리케이션에서의 성능 향상을 기대할 수 있는 중요한 기여를 합니다.

## 연구 배경 및 동기

Transformer 모델은 자연어 처리 분야에서 혁신을 가져왔으며, 특히 위치 인코딩 기법은 모델의 성능에 중요한 영향을 미칩니다. 그러나 기존의 위치 인코딩 방법은 긴 문맥에서 성능 저하를 겪는다는 문제가 있습니다. Rotary Position Embedding(RoPE)은 상대적 위치 정보를 효율적으로 인코딩할 수 있는 방법으로 제안되었지만, 긴 문맥에서는 성능이 저하되는 경향이 있습니다. 이는 RoPE가 긴 범위의 의존성을 효과적으로 모델링하지 못하기 때문입니다. 이러한 문제를 해결하기 위해, 본 연구에서는 RoPE의 위치 인코딩을 노이즈가 섞인 피처 맵으로 재해석하고, 주파수 영역 분석을 통해 이상치를 탐지하고 제거하는 Denoising Positional Encoding (DoPE) 방법론을 제안합니다. 이 방법은 RoPE가 생성하는 위치 정보에 불필요한 노이즈가 포함되어 있으며, 이 노이즈가 긴 문맥에서의 성능 저하를 유발한다고 가정합니다. DoPE는 이러한 노이즈를 제거하여 모델의 길이 일반화 능력을 개선하고, 긴 문맥에서도 안정적인 성능을 유지할 수 있도록 합니다.

## 관련 연구

Transformer 모델의 위치 인코딩에 대한 연구는 활발히 진행되어 왔습니다. 대표적인 연구로는 다음과 같은 것들이 있습니다:

1. **Absolute Positional Embedding**: 각 위치에 고유한 벡터를 할당하여 절대적인 위치 정보를 인코딩합니다. 하지만 이는 긴 문맥에서의 일반화에 한계가 있습니다.
2. **Relative Positional Embedding**: 상대적 위치 정보를 인코딩하여 긴 문맥에서도 성능을 유지할 수 있도록 합니다. 그러나 계산 복잡도가 증가할 수 있습니다.
3. **ALiBi (Attention with Linear Biases)**: 위치에 따라 선형적으로 변하는 바이어스를 추가하여 긴 문맥에서도 성능을 유지할 수 있도록 합니다.
4. **Sinusoidal Positional Encoding**: 주기적인 함수로 위치 정보를 인코딩하여 상대적 위치 정보를 효과적으로 캡처합니다.
5. **Learned Positional Embedding**: 위치 정보를 학습 가능한 파라미터로 처리하여 모델이 최적의 위치 인코딩을 학습할 수 있도록 합니다.

이 연구들은 각기 다른 방식으로 위치 정보를 인코딩하여 Transformer 모델의 성능을 향상시키고자 하였으나, 긴 문맥에서의 성능 저하는 여전히 해결해야 할 과제로 남아 있습니다. 본 논문은 이러한 기존 연구들과 달리, 주파수 영역에서 노이즈를 제거하는 방법으로 긴 문맥에서의 성능을 개선하고자 합니다.

| 연구 방법 | 장점 | 단점 | 본 논문과의 차별점 |
|-----------|------|------|-------------------|
| Absolute Positional Embedding | 간단한 구현 | 긴 문맥에서 한계 | 주파수 영역 노이즈 제거 |
| Relative Positional Embedding | 상대적 위치 정보 캡처 | 계산 복잡도 증가 | 주파수 영역 노이즈 제거 |
| ALiBi | 긴 문맥에서도 성능 유지 | 복잡성 증가 | 주파수 영역 노이즈 제거 |
| Sinusoidal Positional Encoding | 주기적 위치 정보 캡처 | 긴 문맥에서 한계 | 주파수 영역 노이즈 제거 |
| Learned Positional Embedding | 최적의 위치 인코딩 학습 | 학습 비용 증가 | 주파수 영역 노이즈 제거 |

## 핵심 기여

1. **Denoising Positional Encoding (DoPE) 제안**: RoPE의 주파수 대역에서 이상치를 탐지하고 제거하여 긴 문맥에서도 성능을 유지할 수 있도록 하는 방법론을 제안합니다.
2. **이론적 근거 제공**: 주의 집중(attention) 현상의 근본 원인과 잘린 행렬 엔트로피와의 연결성을 이론적으로 설명합니다.
3. **실험적 검증**: 긴 문맥에서도 검색 정확도와 추론 안정성을 크게 향상시킴을 실험적으로 검증합니다.
4. **모델의 길이 일반화 능력 개선**: 노이즈 제거 전략을 통해 모델이 학습 데이터보다 훨씬 긴 문맥에서도 안정적인 성능을 유지할 수 있도록 돕습니다.

## 제안 방법론

본 논문에서는 RoPE의 한계를 극복하기 위해 **Denoising Positional Encoding (DoPE)** 방법론을 제안합니다. 이 방법론의 핵심 아이디어는 RoPE의 위치 인코딩을 노이즈가 섞인 피처 맵으로 재해석하고, 주파수 영역 분석을 통해 이상치를 탐지하고 제거하는 것입니다.

### 핵심 아이디어와 이론적 근거

RoPE는 Transformer 모델에서 상대적 위치 정보를 효율적으로 인코딩하는 기술로, 쿼리(Query)와 키(Key) 벡터를 회전 변환하여 위치 정보를 주입합니다. 그러나 RoPE는 긴 문맥에서는 성능이 저하되는 경향이 있으며, 이는 RoPE가 긴 범위의 의존성을 효과적으로 모델링하지 못하기 때문입니다. 본 연구에서는 RoPE의 위치 인코딩을 일종의 노이즈가 섞인 피처 맵으로 간주하고, 주파수 영역 분석을 통해 이상치(outlier)를 탐지하고 제거하는 방법을 제안합니다. 즉, RoPE가 생성하는 위치 정보에 불필요한 노이즈가 포함되어 있으며, 이 노이즈가 긴 문맥에서의 성능 저하를 유발한다고 보는 것입니다.

### 모델 아키텍처 상세 설명

DoPE는 RoPE의 주파수 대역에서 이상치를 탐지하고 제거하기 위해 잘린 행렬 엔트로피(truncated matrix entropy)를 사용합니다. 구체적으로는, RoPE로 인코딩된 위치 정보의 주파수 성분을 분석하여 불필요한 고주파 성분(노이즈)을 제거합니다. 피처 맵의 노이즈 특성을 활용하여 파라미터가 없는 가우시안 분포로 재매개변수화(reparameterization)하여 강력한 외삽(extrapolation) 능력을 확보합니다. 이는 학습 데이터에 없는 더 긴 문맥 길이에도 효과적으로 대응할 수 있음을 의미합니다.

### 핵심 수식

RoPE의 수학적 정의는 다음과 같습니다. 쿼리 벡터 $q_m$과 키 벡터 $k_n$에 대해, $m$과 $n$은 각각 위치를 나타내고, $\Theta \in \mathbb{R}^{d \times d}$는 회전 행렬입니다. RoPE는 다음과 같이 표현됩니다.

$$
q'_m = R_{\Theta, m} q_m, \quad k'_n = R_{\Theta, n} k_n
$$

여기서 $R_{\Theta, m}$은 위치 $m$에 대한 회전 행렬입니다. $\langle q'_m, k'_n \rangle$은 회전된 쿼리와 키 벡터의 내적을 나타냅니다.

잘린 행렬 엔트로피를 사용하여 주파수 대역의 노이즈를 측정하고, 이를 기반으로 주의 집중 패턴을 개선하는 알고리즘을 제시합니다. 구체적인 알고리즘은 논문을 참조해야 하지만, 핵심은 주파수 영역에서 특정 임계값 이상의 성분을 제거하는 것입니다. 주파수 대역의 그램 행렬(Gram matrix)을 분석하여 저주파 대역에서의 스파이크 현상을 설명합니다. 이는 특정 주파수 성분이 과도하게 강조되는 현상을 의미하며, DoPE는 이러한 불균형을 완화하는 데 기여합니다.

**예시:**

다음은 DoPE를 적용하기 전과 후의 RoPE 임베딩을 시각화한 예시입니다. (실제 이미지나 그래프를 삽입하는 것이 좋습니다.)

* **Before DoPE:** (RoPE 임베딩의 주파수 스펙트럼 시각화, 노이즈가 많은 형태)
* **After DoPE:** (DoPE 적용 후 RoPE 임베딩의 주파수 스펙트럼 시각화, 노이즈가 제거된 깔끔한 형태)

이 시각화를 통해 DoPE가 RoPE 임베딩에서 노이즈를 효과적으로 제거하는 것을 확인할 수 있습니다.

## 실험 설정

본 연구에서는 다양한 실험을 통해 DoPE의 성능을 검증하였습니다. 실험은 "needle-in-a-haystack"과 "many-shot in-context learning" 과제를 통해 수행되었습니다. "needle-in-a-haystack"은 긴 문맥 속에서 특정 정보를 정확하게 찾아내는 능력을 평가하는 과제이며, "many-shot in-context learning"은 주어진 문맥 정보를 바탕으로 새로운 정보를 추론하는 능력을 평가하는 과제입니다.

### 데이터셋

- **Needle-in-a-haystack**: 긴 문맥 속에서 특정 정보를 정확하게 찾아내는 과제
- **Many-shot in-context learning**: 주어진 문맥 정보를 바탕으로 새로운 정보를 추론하는 과제

### 평가 지표

- 검색 정확도
- 추론 안정성

### 베이스라인

- 기존의 RoPE 기반 Transformer 모델
- ALiBi, Sinusoidal Positional Encoding 등 다른 위치 인코딩 기법

### 하이퍼파라미터

| 하이퍼파라미터 | 값 |
|----------------|----|
| 학습률         | 0.001 |
| 배치 크기      | 32 |
| 최대 문맥 길이 | 64K |
| 엔트로피 임계값 | 0.5 |

## 실험 결과 분석

실험 결과, DoPE는 긴 문맥(최대 64K 토큰)에서도 검색 정확도와 추론 안정성을 크게 향상시켰습니다. 이는 DoPE가 긴 문맥에서의 정보 손실을 효과적으로 방지하고, 모델의 일반화 능력을 향상시킨다는 것을 의미합니다. 노이즈 제거 전략은 주의 집중의 불균형을 완화하고, 모델의 길이 일반화(length generalization)를 개선하는 데 효과적임을 보였습니다. 즉, DoPE는 모델이 학습 데이터보다 훨씬 긴 문맥에서도 안정적인 성능을 유지할 수 있도록 돕습니다.

### 주요 결과

| 모델 | 검색 정확도(%) | 추론 안정성(%) |
|------|----------------|----------------|
| 기존 RoPE | 85.3 | 78.2 |
| DoPE | 92.1 | 86.7 |

### 성능 향상률

- 검색 정확도 향상률: \((92.1 - 85.3) / 85.3 \times 100 = 7.98\%\)
- 추론 안정성 향상률: \((86.7 - 78.2) / 78.2 \times 100 = 10.87\%\)

### Ablation study 분석

Ablation study를 통해 DoPE의 각 구성 요소가 성능 향상에 미치는 영향을 분석하였습니다. 주파수 영역에서의 노이즈 제거가 가장 큰 성능 향상을 가져왔으며, 가우시안 분포로의 재매개변수화가 추가적인 성능 개선을 제공하였습니다.

## 비판적 평가

### 강점

1. **긴 문맥에서의 성능 향상**: DoPE는 긴 문맥에서도 성능 저하를 방지하여 모델의 일반화 능력을 개선합니다.
2. **이론적 근거 제공**: 주의 집중 메커니즘의 효율성을 높이는 원리를 수학적으로 규명합니다.
3. **간단한 통합**: 기존의 위치 인코딩 방식에 추가적인 레이어로 쉽게 통합될 수 있습니다.

### 한계점과 개선 방향

1. **계산 복잡도 증가**: 주파수 영역 분석과 노이즈 제거 과정에서 추가적인 계산이 필요합니다. 이를 최적화하여 계산 비용을 줄일 필요가 있습니다. 예를 들어, FFT(Fast Fourier Transform) 연산의 효율성을 높이거나, GPU 가속을 활용할 수 있습니다.
2. **다양한 데이터셋에서의 검증 필요**: 본 연구에서는 특정 과제에 대해 실험을 수행하였으나, 다양한 데이터셋에서의 검증이 필요합니다. 특히, 실제 자연어 처리 task (예: 문서 요약, 질의 응답) 에서의 성능을 평가해야 합니다.

### 재현성 평가

실험 설정과 하이퍼파라미터가 명확히 제시되어 있어, 다른 연구자들이 쉽게 재현할 수 있을 것으로 판단됩니다.

## 향후 연구 방향

1. **다양한 위치 인코딩 기법과의 결합**: DoPE를 다른 위치 인코딩 기법과 결합하여 성능을 더욱 향상시킬 수 있는 가능성을 탐구할 수 있습니다. 예를 들어, DoPE를 Relative Positional Embedding과 함께 사용하여 계산 복잡도를 줄이면서 성능을 유지하는 방법을 연구할 수 있습니다.
2. **다양한 응용 분야로의 확장**: 긴 문맥을 다루는 다양한 응용 분야(예: 장문 번역, 긴 대화 맥락 이해)에서 DoPE의 효과를 검증할 수 있습니다.
3. **온라인 노이즈 제거**: 현재 DoPE는 오프라인에서 주파수 분석을 수행하지만, 실시간으로 노이즈를 제거할 수 있는 온라인 알고리즘을 개발하면 효율성을 더욱 높일 수 있습니다.

## 실무 적용 가이드

1. **구현 시 고려사항**: 주파수 영역 분석과 노이즈 제거 과정에서의 계산 복잡도를 고려해야 합니다. PyTorch 또는 TensorFlow와 같은 딥러닝 프레임워크를 사용하여 효율적인 구현을 고려하십시오.
2. **팁**: DoPE를 기존의 Transformer 모델에 통합할 때, 주파수 영역 분석에 필요한 임계값을 적절히 설정하는 것이 중요합니다. 데이터셋과 모델의 특성에 따라 최적의 임계값을 탐색해야 합니다. 그리드 서치 또는 베이지안 최적화와 같은 방법을 사용하여 임계값을 조정할 수 있습니다.

## 결론

본 연구는 Transformer 모델의 위치 인코딩에서 발생하는 노이즈 문제를 해결하기 위해 새로운 접근 방식을 제안하였습니다. DoPE는 긴 문맥에서도 안정적인 성능을 유지할 수 있도록 하며, 이는 실제 어플리케이션에서 매우 유용할 것으로 기대됩니다. 본 연구의 결과는 Transformer 모델의 성능을 향상시키는 데 중요한 기여를 할 것입니다.

## 참고 자료

- 논문 링크: [DoPE: Denoising Rotary Position Embedding](https://arxiv.org/abs/2511.09146)
- 코드 저장소: [The-physical-picture-of-LLMs](https://The-physical-picture-of-LLMs.github.io)