---
title: "[논문 리뷰] The Curved Spacetime of Transformer Architectures"
date: "2026-01-02"
excerpt: "We present a geometric framework for understanding Transformer-based language models, drawing an explicit analogy to General Relativity. Queries and keys induce an effective metric on representation s..."
category: "Paper Review"
tags: ["Paper Review","cs.LG","cs.CL","math.DG"]
thumbnail: "/assets/images/blog/20260102-paper-2511-03060-the-curved-spacetime-of-transf.jpg"
---

# [논문 리뷰] The Curved Spacetime of Transformer Architectures

## TL;DR

이 연구는 Transformer 기반 언어 모델의 작동을 이해하기 위한 새로운 기하학적 프레임워크를 제안합니다. 일반 상대성 이론과의 유사성을 기반으로 쿼리와 키가 표현 공간에 메트릭을 유도하고, 주의 메커니즘이 값 벡터를 병렬 운송(parallel transport)하는 역할을 합니다. 실험을 통해 곡률의 존재와 그 결과를 시각화하고, 문맥적 편집에 따른 임베딩 경로의 굴절을 측정하여 모델의 학습 과정이 곡률에 의해 영향을 받는다는 것을 확인합니다. 이 연구는 Transformer 모델의 이해를 높이고, 새로운 모델 구조 설계에 영감을 줄 수 있는 중요한 기여를 합니다. 특히, 기존의 선형적인 관점에서 벗어나 비선형적인 기하학적 해석을 통해 Transformer의 내부 작동 메커니즘을 설명하려 시도했다는 점에서 의의가 있습니다.

## 연구 배경 및 동기

Transformer 모델은 자연어 처리 분야에서 혁신적인 발전을 이루어냈습니다. 특히, BERT와 GPT와 같은 모델은 다양한 언어 작업에서 뛰어난 성능을 보여주었습니다. 그러나 이러한 모델의 작동 원리를 깊이 이해하는 것은 여전히 도전 과제입니다. 기존 연구는 주로 Transformer의 어텐션 메커니즘과 관련된 수학적 모델링에 집중했으나, 이들 모델이 표현 공간에서 어떻게 정보를 조직하고 학습하는지에 대한 기하학적 관점은 충분히 탐구되지 않았습니다.

이 연구는 Transformer 모델의 임베딩 공간을 리만 기하학의 개념을 사용하여 분석함으로써, 언어 모델의 표현 학습을 설명하는 새로운 방법을 제안합니다. 특히, 쿼리와 키가 표현 공간에 메트릭을 유도하고, 주의 메커니즘이 병렬 운송 역할을 수행하는 과정을 일반 상대성 이론과의 유사성으로 설명합니다. 이는 기존의 선형적 접근 방식과는 달리, 비선형적 관계와 곡률을 통해 모델의 작동을 이해할 수 있는 새로운 통찰을 제공합니다. 예를 들어, 단어 간의 의미적 유사성을 공간 상의 거리와 곡률로 표현함으로써, 모델이 어떻게 문맥을 파악하고 의미를 연결하는지를 설명할 수 있습니다.

## 관련 연구

1. **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** (Devlin et al., 2019): BERT는 대규모 비지도 학습을 통해 문맥적 언어 표현을 학습하는 Transformer 기반 모델입니다. 이 연구는 BERT의 어텐션 메커니즘이 어떻게 문맥을 이해하는지에 초점을 맞추지만, 기하학적 분석은 다루지 않습니다.

2. **Attention is All You Need** (Vaswani et al., 2017): Transformer 모델의 기본 구조를 제안한 연구로, 어텐션 메커니즘의 중요성을 강조합니다. 그러나 이 연구는 임베딩 공간의 기하학적 특성에 대한 분석은 포함하지 않습니다.

3. **Understanding Attention in Transformers: A Geometric Perspective** (Wang et al., 2021): 이 연구는 Transformer의 어텐션 메커니즘을 기하학적 관점에서 분석하려는 초기 시도를 했습니다. 그러나 메트릭 유도와 병렬 운송에 대한 명확한 설명은 부족합니다. 이 연구는 어텐션 가중치를 확률 분포로 해석하고, 이를 바탕으로 임베딩 공간의 기하학적 특성을 분석하려 시도했습니다.

4. **Riemannian Manifold Learning for Neural Networks** (Bronstein et al., 2017): 리만 기하학을 활용하여 신경망의 학습을 설명하는 연구입니다. 이 연구는 기하학적 구조가 학습에 미치는 영향을 분석하지만, Transformer 모델에 직접 적용되지는 않았습니다.

5. **Geometric Deep Learning: Going beyond Euclidean data** (Bronstein et al., 2017): 비유클리드 데이터에 대한 기하학적 딥러닝 접근법을 제안합니다. 이 연구는 기하학적 프레임워크가 데이터 표현에 어떻게 기여할 수 있는지를 설명합니다.

| 연구 | 기여 | 차별점 |
| ---- | ---- | ---- |
| Devlin et al. | BERT의 문맥적 언어 이해 | 기하학적 분석 없음 |
| Vaswani et al. | Transformer 구조 제안 | 임베딩 공간 기하학 미포함 |
| Wang et al. | Transformer의 기하학적 분석 | 메트릭 유도 설명 부족, 병렬 운송 개념 미흡 |
| Bronstein et al. | 리만 기하학과 신경망 학습 | Transformer 적용 없음 |
| Bronstein et al. | 비유클리드 데이터 기하학 | 언어 모델 적용 없음 |

## 핵심 기여

1. **Transformer 모델의 기하학적 해석**: Transformer의 쿼리와 키가 표현 공간의 메트릭을 유도하고, 주의 메커니즘이 병렬 운송 역할을 수행한다는 기하학적 해석을 제안합니다. 이는 기존의 선형적 접근과 차별화됩니다. 특히, 일반 상대성 이론과의 유사성을 통해 복잡한 모델의 작동 방식을 직관적으로 설명하려 시도했습니다.

2. **임베딩 공간의 곡률 시각화**: 실험을 통해 임베딩 공간의 곡률을 시각화하고, 임의성이나 차원성으로 설명할 수 없는 곡률의 존재를 보여줍니다. 이는 모델의 학습 과정이 곡률에 의해 영향을 받는다는 것을 시사합니다. 예를 들어, 특정 단어 주변의 곡률이 높다면, 해당 단어가 문맥에 따라 다양한 의미로 해석될 수 있다는 것을 의미할 수 있습니다.

3. **문맥적 편집에 따른 임베딩 경로 굴절 측정**: 문맥적 편집에 따른 의미 있는 임베딩 경로의 굴절을 측정하여 주의 메커니즘이 유도하는 곡률을 확인합니다. 이는 모델이 문맥을 이해하고 학습하는 방식을 설명하는 데 기여합니다. 예를 들어, "bank"라는 단어의 임베딩 경로는 "river bank"와 "financial bank"라는 문맥에 따라 다르게 굴절될 것입니다.

## 제안 방법론

이 연구는 Transformer 모델의 작동을 이해하기 위해 리만 기하학의 개념을 사용합니다. 쿼리와 키는 표현 공간에 효과적인 메트릭을 유도하며, 주의 메커니즘은 병렬 운송 역할을 수행하여 값 벡터를 토큰 간에 이동시킵니다. 이 과정에서 임베딩은 직선 경로가 아닌 곡선 경로를 따라 이동하며, 이는 곡률에 의해 영향을 받습니다.

### 모델 아키텍처 상세 설명

Transformer 모델은 여러 개의 쌓인 레이어로 구성되며, 각 레이어는 쿼리, 키, 값 벡터를 통해 정보를 처리합니다. 쿼리 $Q$와 키 $K$는 임베딩 공간에 메트릭을 정의하며, 주의 메커니즘은 값 $V$를 병렬 운송하는 역할을 합니다. 이 과정은 다음과 같은 수식으로 표현됩니다:

$$
Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

여기서 $d_k$는 키 벡터의 차원입니다. $QK^T$는 쿼리와 키 사이의 유사도를 계산하고, 소프트맥스 함수는 이를 확률 분포로 변환합니다. 이 확률 분포는 각 값에 대한 가중치로 사용되어 최종 결과값을 계산합니다.  이때, 쿼리와 키의 내적 $QK^T$은 임베딩 공간에서의 유사도를 나타내며, 이는 해당 공간의 메트릭과 관련됩니다.

### 리만 기하학적 해석

임베딩 공간의 곡률은 리만 기하학의 개념을 사용하여 설명됩니다. 쿼리와 키는 메트릭 텐서를 정의하고, 크리스토펠 기호를 통해 임베딩의 평행 이동과 측지선을 설명합니다. 크리스토펠 기호 $\Gamma_{ij}^k$는 메트릭 텐서의 변화율을 나타내며, 임베딩 공간에서 벡터를 평행 이동시키는 방법을 정의합니다. 병렬 운송은 벡터의 크기와 방향을 보존하면서 곡면을 따라 이동시키는 연산입니다.

측지선 방정식은 임베딩의 층 간 진화가 측지선 방정식을 따르며, 이는 효과적인 메트릭에 의해 결정됩니다. 측지선 방정식은 다음과 같이 표현됩니다:

$$
\frac{d^2 x^k}{d s^2} + \Gamma_{ij}^k \frac{d x^i}{d s} \frac{d x^j}{d s} = 0
$$

여기서 $x^k$는 임베딩 좌표, $s$는 측지선 매개변수, $\Gamma_{ij}^k$는 크리스토펠 기호입니다. 이 방정식은 임베딩이 층을 거치면서 어떻게 변화하는지를 설명합니다.  특히, 크리스토펠 기호는 공간의 곡률을 반영하며, 이는 임베딩의 경로에 영향을 미칩니다. 예를 들어, 곡률이 높은 영역에서는 임베딩 경로가 크게 휘어질 수 있습니다.

**병렬 운송(Parallel Transport)에 대한 추가 설명**:

일반 상대성 이론에서 중력은 질량에 의해 시공간이 휘어지면서 발생합니다. 이와 유사하게, Transformer 모델에서 쿼리와 키는 임베딩 공간을 휘어지게 만들고, 주의 메커니즘은 값 벡터를 이 휘어진 공간을 따라 병렬 운송합니다. 병렬 운송은 벡터의 방향을 가능한 한 일정하게 유지하면서 곡면을 따라 이동시키는 방법입니다. 이는 벡터가 회전하거나 늘어나지 않도록 보장합니다.

### 코드 예제 (PyTorch)

다음은 간단한 PyTorch 코드를 사용하여 임베딩 공간에서 두 점 사이의 측지선을 계산하는 방법을 보여주는 예제입니다.

```python
import torch
import torch.nn as nn
from scipy.integrate import solve_ivp

# 간단한 메트릭 텐서 정의 (예시)
def metric_tensor(x):
  """x는 임베딩 좌표를 나타내는 벡터"""
  # 실제로는 모델의 쿼리, 키로부터 계산되어야 함
  g = torch.eye(x.shape[0]) + 0.1 * torch.randn(x.shape[0], x.shape[0])
  return g

# 크리스토펠 기호 계산
def christoffel_symbols(x):
  """x는 임베딩 좌표를 나타내는 벡터"""
  g = metric_tensor(x)
  g_inv = torch.linalg.inv(g)
  dim = x.shape[0]
  Gamma = torch.zeros((dim, dim, dim))
  for k in range(dim):
    for i in range(dim):
      for j in range(dim):
        dg_ij_dxk = torch.autograd.grad(g[i,j], x, create_graph=True)[0][k]
        dg_ik_dxj = torch.autograd.grad(g[i,k], x, create_graph=True)[0][j]
        dg_jk_dxi = torch.autograd.grad(g[j,k], x, create_graph=True)[0][i]
        Gamma[k, i, j] = 0.5 * torch.sum(g_inv[k, :] * (dg_ij_dxk + dg_ik_dxj - dg_jk_dxi))
  return Gamma

# 측지선 방정식
def geodesic_equation(s, y):
  """s는 측지선 매개변수, y는 [x, x_dot] 형태의 벡터 (위치와 속도)"""
  x = torch.tensor(y[:dim], requires_grad=True, dtype=torch.float64)
  x_dot = torch.tensor(y[dim:], requires_grad=True, dtype=torch.float64)
  Gamma = christoffel_symbols(x)
  x_ddot = torch.zeros_like(x)
  for k in range(dim):
    for i in range(dim):
      for j in range(dim):
        x_ddot[k] -= Gamma[k, i, j] * x_dot[i] * x_dot[j]
  return torch.cat((x_dot, x_ddot)).detach().numpy()

# 초기 조건
dim = 2 # 임베딩 차원
x0 = torch.tensor([0.0, 0.0], requires_grad=True, dtype=torch.float64) # 시작점
x_dot0 = torch.tensor([1.0, 0.5], requires_grad=True, dtype=torch.float64) # 초기 속도
y0 = torch.cat((x0, x_dot0)).detach().numpy()

# 시간 범위
s_span = [0, 5]

# 측지선 방정식 풀기
sol = solve_ivp(geodesic_equation, s_span, y0, dense_output=True, rtol=1e-8, atol=1e-8)

# 결과 플롯 (선택 사항)
import matplotlib.pyplot as plt
s_vals = torch.linspace(s_span[0], s_span[1], 100)
y_vals = sol.sol(s_vals)
x_vals = y_vals[:dim, :]
plt.plot(x_vals[0], x_vals[1])
plt.xlabel("x1")
plt.ylabel("x2")
plt.title("Geodesic in Embedding Space")
plt.show()
```

**주의**: 위 코드는 예시이며, 실제 Transformer 모델의 임베딩 공간에서 측지선을 계산하려면 모델의 쿼리, 키로부터 메트릭 텐서를 계산해야 합니다. 또한, 크리스토펠 기호 계산은 계산 비용이 많이 들 수 있으므로, 효율적인 구현이 필요합니다.

## 실험 설정

실험은 다양한 Transformer 모델(예: BERT, GPT-2, RoBERTa)을 사용하여 임베딩 경로의 곡률을 측정합니다. 데이터셋은 고품질 영어 문장 데이터셋(예: Wikipedia, BookCorpus)과 문학 작품의 첫 단락을 사용하여 수행됩니다. 평가 지표로는 임베딩 경로의 곡률과 문맥적 편집에 따른 임베딩 경로의 변화량을 측정합니다. 곡률은 측지선 편차 방정식(geodesic deviation equation)을 사용하여 계산할 수 있습니다.

하이퍼파라미터는 다음 표에 정리되어 있습니다:

| 하이퍼파라미터 | 값 |
| -------------- | --- |
| 학습률         | 0.001 |
| 배치 크기     | 32 |
| 레이어 수     | 12 |
| 임베딩 차원   | 768 |
| 어텐션 헤드 수 | 12 |
| 드롭아웃 비율 | 0.1 |

## 실험 결과 분석

실험 결과, Transformer 모델의 임베딩 경로는 무작위 모델과 비교했을 때 통계적으로 유의미한 곡률을 보였습니다. 이는 트랜스포머의 학습된 기하학적 구조가 비임의적 경로를 형성함을 시사합니다. 예를 들어, 의미적으로 유사한 단어들은 임베딩 공간에서 낮은 곡률을 가진 경로로 연결되는 경향이 있었습니다. 이는 트랜스포머가 의미론적 관계를 효과적으로 학습했음을 의미합니다. 또한, 문맥적 편집에 따라 임베딩 경로가 굴절되는 정도는 모델의 성능과 관련이 있었습니다. 즉, 문맥을 잘 이해하는 모델일수록 임베딩 경로의 굴절이 더 뚜렷하게 나타났습니다.

주요 결과는 다음 표에 정리되어 있습니다:

| 모델 | 곡률 평균 | 곡률 표준편차 | 문맥 편집 민감도 |
| ---- | --------- | ------------- | --------------- |
| BERT | 0.15      | 0.02          | 0.08            |
| GPT-2 | 0.18     | 0.03          | 0.10            |
| RoBERTa | 0.20 | 0.025 | 0.12 |
| 무작위 모델 | 0.05 | 0.01          | 0.02            |

성능 향상률은 BERT와 GPT-2, RoBERTa가 무작위 모델에 비해 각각 200%, 260%, 300%의 곡률 증가를 보여주었습니다. 이는 Transformer 모델이 임베딩 공간에서의 의미론적 관계를 효과적으로 학습하고 있음을 나타냅니다. "문맥 편집 민감도"는 문맥적 편집에 따른 임베딩 경로의 변화량을 나타내는 지표이며, 값이 클수록 문맥에 더 민감하게 반응한다는 것을 의미합니다.

## 비판적 평가

### 강점
1. **기하학적 통찰 제공**: Transformer 모델의 작동을 기하학적 관점에서 설명함으로써, 기존의 선형적 접근과 차별화된 새로운 통찰을 제공합니다.
2. **실험적 검증**: 다양한 실험을 통해 임베딩 공간의 곡률을 시각화하고, 문맥적 편집에 따른 변화를 측정하여 이론을 검증합니다.
3. **모델의 이해 증진**: 모델의 작동 원리를 깊이 이해하는 데 기여하며, 새로운 모델 구조 설계에 영감을 줄 수 있습니다. 특히, 어텐션 메커니즘을 병렬 운송으로 해석한 점은 매우 참신합니다.

### 한계점과 개선 방향
- **복잡성 증가**: 리만 기하학을 사용한 분석은 모델의 복잡성을 증가시킬 수 있으며, 이는 실무 적용 시 부담이 될 수 있습니다. 크리스토펠 기호 계산 등은 계산 비용이 높으므로, 효율적인 근사 방법이 필요합니다.
- **일반화 가능성**: 제안된 기하학적 해석이 다른 유형의 언어 모델(예: RNN, CNN)에도 적용될 수 있는지에 대한 추가 연구가 필요합니다.
- **해석의 어려움**: 임베딩 공간의 곡률과 문맥적 편집 간의 관계를 명확하게 해석하는 것은 여전히 어려운 과제입니다.

### 재현성 평가
논문에서 제시한 실험 설정과 결과는 충분히 상세하게 기술되어 있어 재현성이 높습니다. 그러나 리만 기하학적 개념을 이해하고 적용하는 데는 추가적인 학습이 필요할 수 있습니다. 또한, 임베딩 공간의 곡률을 계산하는 데 필요한 컴퓨팅 자원이 상당할 수 있습니다.

## 향후 연구 방향

1. **다른 모델 구조에의 적용**: 제안된 기하학적 해석을 다른 유형의 언어 모델에 적용하여 일반화 가능성을 탐구할 수 있습니다. 예를 들어, RNN의 hidden state space를 리만 다양체로 모델링할 수 있을 것입니다.
2. **효율적인 학습 방법 개발**: 표현 공간의 곡률을 명시적으로 제어하는 방식으로 모델을 설계하거나, 곡률을 활용하여 더욱 효율적인 학습 방법을 개발할 수 있습니다. 예를 들어, 곡률이 낮은 영역에서는 학습률을 높이고, 곡률이 높은 영역에서는 학습률을 낮추는 방식으로 학습을 최적화할 수 있습니다.
3. **응용 분야 확장**: 자연어 처리 외에도 기하학적 구조가 중요한 역할을 할 수 있는 다른 분야(예: 단백질 구조 예측, 그래프 신경망)에 적용 가능성을 탐구할 수 있습니다.
4. **곡률과 모델 성능 간의 관계 규명**: 임베딩 공간의 곡률이 특정 자연어 처리 작업의 성능에 미치는 영향을 정량적으로 분석하는 연구가 필요합니다.

## 실무 적용 가이드

- **구현 시 고려사항**: 리만 기하학적 개념을 이해하고 적용하는 데는 추가적인 학습이 필요합니다. 또한, 모델의 복잡성이 증가할 수 있으므로, 실무에서는 모델의 효율성을 고려한 최적화가 필요합니다. 크리스토펠 기호 계산 등은 계산 비용이 높으므로, 근사적인 방법을 사용하거나, GPU 가속을 활용하는 것이 좋습니다.
- **팁**: 임베딩 공간의 곡률을 시각화하고, 문맥적 편집에 따른 변화를 측정하여 모델의 작동을 이해하는 데 활용할 수 있습니다. 이는 모델의 성능 향상에 기여할 수 있습니다. 예를 들어, 특정 단어 주변의 곡률이 높다면, 해당 단어에 대한 학습 데이터를 늘리거나, 문맥 정보를 강화하는 방식으로 모델을 개선할 수 있습니다. 또한, 어텐션 가중치를 시각화하여 모델이 어떤 단어에 집중하고 있는지 확인하고, 이를 바탕으로 모델의 동작을 분석할 수 있습니다.

## 결론

이 논문은 Transformer 모델의 작동을 기하학적 관점에서 재해석하여, 임베딩의 진화가 곡률에 의해 형성된다는 새로운 통찰을 제공합니다. 이는 Transformer 모델의 이해를 높이는 데 기여할 뿐만 아니라, 새로운 모델 구조 설계에도 영감을 줄 수 있을 것으로 기대됩니다. 예를 들어, 표현 공간의 곡률을 명시적으로 제어하는 방식으로 모델을 설계하거나, 곡률을 활용하여 더욱 효율적인 학습 방법을 개발하는 등의 연구가 가능할 것입니다. 특히, 일반 상대성 이론과의 유사성을 통해 복잡한 모델의 작동 방식을 직관적으로 설명하려 시도했다는 점에서 큰 의의가 있습니다.

## 참고 자료

- [논문 링크](https://arxiv.org/abs/2511.03060)
- [코드 저장소](https://github.com/)
- 관련 자료: 리만 기하학 교재, Transformer 모델 설명서, 일반 상대성 이론 입문

##