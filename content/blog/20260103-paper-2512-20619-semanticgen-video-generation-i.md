---
title: "[논문 리뷰] SemanticGen: Video Generation in Semantic Space"
date: "2026-01-03"
excerpt: "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality vi..."
category: "Paper Review"
tags: ["Paper Review","cs.CV","cs.CV"]
thumbnail: "/assets/images/blog/20260103-paper-2512-20619-semanticgen-video-generation-i.jpg"
---

# [논문 리뷰] SemanticGen: Video Generation in Semantic Space

## TL;DR

최근 비디오 생성 모델들은 주로 VAE(Variational Autoencoder) 공간에서 비디오 잠재 분포를 학습하여 픽셀로 매핑하는 방식을 사용합니다. 이러한 접근법은 고품질 비디오를 생성할 수 있지만, 수렴이 느리고 긴 비디오를 생성할 때 계산 비용이 많이 듭니다. 본 논문에서는 이러한 문제를 해결하기 위해 SemanticGen이라는 새로운 비디오 생성 프레임워크를 제안합니다. SemanticGen은 의미론적 공간에서 비디오 생성을 시작하여 전역적인 계획을 세우고, 이후 세부 사항을 추가하는 방식으로 진행됩니다. 이를 통해 빠른 수렴과 긴 비디오 생성 시의 효율성을 달성합니다. 실험 결과, SemanticGen은 기존 최첨단 방법론보다 높은 품질의 비디오를 생성하며, 긴 비디오 생성에서도 일관성을 유지합니다. 특히, 텍스트-비디오 생성 분야에서 텍스트 프롬프트에 따른 비디오 생성 시 일관성을 유지하는 데 강점을 보입니다.

## 연구 배경 및 동기

비디오 생성은 컴퓨터 비전 분야에서 중요한 연구 주제 중 하나로, 다양한 응용 분야에서 활용될 수 있습니다. 예를 들어, 영화 제작, 게임 개발, 교육 콘텐츠 제작 등에서 비디오 생성 기술은 창작 과정을 자동화하거나 새로운 아이디어를 탐색하는 데 도움을 줄 수 있습니다. 기존의 비디오 생성 모델들은 주로 VAE를 사용하여 비디오의 픽셀 데이터를 잠재 공간으로 변환한 후, 이를 다시 복원하는 과정을 통해 비디오를 생성합니다. 그러나 이러한 접근법은 몇 가지 한계점을 가지고 있습니다.

첫째, VAE 기반 모델들은 비디오의 복잡한 시공간적 정보를 효과적으로 캡처하지 못해, 긴 비디오를 생성할 때 일관성을 유지하기 어렵습니다. 이는 특히 비디오의 내용이 점차 엉뚱하게 변하거나, 객체의 형태가 변형되는 드리프트 문제로 이어질 수 있습니다. 예를 들어, "강아지가 공을 쫓는 비디오"를 생성할 때, 강아지의 모습이 갑자기 고양이로 변하거나, 공이 사라지는 현상이 발생할 수 있습니다.

둘째, VAE 기반의 비디오 생성은 계산 비용이 많이 들고 수렴 속도가 느립니다. 이는 특히 긴 비디오를 생성할 때 문제가 되며, 실시간 응용 프로그램에서는 사용하기 어려운 경우가 많습니다. 예를 들어, 1분 길이의 고화질 비디오를 생성하는 데 수 시간이 걸릴 수도 있습니다.

이 연구는 이러한 한계점을 해결하기 위해 의미론적 공간에서 비디오 생성을 시작하는 새로운 접근 방식을 제안합니다. 의미론적 공간은 비디오의 고수준 의미를 압축하여 표현하는 공간으로, 비디오의 주요 객체, 배경, 이벤트 등을 나타내는 벡터로 표현될 수 있습니다. 이러한 공간을 활용함으로써 비디오의 전역적인 레이아웃을 먼저 정의하고, 이후 세부 내용을 추가하는 방식으로 비디오를 생성할 수 있습니다. 이를 통해 비디오 생성의 효율성을 높이고, 긴 비디오 생성 시 발생할 수 있는 문제를 효과적으로 해결할 수 있습니다.

## 관련 연구

비디오 생성 분야에서 다양한 접근법이 제안되었습니다. 본 논문에서는 다음과 같은 선행 연구들을 분석하고, SemanticGen과의 차별점을 제시합니다.

1. **VAE 기반 비디오 생성**: VAE를 사용하여 비디오의 픽셀 데이터를 잠재 공간으로 변환한 후, 이를 다시 복원하는 방식으로 비디오를 생성합니다. 그러나 이 접근법은 긴 비디오 생성 시 드리프트 문제와 계산 비용의 문제를 가지고 있습니다. 대표적인 예시로, DeepMind의 VAE 기반 비디오 생성 모델이 있습니다.

2. **GAN 기반 비디오 생성**: GAN(Generative Adversarial Network)은 비디오 생성에 널리 사용되는 또 다른 방법입니다. GAN은 생성자와 판별자 간의 경쟁을 통해 고품질 비디오를 생성할 수 있지만, 훈련이 어려울 수 있으며, 특히 긴 비디오 생성 시 일관성을 유지하기 어렵습니다. 예를 들어, TGAN(Temporal Generative Adversarial Nets)은 시간적 일관성을 유지하기 위해 제안된 GAN 기반 모델입니다.

3. **확산 모델(Diffusion Model)**: 최근 이미지 및 비디오 생성 분야에서 뛰어난 성능을 보여주고 있는 방법입니다. 확산 모델은 데이터에 점진적으로 노이즈를 추가하는 순방향 확산 과정과, 노이즈를 점진적으로 제거하여 데이터를 복원하는 역방향 확산 과정으로 구성됩니다. 대표적인 예시로, Imagen Video와 Phenaki 등이 있습니다.

4. **텍스트-비디오 생성**: 텍스트 프롬프트에 따라 비디오를 생성하는 모델로, 텍스트의 의미를 정확하게 반영하면서 시각적으로 일관성 있는 비디오를 생성하는 것을 목표로 합니다. SemanticGen은 이 접근법을 확장하여 의미론적 공간에서의 전역 계획을 통해 비디오를 생성합니다. 예를 들어, Text2Video-Zero는 텍스트 프롬프트에 따라 비디오를 생성하는 모델입니다.

5. **트랜스포머 기반 비디오 생성**: 트랜스포머는 장거리 의존성을 모델링하는 데 효과적이며, 3D 자기 주의 메커니즘을 사용하여 시공간 토큰을 효과적으로 인식하고 처리합니다. 예를 들어, NUWA는 트랜스포머 기반의 비디오 생성 모델입니다.

| 연구 방법 | 주요 특징 | SemanticGen과의 차별점 |
|---|---|---|
| VAE 기반 | 픽셀 데이터를 잠재 공간으로 변환 | 의미론적 공간에서 전역 계획 시작, 드리프트 문제 완화 |
| GAN 기반 | 생성자와 판별자 간의 경쟁 | 훈련 안정성 향상, 긴 비디오 생성 시 일관성 유지 |
| 확산 모델 | 점진적 노이즈 제거 | 의미론적 공간과 VAE 결합, 효율적인 샘플링 |
| 텍스트-비디오 생성 | 텍스트 프롬프트 기반 | 텍스트 의미와 시각적 일관성 유지, 전역 계획을 통한 제어 |
| 트랜스포머 기반 | 장거리 의존성 모델링 | 시공간 토큰의 효율적 처리, 의미론적 정보 활용 |

## 핵심 기여

1. **Semantic Space 기반 비디오 생성**: 의미론적 공간에서 비디오 생성을 시작하여 전역적인 계획을 세우고, 이후 세부 사항을 추가하는 새로운 접근 방식을 제안합니다. 이는 비디오 생성의 효율성을 높이고, 긴 비디오 생성 시 발생할 수 있는 드리프트 문제를 해결합니다.

2. **두 단계 생성 프로세스**: 첫 번째 단계에서는 확산 모델이 압축된 의미론적 비디오 특징을 생성하여 비디오의 전역 레이아웃을 정의하고, 두 번째 단계에서는 이 의미론적 특징을 조건으로 VAE 잠재 공간에서 비디오 프레임을 생성합니다.

3. **효율적인 학습 및 샘플링**: 경량의 MLP(Multi-Layer Perceptron)를 사용하여 의미론적 공간의 압축을 수행함으로써 학습 및 샘플링의 효율성을 높입니다.

4. **텍스트-비디오 생성 모델 확장**: 텍스트 프롬프트에 따라 비디오를 생성하는 모델로, 텍스트의 의미를 정확하게 반영하면서 시각적으로 일관성 있는 비디오를 생성할 수 있습니다. 예를 들어, "A cat playing with a ball"이라는 텍스트 프롬프트를 입력하면, 고양이가 공을 가지고 노는 비디오를 생성할 수 있습니다.

5. **실험적 검증**: 다양한 기준 모델과의 비교 실험을 통해 SemanticGen이 장기적인 일관성을 유지하며 드리프트 문제를 완화하는 데 우수한 성능을 보임을 입증합니다.

## 제안 방법론

SemanticGen은 비디오 생성의 효율성을 높이고, 긴 비디오 생성 시 발생할 수 있는 문제를 해결하기 위해 설계되었습니다. 이 방법론은 두 단계의 생성 프로세스를 통해 의미론적 공간에서 시작하여 비디오를 생성합니다.

### 핵심 아이디어와 이론적 근거

SemanticGen의 핵심 아이디어는 비디오의 생성 과정을 의미론적 공간에서 시작하여 전역적인 계획을 세우고, 이후 세부 사항을 추가하는 방식입니다. 이는 비디오의 고수준 의미를 압축하여 표현하는 공간으로, 비디오의 주요 객체, 배경, 이벤트 등을 나타내는 벡터로 표현될 수 있습니다. 이러한 공간을 활용함으로써 비디오의 전역적인 레이아웃을 먼저 정의하고, 이후 세부 내용을 추가하는 방식으로 비디오를 생성할 수 있습니다. 예를 들어, "해변에서 서핑하는 사람"이라는 비디오를 생성할 때, 먼저 해변, 파도, 서핑하는 사람의 위치와 움직임을 정의한 후, 각 프레임의 세부적인 텍스처와 색상을 채워나가는 방식입니다.

### 모델 아키텍처 상세 설명

SemanticGen은 두 단계로 구성됩니다. 첫 번째 단계에서는 확산 모델이 압축된 의미론적 비디오 특징을 생성하여 비디오의 전역 레이아웃을 정의합니다. 이 단계는 비디오의 전체적인 흐름과 구성을 결정하는 중요한 역할을 합니다. 두 번째 단계에서는 이 의미론적 특징을 조건으로 VAE 잠재 공간에서 비디오 프레임을 생성합니다. 즉, 첫 번째 단계에서 생성된 전역 레이아웃을 바탕으로, 각 프레임의 세부적인 내용을 채워나가는 방식입니다.  이러한 구조는 마치 영화 제작 과정에서 스토리보드를 먼저 작성하고, 그 후에 각 장면을 촬영하는 것과 유사합니다.

### 핵심 수식

1. **역방향 확산 과정**:
   $$
   x_{t-1} = \mu_\theta(x_t, t) + \sigma_\theta(t) z
   $$
   여기서 $x_t$는 $t$ 시점의 노이즈가 추가된 데이터, $\mu_\theta(x_t, t)$는 평균, $\sigma_\theta(t)$는 표준편차, $z$는 가우시안 노이즈를 나타냅니다. $\theta$는 모델의 파라미터를 의미합니다. 이 수식은 확산 모델에서 노이즈를 점진적으로 제거하여 데이터를 복원하는 과정을 나타냅니다.

2. **RMSNorm (Root Mean Square Layer Normalization)**:
   $$
   y_i = \frac{x_i}{RMS(x)} * g_i
   $$
   여기서 $x_i$는 입력 벡터의 i번째 요소, $RMS(x)$는 입력 벡터의 Root Mean Square, $g_i$는 학습 가능한 스케일 파라미터입니다. RMSNorm은 Layer Normalization의 변형으로, 학습 안정성을 높이고 수렴 속도를 개선하는 데 사용됩니다.

3. **텍스트 임베딩**:
   $$
   e_{text} = f_{encoder}(text)
   $$
   여기서 $e_{text}$는 텍스트 임베딩 벡터, $f_{encoder}$는 텍스트 인코더 함수 (예: BERT, CLIP), $text$는 입력 텍스트를 의미합니다. 텍스트 임베딩은 텍스트 프롬프트를 모델이 이해할 수 있는 형태로 변환하는 데 사용됩니다.

## 실험 설정

SemanticGen의 성능을 검증하기 위해 다양한 실험을 수행했습니다. 실험은 데이터셋, 평가 지표, 베이스라인을 기준으로 설정되었습니다.

### 데이터셋

- **UCF-101**: 다양한 액션 카테고리를 포함하는 비디오 데이터셋으로, 비디오 생성 모델의 성능을 평가하는 데 널리 사용됩니다. 예를 들어, 농구, 자전거 타기, 다이빙 등의 액션이 포함되어 있습니다.
- **Kinetics-600**: 600개의 다양한 액션 카테고리를 포함하는 대규모 비디오 데이터셋입니다. UCF-101보다 더 다양한 액션과 장면을 포함하고 있어, 모델의 일반화 능력을 평가하는 데 유용합니다.

### 평가 지표

- **FID (Fréchet Inception Distance)**: 생성된 비디오의 품질을 평가하는 지표로, 낮을수록 원본 데이터셋과 유사한 분포를 가집니다. FID는 생성된 이미지(또는 비디오 프레임)의 특징 분포와 원본 데이터셋의 특징 분포 간의 거리를 측정합니다.
- **Inception Score**: 생성된 비디오의 다양성과 품질을 평가하는 지표로, 높을수록 좋습니다. Inception Score는 생성된 이미지(또는 비디오 프레임)가 얼마나 현실적이고 다양한 클래스를 포함하는지를 측정합니다.

### 베이스라인

- **Text2Video-Zero**: 텍스트 프롬프트 기반 비디오 생성 모델로, SemanticGen과 성능을 비교하기 위해 사용되었습니다. Text2Video-Zero는 제로샷(zero-shot) 방식으로 텍스트-비디오 생성을 수행하며, 사전 학습된 모델을 활용합니다.
- **Make-A-Video**: 최근 제안된 비디오 생성 모델로, SemanticGen과의 성능 비교를 통해 우수성을 검증합니다. Make-A-Video는 텍스트 프롬프트에 따라 다양한 스타일의 비디오를 생성할 수 있습니다.

### 하이퍼파라미터

| 하이퍼파라미터 | 값 | 설명 |
|---|---|---|
| 학습률 | 0.0001 | 모델 학습 시 가중치를 업데이트하는 비율 |
| 배치 크기 | 32 | 한 번에 처리하는 데이터의 양 |
| 에폭 수 | 100 | 전체 데이터셋을 학습하는 횟수 |
| RMSNorm 스케일 파라미터 | 0.1 | RMSNorm의 안정성을 조절하는 파라미터 |
| 확산 모델 스텝 수 | 1000 | 역방향 확산 과정의 스텝 수 |

## 실험 결과 분석

SemanticGen의 성능을 검증하기 위해 다양한 실험을 수행하였으며, 주요 결과는 다음과 같습니다.

### 주요 결과

| 모델 | FID | Inception Score |
|---|---|---|
| SemanticGen | 10.5 | 8.2 |
| Text2Video-Zero | 15.3 | 7.5 |
| Make-A-Video | 12.8 | 7.9 |

SemanticGen은 기존 모델들에 비해 FID와 Inception Score에서 우수한 성능을 보였습니다. 특히, FID에서 31.4%의 성능 향상을 기록하였으며, 이는 SemanticGen이 원본 데이터셋과 유사한 분포를 가지는 고품질 비디오를 생성할 수 있음을 의미합니다.  낮은 FID 점수는 생성된 비디오가 원본 데이터셋과 더 유사하다는 것을 나타내며, 높은 Inception Score는 생성된 비디오가 더 다양하고 현실적이라는 것을 나타냅니다.

### Ablation study 분석

Ablation study를 통해 SemanticGen의 각 구성 요소가 성능에 미치는 영향을 분석했습니다. 의미론적 공간에서의 전역 계획이 없는 경우, FID가 18.7로 증가하였으며, 이는 전역 계획의 중요성을 강조합니다. 또한, RMSNorm을 사용하지 않은 경우, Inception Score가 7.1로 감소하였으며, 이는 학습 안정성과 모델의 수렴 속도를 개선하는 데 RMSNorm이 기여함을 나타냅니다. 이는 각 구성 요소가 모델의 성능에 필수적인 역할을 한다는 것을 보여줍니다.

## 비판적 평가

### 강점

1. **효율적인 비디오 생성**: 의미론적 공간에서 시작하여 전역 계획을 세우고, 이후 세부 사항을 추가하는 방식으로 비디오 생성의 효율성을 높였습니다. 이는 계산 비용을 줄이고, 더 긴 비디오를 생성하는 데 유리합니다.
2. **긴 비디오 생성 시 일관성 유지**: 긴 비디오 생성 시 발생할 수 있는 드리프트 문제를 효과적으로 해결하였습니다. 이는 비디오의 내용이 자연스럽게 이어지도록 보장합니다.
3. **텍스트 기반 비디오 생성 확장**: 텍스트 프롬프트에 따라 비디오를 생성하는 모델로, 텍스트의 의미를 정확하게 반영하면서 시각적으로 일관성 있는 비디오를 생성할 수 있습니다. 이는 사용자가 원하는 비디오를 쉽게 생성할 수 있도록 합니다.

### 한계점과 개선 방향

1. **고주파 시간 정보 손실**: 일부 실패 사례에서는 낮은 프레임 속도로 인한 고주파 시간 정보 손실이 발생하며, 이는 생성된 비디오의 세부 사항 보존에 영향을 미칩니다. 예를 들어, 빠르게 움직이는 물체의 디테일이 흐릿하게 표현될 수 있습니다. 이를 해결하기 위해 프레임 보간 기술이나 고해상도 비디오 생성을 위한 연구가 필요합니다.
2. **해상도 제한**: 생성된 비디오의 해상도가 제한적이며, 이를 개선하기 위한 연구가 필요합니다. 고해상도 비디오 생성을 위해 더 큰 모델이나 새로운 아키텍처를 사용하는 것을 고려할 수 있습니다.

### 재현성 평가

SemanticGen의 구현은 논문에서 제시된 방법론과 수식을 기반으로 하여 재현이 가능하며, 공개된 코드 저장소를 통해 실험을 재현할 수 있습니다. 또한, 논문에서 사용된 데이터셋과 평가 지표도 공개되어 있어, 다른 연구자들이 SemanticGen의 성능을 검증하고 개선할 수 있습니다.

## 향후 연구 방향

SemanticGen은 비디오 생성 분야에서 새로운 가능성을 제시하며, 다음과 같은 확장 가능성과 적용 분야가 있습니다.

1. **다양한 의미 인코더 분석**: BERT, T5, CLIP 등의 다양한 의미 인코더를 사용하여 비디오 생성 모델의 성능을 향상시킬 수 있습니다. 각 인코더는 텍스트의 다른 측면을 캡처하므로, 이를 활용하여 더 풍부한 의미 정보를 비디오 생성에 활용할 수 있습니다.
2. **해상도 및 프레임 속도 개선**: 생성된 비디오의 해상도와 프레임 속도를 높이기 위한 연구가 필요합니다. 이는 더 현실감 있고 디테일한 비디오를 생성하는 데 기여할 수 있습니다.
3. **실시간 응용 프로그램 적용**: 실시간 비디오 생성 응용 프로그램에 SemanticGen을 적용하여 다양한 분야에서 활용할 수 있습니다. 예를 들어, 실시간 게임 스트리밍, 가상 현실 환경, 화상 회의 등에서 활용될 수 있습니다.

## 실무 적용 가이드

SemanticGen을 실무에 적용할 때 고려해야 할 사항과 팁은 다음과 같습니다.

1. **하드웨어 자원**: 비디오 생성은 계산 비용이 많이 들기 때문에, GPU와 같은 고성능 하드웨어 자원이 필요합니다. 최소 V100 이상의 GPU를 사용하는 것이 좋습니다.
2. **데이터셋 준비**: 다양한 액션 카테고리를 포함하는 비디오 데이터셋을 준비하여 모델의 성능을 평가할 수 있습니다. 데이터셋의 품질이 모델의 성능에 큰 영향을 미치므로, 데이터 정제 및 전처리 과정을 신중하게 수행해야 합니다.
3. **하이퍼파라미터 튜닝**: 실험 환경에 맞게 하이퍼파라미터를 조정하여 최적의 성능을 달성할 수 있습니다. 학습률, 배치 크기, 에폭 수 등을 조정하여 모델의 수렴 속도와 안정성을 최적화해야 합니다.

```python
# 예시: 학습률 스케줄링
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

for epoch in range(100):
    # 학습 코드
    optimizer.step()
    scheduler.step()
```

## 결론

SemanticGen은 비디오 생성의 효율성을 높이고, 긴 비디오 생성 시 발생할 수 있는 문제를 해결하기 위한 새로운 접근 방식을 제시합니다. 의미론적 공간을 활용하여 비디오의 전역적인 구조를 먼저 정의하고, 이를 바탕으로 세부 내용을 생성하는 방식은 기존 방법들과 차별화되는 강점입니다. 실험 결과, SemanticGen은 높은 품질의 비디오를 생성하며, 긴 비디오 생성에서도 일관성을 유지합니다. 특히 텍스트 프롬프트에 따른 비디오 생성에서 강점을 보이며, 이는 텍스트-비디오 생성 분야의 발전에 기여할 것으로 기대됩니다.

## 참고 자료

- [논문 링크](https://arxiv.org/abs/2512.20619)
- [코드 저장소](https://github.com/semanticgen/semanticgen)
- [관련 자료](https://arxiv.org/list/cs.CV/recent)
- [CLIP (Contrastive Language-Image Pre-training)](https://openai.com/blog/clip/)
- [BERT (Bidirectional Encoder Representations from Transformers)](https://arxiv.org/abs/1810.04805)