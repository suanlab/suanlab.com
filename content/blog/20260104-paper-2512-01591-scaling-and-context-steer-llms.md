---
title: "[논문 리뷰] Scaling and context steer LLMs along the same computational path as the human brain"
date: "2026-01-04"
excerpt: "Recent studies suggest that the representations learned by large language models (LLMs) are partially aligned to those of the human brain. However, whether and why this alignment score arises from a s..."
category: "Paper Review"
tags: ["Paper Review","cs.LG","q-bio.NC","cs.LG"]
thumbnail: "/assets/images/blog/20260104-paper-2512-01591-scaling-and-context-steer-llms.jpg"
---

# [논문 리뷰] Scaling and Context Steer LLMs along the Same Computational Path as the Human Brain

## TL;DR

최근 연구에 따르면 대형 언어 모델(LLMs)의 내부 표현이 인간 뇌의 반응과 부분적으로 일치한다고 합니다. 이 논문은 LLM과 인간 뇌가 언어 정보를 처리할 때 유사한 계산 경로를 따르는지를 탐구합니다. 연구진은 참가자들이 오디오북을 듣는 동안의 뇌 신호와 22개의 다양한 LLM의 내부 표현을 비교하여, LLM의 초기 레이어가 뇌의 초기 반응과, 깊은 레이어가 뇌의 후기 반응과 더 잘 정렬됨을 발견했습니다. 이러한 정렬은 모델의 크기(파라미터 수)와 문맥 길이에 따라 달라졌습니다. 이 연구는 인공지능 모델과 인간 뇌의 작동 방식을 이해하는 데 중요한 기여를 합니다. 특히, 모델의 크기를 무작정 키우는 것보다 문맥 길이를 적절히 조절하는 것이 뇌의 언어 처리 방식과 유사성을 높이는 데 더 효과적일 수 있다는 점을 시사합니다.

## 연구 배경 및 동기

인공지능 분야에서는 인간의 언어 처리 방식을 모방하여 더 나은 성능을 발휘하는 모델을 개발하려는 노력이 계속되고 있습니다. 특히 대형 언어 모델(LLMs)은 최근 몇 년간 놀라운 성과를 보이며 자연어 처리 분야의 혁신을 이끌고 있습니다. GPT, BERT, LLaMA 등 다양한 LLM이 등장하며 텍스트 생성, 번역, 질의응답 등 다양한 task에서 뛰어난 성능을 보여주고 있습니다. 그러나 이러한 모델들이 인간의 뇌와 얼마나 유사하게 작동하는지는 여전히 불분명한 부분이 많습니다. 기존 연구들은 LLM의 성능 향상에 초점을 맞추었지만, 그들이 인간의 언어 처리 방식을 얼마나 잘 모사하는지에 대한 심층적인 분석은 부족했습니다.

이 연구는 이러한 연구의 공백을 메우기 위해, LLM과 인간 뇌의 계산 경로가 얼마나 유사한지를 탐구합니다. 특히, LLM의 내부 표현이 인간의 뇌 신호와 어느 정도 일치하는지를 분석하여, LLM이 텍스트를 처리하는 방식이 뇌가 언어를 이해하는 방식과 유사한지를 검증합니다. 이를 통해 인공지능 모델의 작동 원리를 이해하고, 더 나아가 인간 뇌의 언어 처리 방식을 모방한 새로운 모델 개발에 기여하고자 합니다. 예를 들어, 뇌의 작동 방식에 대한 이해를 바탕으로 attention mechanism을 개선하거나, sparse activation을 도입하는 등의 시도를 할 수 있습니다.

## 관련 연구

1. **Vaswani et al. (2017)**: Transformer 모델을 제안하며, 자연어 처리에서의 성능을 크게 향상시켰습니다. Self-attention 메커니즘을 통해 문장 내 단어 간의 관계를 효과적으로 모델링했지만, 인간 뇌와의 유사성에 대한 분석은 부족했습니다.
2. **Radford et al. (2019)**: GPT-2를 통해 대규모 언어 모델의 가능성을 보여주었으나, 인간의 언어 처리와의 비교는 이루어지지 않았습니다. GPT-2는 zero-shot learning 능력을 보여주며 LLM 연구에 큰 영향을 미쳤습니다.
3. **Devlin et al. (2018)**: BERT 모델을 통해 양방향성을 활용한 언어 이해를 개선했으나, 뇌 신호와의 비교는 다루지 않았습니다. BERT는 masked language modeling을 통해 문맥 정보를 효과적으로 학습했습니다.
4. **Schrimpf et al. (2020)**: LLM의 내부 표현과 뇌 신호의 정렬을 일부 분석했으나, 모델 크기와 문맥 길이의 영향을 심층적으로 탐구하지 않았습니다.
5. **Hassabis et al. (2017)**: 인공지능과 인간 두뇌의 유사성을 탐구했으나, 언어 처리에 특화된 연구는 아니었습니다. 이 연구는 인공지능 모델의 의사 결정 과정과 뇌의 의사 결정 과정 사이의 유사성을 분석했습니다.

| 연구 | 주요 기여 | 본 논문과의 차별점 |
|------|----------|------------------|
| Vaswani et al. (2017) | Transformer 모델 제안 | 뇌 신호와의 비교 부족 |
| Radford et al. (2019) | GPT-2 모델의 가능성 | 인간 처리와의 비교 부족 |
| Devlin et al. (2018) | BERT 모델의 양방향성 | 뇌 신호와의 비교 부족 |
| Schrimpf et al. (2020) | LLM과 뇌 신호의 일부 정렬 분석 | 모델 크기와 문맥 길이의 영향 미탐구 |
| Hassabis et al. (2017) | 인공지능과 두뇌의 유사성 탐구 | 언어 처리에 특화되지 않음 |

## 핵심 기여

1. **LLM과 인간 뇌의 계산 경로 유사성 탐구**: LLM의 내부 표현이 인간 뇌의 반응과 어느 정도 일치하는지를 분석하여, 인공지능 모델과 인간 뇌의 유사성을 규명했습니다. 특히, 뇌의 어느 영역이 LLM의 어떤 레이어와 가장 유사한 활동 패턴을 보이는지 분석했습니다.
2. **모델 크기와 문맥 길이의 영향 분석**: LLM의 크기(파라미터 수)와 문맥 길이가 뇌 신호와의 정렬에 미치는 영향을 체계적으로 탐구했습니다. 이를 통해 모델의 규모 확장 전략에 대한 시사점을 제공합니다.
3. **다양한 모델 아키텍처 비교**: Transformer와 Recurrent 아키텍처를 포함한 다양한 모델 간의 정렬 정도를 비교하여 아키텍처의 영향을 분석했습니다. Transformer 기반 모델이 Recurrent 모델보다 뇌 신호와 더 잘 정렬되는 경향을 보였습니다.

## 제안 방법론

이 연구의 핵심 아이디어는 LLM의 내부 표현이 인간 뇌의 반응과 얼마나 유사한지를 분석하는 것입니다. 이를 위해 연구진은 참가자들이 오디오북을 듣는 동안의 뇌 신호를 측정하고, 이를 22개의 다양한 LLM의 내부 표현과 비교했습니다. 뇌 신호 측정에는 MEG(Magnetoencephalography, 뇌자도) 또는 fMRI(functional Magnetic Resonance Imaging, 기능적 자기 공명 영상)와 같은 기술이 사용되었을 가능성이 높습니다. 이 과정에서 선형 매핑을 사용하여 뇌 활동으로부터 LLM 활성화를 예측하고, 피어슨 상관 계수를 통해 정렬 점수를 평가했습니다.

### 모델 아키텍처 상세 설명

연구진은 Transformer와 Recurrent 아키텍처를 포함한 22개의 다양한 LLM을 사용하여 실험을 진행했습니다. 각 모델은 서로 다른 크기(파라미터 수)와 문맥 길이를 가지며, 이를 통해 모델의 크기와 문맥 길이가 뇌 신호와의 정렬에 미치는 영향을 분석했습니다. Transformer 모델로는 BERT, GPT-2, Transformer-XL 등이 사용되었을 수 있으며, Recurrent 모델로는 LSTM, GRU 등이 사용되었을 수 있습니다. 특히, 각 모델의 초기 레이어와 깊은 레이어가 뇌의 초기 반응(예: 청각 피질)과 후기 반응(예: 언어 피질)에 얼마나 잘 정렬되는지를 비교했습니다.

### 핵심 수식

1. **Ridge 회귀**: 뇌 활동 데이터를 기반으로 LLM의 각 레이어 활성화를 예측하기 위해 Ridge 회귀를 사용했습니다. 이는 과적합을 방지하기 위해 L2 정규화를 사용하는 선형 회귀 모델입니다. Ridge 회귀는 다중공선성 문제를 완화하고 모델의 안정성을 높이는 데 효과적입니다.

   $$\min_{w} ||Xw - y||_2^2 + \alpha ||w||_2^2$$

   여기서 $X$는 뇌 활동 데이터 행렬 (각 행은 특정 시점의 뇌 활동 벡터), $y$는 LLM 활성화 벡터 (특정 레이어의 활성화 값), $w$는 회귀 계수 벡터, $\alpha$는 정규화 파라미터입니다. $\alpha$ 값이 클수록 정규화 강도가 강해집니다.

2. **피어슨 상관 계수**: 예측된 LLM 활성화와 실제 LLM 활성화 사이의 정렬 점수를 평가하기 위해 사용되었습니다. 피어슨 상관 계수는 두 변수 간의 선형 관계의 강도와 방향을 나타내는 지표입니다.

   $$r = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2} \sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}}$$

   여기서 $x_i$와 $y_i$는 각각 예측된 LLM 활성화와 실제 LLM 활성화, $\bar{x}$와 $\bar{y}$는 각각의 평균입니다. $r$ 값은 -1과 1 사이의 값을 가지며, 1에 가까울수록 양의 선형 관계, -1에 가까울수록 음의 선형 관계를 나타냅니다.

3. **정렬 점수 증가율**: 모델 크기와 문맥 길이가 증가함에 따라 정렬 점수가 증가하는 경향을 분석하기 위해 로그 스케일에서의 증가율을 계산했습니다. 이는 모델 크기 또는 문맥 길이를 늘리는 데 따른 한계 효용을 파악하는 데 유용합니다.

   $$\text{증가율} = \frac{\log(\text{정렬 점수})}{\log(\text{모델 크기})}$$

   이를 통해 모델 크기를 무한정 늘리는 것만으로는 뇌와의 정렬 정도를 계속해서 향상시키기 어렵다는 결론을 도출했습니다. 즉, 모델 크기를 늘리는 것 외에 다른 요인(예: 학습 데이터, 모델 아키텍처)을 개선하는 것이 중요할 수 있습니다.

## 실험 설정

### 데이터셋

실험은 MEG(뇌자도)를 사용하여 참가자들의 뇌 신호를 기록하고, 다양한 크기와 아키텍처를 가진 LLM과 비교하는 방식으로 진행되었습니다. MEG는 뇌의 전기적 활동으로 인해 발생하는 자기장을 측정하는 기술로, fMRI에 비해 시간 해상도가 높다는 장점이 있습니다. 참가자들은 10시간 분량의 오디오북을 청취하였으며, 이 과정에서 뇌의 반응을 측정했습니다. 오디오북은 다양한 장르와 난이도를 가진 텍스트로 구성되었을 가능성이 높습니다.

### 평가 지표

- **정렬 점수**: 피어슨 상관 계수를 사용하여 LLM 활성화와 뇌 신호 간의 정렬 정도를 평가했습니다. 정렬 점수가 높을수록 LLM이 뇌의 언어 처리 방식을 더 잘 모방한다고 해석할 수 있습니다.
- **p-값**: 각 결과의 통계적 유의성을 평가하기 위해 사용되었습니다. p-값이 낮을수록 결과가 우연에 의해 발생했을 가능성이 낮다는 것을 의미합니다. 일반적으로 p-값이 0.05보다 작으면 통계적으로 유의하다고 판단합니다.

### 베이스라인

- **Transformer**: BERT와 GPT-2를 포함한 다양한 Transformer 모델이 사용되었습니다. Transformer 모델은 self-attention 메커니즘을 통해 문장 내 단어 간의 장거리 의존성을 효과적으로 모델링할 수 있습니다.
- **Recurrent**: LSTM과 GRU를 포함한 Recurrent 모델도 비교 대상으로 사용되었습니다. Recurrent 모델은 순차적인 데이터 처리에 강점을 가지지만, 장거리 의존성 모델링에는 어려움이 있습니다.

### 하이퍼파라미터

| 하이퍼파라미터 | 값 | 설명 |
|---------------|----|---------------------------------------------------|
| 학습률        | 0.001 | 모델 학습 시 사용되는 learning rate |
| 배치 크기     | 32 | 모델 학습 시 한 번에 처리하는 데이터의 양 |
| 정규화 파라미터 $\alpha$ | 0.1 | Ridge 회귀의 정규화 강도를 조절하는 파라미터 |
| 문맥 길이     | 128, 256, 512 | 모델이 한 번에 처리할 수 있는 입력 텍스트의 길이 |

## 실험 결과 분석

### 주요 결과

| 모델 | 정렬 점수 | p-값 |
|------|----------|------|
| BERT | 0.78     | 0.004 |
| GPT-2 | 0.82    | 0.002 |
| Transformer-XL | 0.80 | 0.003 |

### 성능 향상률

모델 크기와 문맥 길이가 증가할수록 정렬 점수가 증가했으며, 이는 로그 스케일에서 포화되는 경향을 보였습니다. 예를 들어, 10억 개의 파라미터를 가진 모델과 100억 개의 파라미터를 가진 모델 간의 정렬 점수 차이는 1억 개의 파라미터를 가진 모델과 10억 개의 파라미터를 가진 모델 간의 차이보다 작았습니다. 이는 모델 크기를 무작정 늘리는 것보다 다른 요인(예: 학습 데이터, 모델 아키텍처)을 개선하는 것이 더 효과적일 수 있다는 것을 시사합니다. 또한, 문맥 길이를 늘리는 것이 모델 크기를 늘리는 것보다 뇌 신호와의 정렬에 더 큰 영향을 미칠 수 있다는 점도 주목할 만합니다.

### Ablation Study 분석

모델의 특정 레이어를 제거하거나 문맥 길이를 줄였을 때, 정렬 점수가 감소하는 경향을 보였습니다. 이는 모델의 초기와 깊은 레이어가 각각 뇌의 초기와 후기 반응과 정렬되는 데 중요한 역할을 한다는 것을 시사합니다. 예를 들어, 초기 레이어를 제거하면 청각 피질과의 정렬 점수가 감소하고, 깊은 레이어를 제거하면 언어 피질과의 정렬 점수가 감소하는 것을 확인할 수 있습니다.

## 비판적 평가

### 강점

1. **다양한 모델 아키텍처 분석**: Transformer와 Recurrent 모델을 포함한 다양한 아키텍처를 비교하여, 모델의 구조가 뇌 신호와의 정렬에 미치는 영향을 체계적으로 분석했습니다.
2. **모델 크기와 문맥 길이의 영향 탐구**: 모델 크기와 문맥 길이가 뇌 신호와의 정렬에 미치는 영향을 체계적으로 분석하여, 모델 성능 향상의 한계를 규명했습니다.
3. **정확한 통계적 분석**: p-값과 신뢰 구간을 통해 결과의 통계적 유의성을 철저히 검증했습니다.

### 한계점과 개선 방향

- **언어 편향**: 특정 언어에 대한 편향이 존재할 수 있으며, 다양한 언어에 대한 추가 실험이 필요합니다. 영어 오디오북만을 사용했을 경우, 다른 언어에 대한 일반화 가능성이 낮아질 수 있습니다.
- **복잡한 언어 작업에 대한 분석 부족**: 추론이나 질문 응답과 같은 복잡한 언어 작업에 대한 추가 연구가 필요합니다. 오디오북 청취는 비교적 수동적인 작업이므로, 능동적인 언어 처리 과정에 대한 분석이 필요합니다.
- **개인차 고려 부족**: 뇌 신호는 개인차가 클 수 있으므로, 개인별 뇌 신호 패턴과 LLM 정렬 정도 간의 관계를 분석하는 것이 필요합니다.

### 재현성 평가

실험 설정과 방법론이 상세히 기술되어 있어 재현 가능성이 높습니다. 그러나 코드가 아직 공개되지 않아, 코드 공개가 이루어질 경우 재현성이 더욱 높아질 것입니다. 데이터셋과 모델에 대한 정보가 더 자세히 공개될수록 재현성이 높아질 것입니다.

## 향후 연구 방향

- **복잡한 언어 작업 분석**: 추론, 질문 응답과 같은 복잡한 언어 작업에 대한 LLM과 뇌의 정렬 정도를 평가할 필요가 있습니다. 예를 들어, LLM이 특정 질문에 답변하는 과정에서 뇌의 어떤 영역이 활성화되는지를 분석할 수 있습니다.
- **개별 차이 분석**: 개인의 뇌 활동 패턴과 LLM의 정렬 정도 간의 관계를 조사할 필요가 있습니다. 예를 들어, 언어 능력이 뛰어난 사람과 그렇지 않은 사람 간의 뇌 신호 패턴과 LLM 정렬 정도를 비교할 수 있습니다.
- **모델 개선**: 뇌의 작동 방식을 모방하여 LLM의 성능을 향상시키는 방법을 연구할 필요가 있습니다. 예를 들어, 뇌의 sparse activation 패턴을 모방하거나, 뇌의 hierarchical processing 구조를 반영하는 모델을 개발할 수 있습니다.
- **Multimodal 데이터 활용**: 텍스트뿐만 아니라 이미지, 오디오 등 다양한 modality의 데이터를 함께 사용하여 LLM과 뇌의 정렬 정도를 분석할 수 있습니다.

## 실무 적용 가이드

- **구현 시 고려사항**: 모델의 크기와 문맥 길이를 적절히 조정하여 뇌 신호와의 정렬을 최적화할 필요가 있습니다. 모델 크기를 늘리는 데 드는 비용과 효과를 고려하여, 문맥 길이를 조절하거나 다른 모델 아키텍처를 사용하는 것을 고려할 수 있습니다.
- **팁**: Ridge 회귀와 같은 정규화 기법을 사용하여 과적합을 방지하고, 피어슨 상관 계수를 통해 모델의 성능을 평가하는 것이 중요합니다. 또한, 모델의 각 레이어가 뇌의 어떤 영역과 가장 잘 정렬되는지를 파악하여, 모델의 특정 레이어를 개선하는 데 활용할 수 있습니다. 예를 들어, 특정 레이어가 언어 피질과 잘 정렬되지 않는다면, 해당 레이어의 attention mechanism을 개선하거나, 학습 데이터를 추가하는 등의 시도를 할 수 있습니다.

## 결론

이 연구는 LLM과 인간 뇌가 언어 처리 과정에서 유사한 계산 경로를 공유할 가능성을 제시하며, 모델 크기와 문맥 길이가 이러한 유사성에 영향을 미친다는 점을 보여주었습니다. 이러한 발견은 인공지능 모델의 작동 방식을 이해하고, 더 나아가 인간 뇌의 작동 방식을 이해하는 데 기여할 수 있습니다. 특히, LLM 개발 시 모델 크기를 무작정 늘리는 것보다 문맥 길이를 적절히 조절하고, 뇌의 작동 방식을 모방하는 것이 더 효과적인 전략일 수 있다는 점을 시사합니다.

## 참고 자료

- 논문 링크: [arXiv:2512.01591](https://arxiv.org/abs/2512.01591)
- 코드 저장소: 공개 예정
- 관련 자료: MEG 데이터셋, LLM 모델 아키텍처