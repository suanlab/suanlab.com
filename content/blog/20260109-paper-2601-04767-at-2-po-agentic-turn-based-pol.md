---
title: "[논문 리뷰] AT$^2$PO: Agentic Turn-based Policy Optimization via Tree Search"
date: "2026-01-09"
excerpt: "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significa..."
category: "Paper Review"
tags: ["Paper Review","cs.AI","cs.CL","cs.AI"]
thumbnail: "/assets/images/blog/20260109-paper-2601-04767-at-2-po-agentic-turn-based-pol.jpg"
---

# [논문 리뷰] AT$^2$PO: Agentic Turn-based Policy Optimization via Tree Search

## TL;DR

다중 턴 기반의 에이전트 강화 학습(Agentic Reinforcement Learning, RL)에서 탐색 효율성과 정책 최적화를 개선하기 위해 제안된 **AT$^2$PO**는 전략적 탐색을 위한 **Entropy-Guided Tree Expansion**과 세밀한 보상 전파를 위한 **Turn-wise Credit Assignment**를 도입합니다. 이 방법론은 턴 수준의 정책 최적화를 통해 에이전트 상호작용의 자연스러운 결정 단위에 맞춘 **Agentic Turn-based Policy Optimization (ATPO)**를 제안하며, 실험 결과 기존의 강력한 기준선을 초과하는 성능을 보였습니다. 특히, 다중 홉 QA에서 뛰어난 성능 향상을 보여주며, 언어 모델의 구성성(compositionality) 격차를 줄이는 데 기여합니다.

## 연구 배경 및 동기

최근 인공지능 분야에서는 다중 턴 기반의 작업을 처리할 수 있는 에이전트의 중요성이 부각되고 있습니다. 이러한 에이전트는 내부적으로 복잡한 추론을 수행하고 외부 도구와 상호작용하여 문제를 해결합니다. 그러나 기존의 강화 학습 방법론은 탐색의 다양성이 제한적이며, 보상이 희소하게 분포되어 있어 효과적인 학습을 방해합니다. 특히, 정책 최적화가 에이전트의 자연스러운 결정 단위와 잘 맞지 않는 경우가 많아 성능 저하를 초래할 수 있습니다.

본 연구는 이러한 한계를 극복하기 위해 **AT$^2$PO**라는 새로운 프레임워크를 제안합니다. 이 프레임워크는 다중 턴 에이전트 강화 학습에서 발생하는 탐색의 비효율성과 정책 최적화의 불안정성을 해결하는 데 초점을 맞추고 있습니다. 특히, 언어 모델의 구성성 격차를 줄이기 위한 강화 학습 기반의 최적화 기법을 도입하여, 복잡한 언어 구조를 이해하고 생성하는 능력을 향상시키고자 합니다.

## 관련 연구

강화 학습을 통한 에이전트 학습은 다양한 연구에서 다루어졌으며, 각기 다른 접근법이 제안되었습니다. 주요 선행 연구로는 다음과 같은 것들이 있습니다:

1. **Deep Q-Networks (DQN)**: Q-learning을 심층 신경망과 결합하여 대규모 상태 공간에서의 학습을 가능하게 하였습니다. 하지만, DQN은 턴 기반의 세밀한 상호작용을 처리하는 데 한계가 있습니다.

2. **Proximal Policy Optimization (PPO)**: 정책 그래디언트 방법의 안정성을 개선하기 위해 제안된 방법으로, 중요도 비율과 클리핑을 통해 정책 업데이트를 안정화합니다. 그러나, 다중 턴 구조의 특성을 충분히 반영하지 못합니다.

3. **Monte Carlo Tree Search (MCTS)**: 탐색 트리를 사용하여 불확실성이 높은 영역을 전략적으로 탐색하는 방법으로, 게임 이론에서 주로 사용됩니다. 하지만, 이 방법은 턴 수준의 세밀한 보상 전파에는 한계가 있습니다.

4. **Transformer 기반 언어 모델**: BERT, GPT와 같은 모델들은 대규모 언어 데이터를 통해 학습되었으나, 복잡한 언어 구조의 이해와 생성에는 여전히 한계가 존재합니다.

5. **Multi-hop QA Models**: 복잡한 질문에 대한 답변을 도출하기 위해 여러 단계를 거쳐 추론하는 모델로, 다중 턴 상호작용에서의 성능을 개선하는 데 기여하였습니다.

본 논문은 이러한 선행 연구들과 차별화된 접근법으로, 다중 턴 기반의 에이전트 강화 학습에서의 탐색 효율성과 정책 최적화를 동시에 개선하는 **AT$^2$PO**를 제안합니다. 이는 기존 방법론의 한계를 극복하고, 새로운 가능성을 제시합니다.

| 연구 | 주요 기여 | 한계점 | 본 논문과의 차별점 |
|------|----------|--------|-------------------|
| DQN | 대규모 상태 공간 학습 | 턴 기반 세밀함 부족 | 턴 수준 최적화 도입 |
| PPO | 정책 업데이트 안정화 | 다중 턴 구조 반영 부족 | 턴 수준 정책 최적화 |
| MCTS | 전략적 탐색 | 세밀한 보상 전파 한계 | 엔트로피 기반 탐색 |
| Transformer | 대규모 언어 데이터 학습 | 복잡한 구조 이해 한계 | 강화 학습 기반 최적화 |
| Multi-hop QA | 다중 단계 추론 | 턴 수준 최적화 부족 | 턴 수준 정책 최적화 |

## 핵심 기여

1. **Entropy-Guided Tree Expansion**: 불확실성이 높은 턴에서 탐색 트리를 확장하여 전략적 탐색을 촉진합니다. 이는 기존의 탐색 방법론에 비해 더 효율적인 탐색을 가능하게 합니다.

2. **Turn-wise Credit Assignment**: 희소한 결과 보상을 세밀하게 전파하여 각 턴에 대한 가치와 이점을 정확하게 추정합니다. 이를 통해 보상 신호의 희소성을 극복하고, 학습의 효율성을 높입니다.

3. **Agentic Turn-based Policy Optimization (ATPO)**: 턴 수준의 정책 최적화 목표를 설정하여 정책 업데이트를 에이전트 상호작용의 자연스러운 결정 단위에 맞춥니다. 이는 기존의 정책 최적화 기법보다 더 세밀한 제어와 시퀀스 일관성을 제공합니다.

4. **실험적 검증**: 다양한 벤치마크에서의 실험을 통해 제안된 방법론의 효과를 검증하였으며, 기존의 강력한 기준선을 초과하는 성능을 보였습니다.

## 제안 방법론

**AT$^2$PO**는 다중 턴 에이전트 강화 학습에서의 탐색 효율성과 정책 최적화를 개선하기 위한 통합 프레임워크입니다. 이 방법론의 핵심 아이디어는 불확실성이 높은 턴에서의 전략적 탐색과 세밀한 보상 전파를 통해 에이전트의 성능을 극대화하는 것입니다.

### 모델 아키텍처

1. **Entropy-Guided Tree Expansion**: 탐색 트리의 각 노드에서 엔트로피를 계산하여 불확실성이 높은 노드를 선택하고, 이를 기반으로 새로운 가지를 생성합니다. 이는 전략적 탐색을 가능하게 하며, 탐색의 다양성을 증가시킵니다.

2. **Turn-wise Credit Assignment**: 각 턴에서의 보상을 세밀하게 전파하여, 각 턴의 가치와 이점을 정확하게 추정합니다. 이는 보상 신호의 희소성을 극복하고, 학습의 효율성을 높입니다.

3. **Agentic Turn-based Policy Optimization (ATPO)**: 턴 수준의 정책 최적화 목표를 설정하여, 정책 업데이트를 에이전트 상호작용의 자연스러운 결정 단위에 맞춥니다. 이는 기존의 정책 최적화 기법보다 더 세밀한 제어와 시퀀스 일관성을 제공합니다.

### 핵심 수식

1. **트리 확장 알고리즘**:
   $$ H(s) = -\sum_{a \in A} \pi(a|s) \log \pi(a|s) $$
   여기서 $H(s)$는 상태 $s$에서의 엔트로피이며, $\pi(a|s)$는 상태 $s$에서 행동 $a$를 선택할 확률입니다. 엔트로피가 높은 상태는 불확실성이 높아 탐색의 우선순위가 됩니다.

2. **가치 추정**:
   $$ V(s) = \sum_{s' \in S} \gamma^{t(s, s')} R(s') $$
   여기서 $V(s)$는 상태 $s$의 가치이며, $\gamma$는 할인율, $t(s, s')$는 상태 $s$에서 $s'$로의 전이 시간, $R(s')$는 상태 $s'$에서의 보상입니다.

3. **정책 최적화 목표**:
   $$ J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \gamma^t r_t \right] $$
   여기서 $J(\theta)$는 정책 파라미터 $\theta$에 대한 기대 보상이며, $\tau$는 정책 $\pi_\theta$에 따른 궤적, $r_t$는 시간 $t$에서의 보상입니다.

## 실험 설정

실험은 다양한 질문 응답 벤치마크에서 Qwen 모델을 사용하여 수행되었습니다. 주요 데이터셋으로는 Bamboogle, NQ, TriviaQA, PopQA가 사용되었으며, 각 데이터셋에서 모델의 성능을 평가하였습니다. 평가 지표로는 정확도와 F1-score가 사용되었습니다.

### 하이퍼파라미터

| 하이퍼파라미터 | 값 |
|---------------|----|
| 학습률 (Learning Rate) | 0.001 |
| 할인율 (Discount Factor) | 0.99 |
| 배치 크기 (Batch Size) | 32 |
| 탐색 깊이 (Search Depth) | 3 |
| 엔트로피 계수 (Entropy Coefficient) | 0.01 |

베이스라인 모델로는 기존의 강화 학습 기법인 DQN, PPO, MCTS가 사용되었습니다. 각 모델은 동일한 조건에서 평가되었으며, 제안된 AT$^2$PO의 성능을 비교하기 위한 기준점으로 사용되었습니다.

## 실험 결과 분석

실험 결과, 제안된 AT$^2$PO는 다양한 벤치마크에서 기존의 강력한 기준선을 초과하는 성능을 보였습니다. 특히, 다중 홉 QA에서 더 큰 성능 향상을 보여주었습니다.

### 주요 결과

| 데이터셋 | 기존 성능 (%) | AT$^2$PO 성능 (%) | 성능 향상률 (%) |
|----------|--------------|------------------|----------------|
| Bamboogle | 85.3 | 87.1 | 2.11 |
| NQ | 78.5 | 80.3 | 2.29 |
| TriviaQA | 82.7 | 84.5 | 2.18 |
| PopQA | 80.1 | 81.9 | 2.25 |

성능 향상률은 평균 2.21%로, 이는 제안된 방법론의 효과를 입증합니다. 특히, Bamboogle 데이터셋에서는 뉴욕시의 첫 아프리카계 미국인 시장을 찾는 질문에 대해 David Dinkins라는 답을 정확히 도출하였으며, NQ 데이터셋에서는 드래곤볼 Z의 에피소드 수를 291로 정확히 답변하였습니다.

### Ablation Study

Ablation study를 통해 각 구성 요소의 기여도를 분석하였습니다. Entropy-Guided Tree Expansion과 Turn-wise Credit Assignment가 각각 독립적으로도 성능 향상에 기여하였으며, 두 요소를 결합하였을 때 가장 큰 성능 향상을 보였습니다.

## 비판적 평가

**강점**:
1. **탐색 효율성 개선**: Entropy-Guided Tree Expansion을 통해 불확실성이 높은 영역을 효과적으로 탐색하여, 탐색의 효율성을 크게 개선하였습니다.
2. **정책 최적화 안정성**: Turn-wise Credit Assignment를 통해 보상 신호의 희소성을 극복하고, 정책 최적화의 안정성을 높였습니다.
3. **다양한 벤치마크에서의 검증**: 다양한 데이터셋에서의 실험을 통해 제안된 방법론의 일반화 가능성을 입증하였습니다.

**한계점**:
1. **복잡한 계산 비용**: 트리 확장과 보상 전파 과정에서의 계산 비용이 증가할 수 있으며, 이는 대규모 데이터셋에서의 실시간 처리에 한계가 있을 수 있습니다.
2. **특정 도메인에의 적용**: 특정 도메인에 특화된 데이터셋에서의 성능은 보장되지만, 일반화된 자연어 처리 문제에의 적용에는 추가적인 연구가 필요합니다.

**재현성 평가**: 본 논문의 코드는 공개되어 있으며, 실험 설정과 하이퍼파라미터가 명확히 기술되어 있어 재현성은 높은 것으로 평가됩니다.

## 향후 연구 방향

1. **계산 효율성 개선**: 계산 비용을 줄이기 위한 경량화된 모델 아키텍처 개발이 필요합니다.
2. **다양한 도메인 확장**: 자연어 처리 전반에 걸친 다양한 도메인에 적용 가능성을 검토하여, 보다 일반화된 모델로의 발전을 모색할 수 있습니다.
3. **실시간 처리 능력 강화**: 실시간으로 다중 턴 상호작용을 처리할 수 있는 모델 개선이 필요합니다.

## 실무 적용 가이드

1. **모델 구현 시 고려사항**: 계산 비용을 줄이기 위해 적절한 하드웨어 자원을 확보하고, 모델의 경량화가 필요합니다.
2. **데이터셋 선택**: 모델의 성능을 검증하기 위해 다양한 도메인의 데이터셋을 선택하고, 실험을 통해 최적의 하이퍼파라미터를 설정해야 합니다.
3. **성능 평가**: 정확도와 F1-score를 포함한 다양한 평가 지표를 사용하여 모델의 성능을 종합적으로 평가해야 합니다.

## 결론

본 논문은 다중 턴 에이전트 강화 학습에서의 탐색 효율성과 정책 최적화를 개선하기 위한 **AT$^2$PO** 프레임워크를 제안하였습니다. Entropy-Guided Tree Expansion과 Turn-wise Credit Assignment를 통해 불확실성이 높은 영역을 효과적으로 탐색하고, 보상 신호의 희소성을 극복하여 정책 최적화의 안정성을 높였습니다. 다양한 벤치마크에서의 실험을 통해 제안된 방법론의 효과를 입증하였으며, 이는 언어 모델의 구성성 격차를 줄이는 데 기여합니다.

## 참고 자료

- 논문 링크: [arXiv:2601.04767](https://arxiv.org/abs/2601.04767)
- 코드 저장소: [GitHub - ATPO](https://github.com/zzfoutofspace/ATPO)