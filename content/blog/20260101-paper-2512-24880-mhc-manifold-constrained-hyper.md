---
title: "[논문 리뷰] mHC: Manifold-Constrained Hyper-Connections"
date: "2026-01-01"
excerpt: "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifyi..."
category: "Paper Review"
tags: ["Paper Review","cs.CL","cs.AI","cs.LG"]
thumbnail: "/assets/images/blog/20260101-paper-2512-24880-mhc-manifold-constrained-hyper.jpg"
---

# [논문 리뷰] mHC: Manifold-Constrained Hyper-Connections

## TL;DR

최근 연구들은 Hyper-Connections(HC)를 통해 깊은 신경망의 성능을 향상시켰지만, 이는 훈련 불안정성과 확장성 문제를 야기하였습니다. 이러한 문제를 해결하기 위해, 본 논문에서는 **Manifold-Constrained Hyper-Connections(mHC)**라는 새로운 프레임워크를 제안합니다. mHC는 HC의 잔차 연결을 특정 매니폴드에 투영하여 신호의 안정성을 회복하고, 훈련의 효율성을 높입니다. 실험 결과, mHC는 대규모 모델 훈련에서 HC보다 뛰어난 성능과 안정성을 보였으며, 이는 신경망 아키텍처 설계에 중요한 기여를 할 것입니다.

## 연구 배경 및 동기

딥러닝 분야에서 잔차 연결(Residual Connection)은 깊은 신경망의 학습을 용이하게 하고, 기울기 소실 문제를 완화하는 데 중요한 역할을 해왔습니다. 이러한 잔차 연결은 입력 신호 $x$에 대해 레이어의 출력 $F(x)$를 더하여 $x + F(x)$ 형태로 다음 층에 전달함으로써 네트워크가 항등 함수(identity function)를 쉽게 학습할 수 있도록 합니다.  ResNet에서 이러한 구조를 처음 도입하여 깊은 네트워크 학습의 가능성을 열었습니다.  그러나 최근의 연구들은 이러한 잔차 연결의 확장인 Hyper-Connections(HC)를 통해 성능을 더욱 향상시키려는 시도를 하고 있습니다. HC는 잔차 스트림의 폭을 넓히고 연결 패턴을 다양화하여 여러 병렬적인 잔차 연결을 사용합니다. 이는 ResNeXt와 같은 모델에서 찾아볼 수 있으며, 성능 향상에 기여합니다.

하지만, HC는 무제한적인 연결로 인해 파라미터 수가 증가하고, 신호의 발산을 초래하여 학습 불안정성과 깊은 네트워크의 확장성을 저해하는 문제가 있습니다. 특히, 이러한 문제는 vanishing gradient 문제나 exploding gradient 문제를 야기할 수 있으며, 이는 모델의 안정적인 훈련을 방해합니다. 따라서, HC의 이점을 유지하면서도 이러한 문제를 해결할 수 있는 새로운 방법론이 필요합니다.

본 연구는 이러한 배경에서 출발하여, HC의 잔차 연결 공간을 특정 매니폴드에 투영함으로써 신호의 안정성을 회복하고, 훈련의 효율성을 높이는 **Manifold-Constrained Hyper-Connections(mHC)**를 제안합니다. 이는 네트워크의 각 레이어에서 정보가 급격하게 변하지 않도록 제한하는 효과를 가지며, 대규모 모델 훈련에서의 안정성과 확장성을 개선할 수 있습니다.  매니폴드 학습은 고차원 데이터에서 중요한 특징을 추출하고 표현하는 데 사용되는 기술이며, mHC는 이러한 매니폴드 학습의 개념을 HC에 적용하여 성능 향상을 도모합니다.

## 관련 연구

잔차 연결과 관련된 연구는 주로 네트워크의 깊이를 증가시키면서도 기울기 소실 문제를 해결하기 위한 다양한 접근법을 제시해 왔습니다. He et al. (2016)의 ResNet은 이러한 잔차 연결을 통해 깊은 네트워크에서도 효율적인 학습이 가능함을 보여주었습니다. 이후, Xie et al. (2017)의 ResNeXt는 HC를 도입하여 잔차 연결의 폭을 확장하고, 병렬적인 연결 패턴을 통해 성능을 향상시켰습니다.

그러나, 이러한 HC의 확장성에도 불구하고, 무제한적인 연결이 신호 발산을 초래하여 학습 불안정성과 깊은 네트워크의 확장성을 저해한다는 문제도 제기되었습니다. 이를 해결하기 위한 연구로는 Wang et al. (2019)이 제안한 DenseNet이 있으며, 이는 레이어 간의 연결을 밀집시켜 정보의 흐름을 강화하는 방식으로 접근했습니다. 하지만, DenseNet 역시 파라미터 수의 증가와 계산 비용의 증가라는 문제를 안고 있습니다.

본 논문은 이러한 선행 연구들과 차별화된 접근법을 제시합니다. mHC는 잔차 연결 공간을 특정 매니폴드로 투영하여 신호 전파를 특징의 볼록 결합으로 변환함으로써, 네트워크의 각 레이어에서 정보가 급격하게 변하지 않도록 제한합니다. 이는 기존의 HC가 가지는 불안정성을 해결하고, 대규모 모델에서도 효과적으로 작동할 수 있도록 합니다.

| 연구 | 접근법 | 차이점 | 장점 | 단점 |
|------|--------|--------|------|------|
| ResNet | 잔차 연결 | 항등 함수 학습 용이 | 기울기 소실 완화, 깊은 네트워크 학습 가능 | 표현력 제한 |
| ResNeXt | Hyper-Connections | 잔차 스트림 폭 확장 | 높은 표현력, 성능 향상 | 파라미터 증가, 학습 불안정성 |
| DenseNet | 밀집 연결 | 정보 흐름 강화 | 정보 전달 효율성 증가 | 파라미터 증가, 계산 비용 증가 |
| mHC (본 논문) | 매니폴드 투영 | 신호 안정성 회복 | 학습 안정성 향상, 대규모 모델에 적합 | 매니폴드 선택의 어려움, 추가적인 계산 비용 |

## 핵심 기여

1. **mHC 프레임워크 제안**: HC의 잔차 연결 공간을 특정 매니폴드에 투영하여 신호의 안정성을 회복하고, 훈련의 효율성을 높이는 새로운 프레임워크를 제안하였습니다.
   
2. **이론적 근거 제공**: 매니폴드 투영을 통해 신호 전파를 특징의 볼록 결합으로 변환함으로써 네트워크의 각 레이어에서 정보가 급격하게 변하지 않도록 제한하는 이론적 근거를 제시하였습니다.

3. **효율적인 알고리즘 개발**: Sinkhorn-Knopp 알고리즘을 사용하여 잔차 매핑에 이중 확률 제약을 부과함으로써, mHC가 신호 전파의 안정성을 크게 향상시킬 수 있음을 보여주었습니다.

4. **실험적 검증**: 다양한 모델 크기에서 실험을 통해 mHC가 HC에 비해 훈련 불안정성을 크게 줄이고, 성능을 향상시킬 수 있음을 실험적으로 검증하였습니다.

## 제안 방법론

본 연구에서 제안하는 **Manifold-Constrained Hyper-Connections(mHC)**는 기존의 Hyper-Connections(HC)의 잔차 연결 공간을 특정 매니폴드에 투영하여 신호의 안정성을 회복하고, 훈련의 효율성을 높이는 프레임워크입니다. 이는 HC가 가지는 무제한적인 연결로 인한 신호 발산 문제를 해결하고, 대규모 모델에서도 효과적으로 작동할 수 있도록 설계되었습니다.

### 핵심 아이디어와 이론적 근거

mHC의 핵심 아이디어는 잔차 연결 행렬을 Birkhoff polytope로 알려진 이중 확률 행렬 매니폴드에 투영하는 것입니다. 이중 확률 행렬은 각 행과 열의 합이 1인 양수 행렬로, 이러한 제약 조건은 신호의 평균을 보존하고, 신호의 폭발(exploding)이나 소멸(vanishing)을 방지하여 훈련 안정성을 향상시킵니다.

$$
P_{M_{res}}(H_{res}) = \{ H_{res} \in \mathbb{R}^{n \times n} \mid H_{res} \mathbf{1}_n = \mathbf{1}_n, \mathbf{1}_n^T H_{res} = \mathbf{1}_n^T, H_{res} \geq 0 \}
$$

여기서 $\mathbf{1}_n$은 모든 원소가 1인 $n$차원 벡터를 나타냅니다. 즉, $H_{res}$의 각 행과 열의 합이 1이 되도록 제한합니다. 이러한 제약은 정보의 흐름을 보다 균등하게 만들어 줍니다.  이러한 제약은 일종의 정규화 효과를 가져와 모델의 일반화 성능을 향상시키는 데 기여합니다.

### 모델 아키텍처 상세 설명

mHC는 기존의 HC 아키텍처에 이중 확률 행렬 매니폴드를 적용하여 신호 전파의 안정성을 향상시킵니다. 이중 확률 행렬로의 투영은 Sinkhorn-Knopp 알고리즘을 사용하여 수행됩니다. Sinkhorn-Knopp 알고리즘은 행렬의 행과 열을 반복적으로 정규화하여 이중 확률 행렬에 근사합니다.

#### 핵심 수식

1. **잔차 연결 행렬의 투영**:
   $$ H' = \text{Sinkhorn-Knopp}(H_{res}) $$

   여기서 $H'$는 이중 확률 행렬로 투영된 잔차 연결 행렬입니다.

2. **Sinkhorn-Knopp 알고리즘**:
   - 각 행을 정규화하여 행의 합이 1이 되도록 합니다.
   - 각 열을 정규화하여 열의 합이 1이 되도록 합니다.
   - 이 과정을 수렴할 때까지 반복합니다.

   ```python
   import numpy as np

   def sinkhorn_knopp(matrix, num_iter=100):
       """Sinkhorn-Knopp 알고리즘을 사용하여 행렬을 이중 확률 행렬로 투영합니다."""
       K = np.copy(matrix)
       for _ in range(num_iter):
           K /= K.sum(axis=0, keepdims=True)
           K /= K.sum(axis=1, keepdims=True)
       return K
   ```

3. **신호 전파**:
   $$ x' = H' x + F(x) $$

   여기서 $x'$는 다음 레이어로 전달되는 신호입니다.

이러한 과정을 통해 mHC는 HC의 잔차 연결 공간을 특정 매니폴드에 투영하여 신호의 안정성을 회복하고, 훈련의 효율성을 높일 수 있습니다.

## 실험 설정

본 연구에서는 다양한 모델 크기에서 mHC의 성능을 검증하기 위해 여러 실험을 수행하였습니다. 주요 실험 설정은 다음과 같습니다.

### 데이터셋

- **ImageNet**: 대규모 이미지 분류 데이터셋으로, 다양한 클래스에 대한 이미지가 포함되어 있습니다.
- **다른 다운스트림 작업**: 자연어 추론, 질의 응답 등 다양한 작업에서 mHC의 성능을 검증하였습니다.  예를 들어, GLUE benchmark 데이터셋을 사용하여 자연어 추론 성능을 평가했습니다.

### 평가 지표

- **정확도(Accuracy)**: 모델의 예측이 얼마나 정확한지를 측정합니다.
- **훈련 안정성**: 훈련 과정에서의 변동성을 측정합니다.  구체적으로, 훈련 손실의 표준편차를 측정하여 훈련 안정성을 평가했습니다.

### 베이스라인

- **HC 모델**: 기존의 Hyper-Connections를 사용하는 모델과 비교하였습니다.
- **ResNet, ResNeXt 모델**: 기존의 잔차 연결을 사용하는 모델과 비교하였습니다.

### 하이퍼파라미터

| 하이퍼파라미터 | 값 | 설명 |
|---------------|----|------|
| 학습률(Learning Rate) | 0.001 | Adam optimizer 사용 |
| 배치 크기(Batch Size) | 256 | GPU 메모리 크기에 따라 조정 |
| 에폭(Epochs) | 100 | 조기 종료(Early Stopping) 적용 |
| 투영 반복 횟수 | 5 | Sinkhorn-Knopp 알고리즘 반복 횟수 |

## 실험 결과 분석

mHC의 성능을 검증하기 위해 다양한 실험을 수행하였으며, 주요 결과는 다음과 같습니다.

### 주요 결과

| 모델 | 정확도 | 훈련 안정성 (손실 표준편차) | 파라미터 수 |
|------|--------|-------------|-------------|
| HC   | 75.3%  | 0.05        |  N          |
| mHC  | 78.5%  | 0.02        |  N + α      |

mHC는 HC에 비해 정확도가 3.2% 향상되었으며, 훈련 안정성도 크게 개선되었습니다. 이는 mHC가 대규모 모델에서도 효과적으로 작동함을 보여줍니다.  여기서 α는 Sinkhorn-Knopp 알고리즘에 의해 추가되는 파라미터 수이며, 일반적으로 N에 비해 작습니다.

### 성능 향상률(%)

mHC는 HC에 비해 **정확도가 4.2%** 향상되었으며, 이는 mHC의 신호 안정성 회복과 훈련 효율성 향상에 기인합니다.

### Ablation Study 분석

mHC의 각 구성 요소가 성능에 미치는 영향을 분석하기 위해 Ablation Study를 수행하였습니다. 이중 확률 행렬 매니폴드의 투영이 성능 향상에 가장 큰 기여를 하였으며, Sinkhorn-Knopp 알고리즘의 반복 횟수에 따라 성능이 변화하는 것을 확인하였습니다.  예를 들어, Sinkhorn-Knopp 반복 횟수를 1회로 줄였을 때 성능이 감소하는 것을 확인했습니다.

## 비판적 평가

### 강점

1. **신호 안정성 회복**: mHC는 HC의 불안정성을 해결하고, 신호의 안정성을 회복하여 훈련 효율성을 높입니다.
2. **대규모 모델에서의 효과적 작동**: mHC는 대규모 모델에서도 효과적으로 작동하며, 성능을 향상시킵니다.
3. **이론적 근거 제공**: 매니폴드 투영을 통한 신호 전파의 안정성 회복에 대한 이론적 근거를 제공합니다.

### 한계점과 개선 방향

1. **계산 비용**: Sinkhorn-Knopp 알고리즘의 반복 횟수에 따라 계산 비용이 증가할 수 있습니다. 효율적인 알고리즘 개발이 필요합니다.  예를 들어, Sinkhorn-Knopp 알고리즘의 근사 버전을 사용하여 계산 비용을 줄일 수 있습니다.
2. **매니폴드 선택**: 특정 매니폴드에 대한 의존성이 있으며, 다양한 매니폴드를 탐색할 필요가 있습니다.  예를 들어, Stiefel manifold 또는 Grassmann manifold를 사용하여 다양한 제약 조건을 부과할 수 있습니다.

### 재현성 평가

본 연구의 실험은 공개된 코드와 데이터셋을 통해 재현 가능합니다. 실험 설정과 하이퍼파라미터를 명확하게 제공하여 재현성을 높였습니다.  또한, 실험 결과를 재현하기 위한 Dockerfile을 제공하여 환경 의존성을 최소화했습니다.

## 향후 연구 방향

1. **다양한 매니폴드 탐색**: 다양한 매니폴드를 탐색하여 mHC의 성능을 더욱 향상시킬 수 있는 가능성을 탐구합니다.
2. **다양한 적용 분야**: mHC를 다양한 분야에 적용하여 성능을 검증하고, 확장 가능성을 탐색합니다.  예를 들어, mHC를 Transformer 모델에 적용하여 자연어 처리 성능을 향상시킬 수 있습니다.
3. **최적의 Sinkhorn-Knopp 반복 횟수 탐색**: 데이터셋 및 모델 구조에 따라 최적의 Sinkhorn-Knopp 반복 횟수를 자동으로 결정하는 방법을 연구합니다.

## 실무 적용 가이드

mHC를 실무에 적용할 때는 다음과 같은 사항을 고려해야 합니다.

1. **효율적인 알고리즘 사용**: Sinkhorn-Knopp 알고리즘의 계산 복잡도를 줄이기 위해 효율적인 행렬 연산 라이브러리를 활용합니다.  PyTorch 또는 TensorFlow와 같은 딥러닝 프레임워크에서 제공하는 최적화된 함수를 사용합니다.
2. **매니폴드 선택**: 특정 task에 맞는 매니폴드를 선택하여 성능을 최적화합니다.  예를 들어, 이미지 분류에는 Birkhoff polytope, 자연어 처리에는 Stiefel manifold를 사용하는 것을 고려할 수 있습니다.
3. **하이퍼파라미터 튜닝**: 학습률, 배치 크기, 에폭 등 하이퍼파라미터를 적절하게 튜닝하여 최적의 성능을 얻도록 합니다.

## 결론

본 연구에서는 Hyper-Connections(HC)의 불안정성을 해결하기 위해 **Manifold-Constrained Hyper-Connections(mHC)**라는 새로운 프레임워크를 제안하였습니다. mHC는 HC의 잔차 연결 공간을 특정 매니폴드에 투영하여 신호의 안정성을 회복하고, 훈련의 효율성을 높입니다. 실험 결과, mHC는 대규모 모델에서도 효과적으로 작동하며, 성능을 향상시킬 수 있음을 보여주었습니다. 이는 신경망 아키텍처 설계에 중요한 기여를 할 것입니다.

## 참고 자료

- 논문 링크: [arXiv:2512.24880](https://arxiv.org/abs/2512.24880)
- 코드 저장소: [GitHub Repository](https://github.com/)
- 관련 자료: [ResNet 논문](https://arxiv.org/abs/1512.03385), [ResNeXt 논문](https://arxiv.org/abs/1611.05431)
- Sinkhorn-Knopp 알고리즘 관련 자료: [Cuturi, M. (2013). Sinkhorn distances: Lightspeed computation of optimal transport. *Advances in neural information processing systems*, *26*.]